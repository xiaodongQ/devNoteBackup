## 数据结构与算法之美

* 极客时间专栏：[数据结构与算法之美](https://time.geekbang.org/column/intro/126)
* 复杂度分析
    - 复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半
    - 你可能会有些疑惑，我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？这种方式叫`事后统计法`。但是，这种统计方法有非常大的局限性。
        + 1. 测试结果非常依赖测试环境
            * 换到另一台机器上时，可能会有截然相反的结果
        + 2. 测试结果受数据规模的影响很大
            * 对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别
            * 除此之外，如果测试数据规模太小，测试结果可能无法真实地反应算法的性能
            * 我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法
    - `大 O 复杂度表示法`
        + 大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度(asymptotic time complexity)，简称`时间复杂度`
    - 如何分析一段代码的时间复杂度？三个比较**实用的方法**：
        + 1. 只关注循环执行次数最多的一段代码
        + 2. 加法法则：总复杂度等于量级最大的那段代码的复杂度
            * 如果 `T1(n)=O(f(n))`，`T2(n)=O(g(n))`；那么 `T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n)))`
        + 3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
            * 如果 `T1(n)=O(f(n))`，`T2(n)=O(g(n))`；那么 `T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))`
    - 几种常见时间复杂度实例分析
        + 常见的复杂度量级并不多：`O(1)`、`O(logn)`、`O(n)`、`O(nlogn)`、`O(n^2)`/`O(n^3)`/.../`O(n^k)`、`O(2^n)`、`O(n!)`
        + 上述复杂度粗略地分为两类：*多项式量级*和*非多项式量级*。
            * 其中，非多项式量级只有两个：`O(2^n)` 和 `O(n!)`
                - 我们把时间复杂度为非多项式量级的算法问题叫作*NP（Non-Deterministic Polynomial，非确定多项式）问题*
                - 当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长
        + 主要看几种常见的*多项式时间复杂度*
            * 常数阶`O(1)`：只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)
            * 对数阶`O(logn)`、线性对数阶`O(nlogn)`
                - 实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。
                - 由于对数之间是可以互相转换的，`log3(n)`(此处为以3为底n的对数，由于编辑器排版问题展示不好位置) 就等于 `log3(2) * log2(n)`，所以 `O(log3n) = O(C * log2n)`，其中 `C=log3(2)` 是一个常量
                - 在采用大 O 标记复杂度的时候，可以忽略系数，即 `O(Cf(n)) = O(f(n))`，所以，`O(log2(n))` 就等于 `O(log3(n))`，因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 `O(logn)`
                - 如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了，O(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是 O(nlogn)
            * `O(m+n)`、`O(m*n)`
                - 跟前面都不一样的时间复杂度，代码的复杂度由两个数据的规模来决定
                - 我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个
    - *空间复杂度分析*
        + 类比一下时间复杂度，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系
        + 常见的空间复杂度就是 `O(1)`、`O(n)`、`O(n^2)`，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多
    - 四个复杂度分析方面的知识点
        + *最好情况时间复杂度（best case time complexity）*
            * 在最理想的情况下，执行这段代码的时间复杂度。
        + *最坏情况时间复杂度（worst case time complexity）*
            * 在最糟糕的情况下，执行这段代码的时间复杂度
        + *平均情况时间复杂度（average case time complexity）*
            * 最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，我们需要引入另一个概念：平均情况时间复杂度，后面简称为平均时间复杂度
            * 平均时间复杂度 的全称应该叫`加权平均时间复杂度`或者`期望时间复杂度`(将各种情况发生的概率考虑进去)
        + *均摊时间复杂度（amortized time complexity）*
            * 大部分情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限
    - [03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？](https://time.geekbang.org/column/article/40036)
    - [04 | 复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度](https://time.geekbang.org/column/article/40447)
* [05 | 数组：为什么很多编程语言中数组都从0开始编号？](https://time.geekbang.org/column/article/40961)
    - 数组（Array）是一种线性表数据结构。它用一组`连续`的内存空间，来存储一组具有`相同类型`的数据
    - 数组为了保证内存数据连续性，会导致插入、删除操作比较低效
        + 插入时间复杂度分析
            * 假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位
                - 如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 `O(1)`
                - 但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 `O(n)`
                - 因为我们在每个位置插入元素的`概率是一样的`，所以平均情况时间复杂度为 (1+2+…n)/n=`O(n)`
            * 若数组中的数据是*有序*的，则在某位置插入一个新元素时，就必须按照刚才的方法搬移 k 之后的数据
            * 但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合，则可`改进`为：
                - 在这种情况下，如果要将某个数据插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，*直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置*
                - 利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 `O(1)`。这个处理思想在快排中也会用到
        + 删除操作复杂度分析
            * 跟插入数据类似，如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了
                - 和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 `O(1)`；如果删除开头的数据，则最坏情况时间复杂度为 `O(n)`；平均情况时间复杂度也为 `O(n)`(删除末尾只操作1个数，删除开头操作n个数，第二个n-1...，删除每个元素概率为1/n，`(1+...+n)/n=(n+1)n/2n=(n+1)/2`)
            * 实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？
                - 可以*先记录*下已经删除的数据。每次的删除操作*并不是真正地搬移数据，只是记录数据已经被删除*。当数组没有更多空间存储数据时，我们再*触发执行一次*真正的删除操作，这样就大大减少了删除操作导致的数据搬移。(JVM 标记清除垃圾回收算法与之类似)
    - 开篇的问题：为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？
        + 从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”
            * 如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 `type_size` 的位置，a[k]的内存地址只需要用这个公式：
                - `a[k]_address = base_address + k * type_size`
            * 如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为
                - `a[k]_address = base_address + (k-1)*type_size`
            * 从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于CPU来说，就是多了一次减法指令。为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。
    - 在平时的业务开发中，我们可以直接使用编程语言提供的容器类(如Java的ArrayList)，但是，如果是特别底层的开发，直接使用数组可能会更合适
* [06 | 链表（上）：如何实现LRU缓存淘汰算法?](https://time.geekbang.org/column/article/41013)
    - 链表 Linked list
        + 相关概念：结点、后继指针next、头结点、尾结点
    - 缓存淘汰算法，常见策略：
        + 先进先出策略FIFO(First In, First Out)
        + 最少使用策略LFU(Least Frequently Used)
        + 最近最少使用策略LRU(Least Recently Used)
    - 针对链表的插入和删除，时间复杂度为`O(1)`
        + 只需要考虑相邻结点的指针改变
    - 三种最常见的链表结构
        + 单链表
            * 想访问第k个元素，只能根据指针一个结点一个结点依次遍历，时间复杂度`O(n)`
        + 双向链表
            * 每个结点不止有后继指针next，还有一个前驱指针prev指向前面的结点
            * 从结构上来看，双向链表可以支持`O(1)`时间复杂度的情况下找到前驱结点
                - 这也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效
                - 单链表的插入、删除已经是O(1)时间复杂度了，还能再怎么高效？
                - 删除结点：
                    + `情况1.` 删除结点中“值等于某个给定值”的结点：尽管单纯的删除操作时间复杂度是O(1)，但**遍历查找**的时间是主要的耗时点，对应的时间复杂度为`O(n)`，根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的*总时间复杂度*为`O(n)`
                    + `情况2.` 删除给定指针指向的结点：已经找到了某个结点q，但是要删除q需要知道它的前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表(`O(n)`)，而对于双向链表就不需要遍历了(`O(1)`)
                - 插入结点
                    + 和删除结点类似，对于单链表需要遍历其前驱结点以便插入，而双向链表则不需要
            * 除了插入、删除操作有优势之外，对于一个*有序链表*，双向链表的按值查询的效率也要比单链表高一些。
                - 因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据
                - Java的`LinkedHashMap`就用了双向链表这个结构(此外还用到散列表)
            * `空间换时间`的设计思想
                - 当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。
                - 相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路
                - 缓存就是利用了空间换时间的设计思想
        + 循环链表
            * 单链表的尾结点指针指向空地址，而循环链表的尾节点指向链表的头结点
            * 和单链表相比，循环链表的优点是从链尾到链头比较方便
            * 和双向链表整合在一起，就是双向循环链表
    - 链表和数组
        + 它们插入、删除、随机访问操作的时间复杂度正好相反
        + 数组使用连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高
        + 数组的缺点是大小固定，一经声明就要占用整块连续内存空间，过大可能没有足够内存来分配，过小可能不够用而只能申请更大空间进行拷贝；
        + 链表本身没有大小的限制，天然地支持动态扩容，链接中描述为这是它与数组最大的区别
    - 解答开篇问题：基于*链表*实现 LRU 缓存淘汰算法
        + 维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。
        + 当有一个新数据被访问时，就从链表头开始遍历链表
            * a. 若链表中有该数据(表示已缓存在链表中)，则将该结点删除，并插入到链表头部
            * b. 若链表中未缓存在链表中
                - 缓存未满时：将该结点插入到链表头部
                - 缓存已满：则删除链表尾结点，将新的结点插入到链表头部
        + 时间复杂度：遍历`O(n)`，结点操作`O(1)`，因此缓存访问的时间复杂度为`O(n)`
            * 优化：可引入*散列表（Hash table）*(后续)，记录每个数据的位置，将复杂度降到`O(1)`
        + 基于*数组*实现LRU缓存淘汰算法的思路：
            * 维护一个数组缓存访问的数据，越靠近数组开始位置的元素为越早访问
            * 有新数据访问时，从数组尾部开始查找
                - 若该数据已缓存在数组中，则将其删除(?涉及后面的数据搬移)，新增到数组尾
                - 若不在，直接在数组尾新增
                    + 缓存满时，删除数组首结点，元素全部前移
            * 删除结点搬移数据想想都觉得太暴力了，查找`O(n)`，搬移`O(n)`，复杂度是链表2倍，虽然最后还是为`O(n)`
            * 优化思路：
                - 由于每次删除结点搬移其后面的数据太耗时，用一个新的数组Arr保存要删除结点的位置(需要额外空间了，空间换时间)
                - 还是上面的逻辑走，发现要删除结点则先不做删除，而是记录位置到Arr，待缓存的空间不够时再做一次性删除和搬移处理
                - 复杂度：
                    + 由于需要额外空间，可指定固定的空间，当空间满时也触发批量删除，固定空间则空间复杂度也为`O(1)`
                    + 时间复杂度：还是需要查找和搬移，不过性能比上面要好些，虽然还是`O(n)`
    - 思考题：如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？
        + 解法
            * a. 将链表各结点的值复制到数组中，再从首尾依次取结点比较是否相同，有不同则非回文
                - 空间复杂度O(n)，时间复杂度 复制O(n)+判断O(n/2)=O(n)
            * b. 双指针法，定义两个指针以步长1和以步长2移动，直到找到中间结点，中间结点之后的部分反转，再各自比较两部分，最后再反转还原
                - 空间复杂度O(1), 时间复杂度 中间结点O(n/2)+反转O(n/2)+比较O(n/2)+反转O(n/2) = O(n)
        + 代码参考github：[linkedlist_palindromic](https://github.com/xiaodongQ/LeetCode/blob/master/datastruct_algo/linkedlist_palindromic.go)
        + LeetCode有该题：[234. 回文链表](https://leetcode-cn.com/problems/palindrome-linked-list/submissions/)
    - 关于链表的更多操作可见LeetCode
        + 参考：[链表标签](https://leetcode-cn.com/problemset/all/?topicSlugs=linked-list)




## 7天入门数据结构与算法

* 极客时间视频课程：[7天入门数据结构与算法](https://u.geekbang.org/lesson/7?article=159518)
    - [数据结构脑图](https://naotu.baidu.com/file/b832f043e2ead159d584cca4efb19703?token=7a6a56eb2630548c)
    - [算法脑图](https://naotu.baidu.com/file/0a53d3a5343bd86375f348b2831d3610?token=5ab1de1c90d5f3ec)

知识简洁记忆

听七牛云许式伟老师的架构课里说到，“架构能力的提升，本质上是对你的知识脉络的反复梳理与融会贯通的过程”。
目标是把这些内容先读厚，再读薄，融会贯通。

* 数据结构(分三大块，一维数据结构、二维数据结构、特殊数据结构)
    - 一维：
        + 基础型
            * 数组 array
            * 链表 linked list
        + 高级
            * 栈 stack
            * 队列 queue
            * 双端队列 deque
            * 集合 set
            * 映射 map (hash or map)
    - 二维(从一维泛化过来)：
        + 基础型
            * 树 tree
            * 图 graph
        + 高级(在树的基础上加了很多判断)
            * 二叉搜索树 binary search tree (red-black tree, AVL)
            * 堆 heap
            * 并查集 disjoint set
            * 字典树 Trie
    - 特殊(用于工程中特定的场景)
        + 位运算 Bitwise
        + 布隆过滤器 BloomFilter
        + LRU Cache 缓存
* 算法(8大点，前三点是算法最基础的地方，)
    - 分支 Branch: if-else/switch
    - 循环 Iteration: for/while loop
    - 递归 Recursion: Divide & Conquer/Backtrace
    - 搜索 Search: 深度优先搜索 Depth first search/广度优先搜索 Breadth first search/启发式搜索 A*
    - 动态规划 Dynamic Programming
    - 二分查找 Binary Search
    - 贪心 Greedy
    - 数学 Math, 几何 Geometry(英 /dʒiˈɒmətri/)
