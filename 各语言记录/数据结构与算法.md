## 数据结构与算法之美

* 极客时间专栏：[数据结构与算法之美](https://time.geekbang.org/column/intro/126)
    - 学习过程中的代码实践，也放在LeetCode的练习里：[xiaodongQ/LeetCode](https://github.com/xiaodongQ/LeetCode)
    - VSCode有LeetCode的插件，用Go来刷题，配置记录(搜`LeetCode插件配置`)：[vscode快捷键及插件](https://github.com/xiaodongQ/devNoteBackup/blob/master/工具使用/vscode快捷键及插件.md)
* 复杂度分析
    - 复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半
    - 你可能会有些疑惑，我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？这种方式叫`事后统计法`。但是，这种统计方法有非常大的局限性。
        + 1. 测试结果非常依赖测试环境
            * 换到另一台机器上时，可能会有截然相反的结果
        + 2. 测试结果受数据规模的影响很大
            * 对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别
            * 除此之外，如果测试数据规模太小，测试结果可能无法真实地反应算法的性能
            * 我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法
    - `大 O 复杂度表示法`
        + 大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度(asymptotic time complexity)，简称`时间复杂度`
    - 如何分析一段代码的时间复杂度？三个比较**实用的方法**：
        + 1. 只关注循环执行次数最多的一段代码
        + 2. 加法法则：总复杂度等于量级最大的那段代码的复杂度
            * 如果 `T1(n)=O(f(n))`，`T2(n)=O(g(n))`；那么 `T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n)))`
        + 3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
            * 如果 `T1(n)=O(f(n))`，`T2(n)=O(g(n))`；那么 `T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))`
    - 几种常见时间复杂度实例分析
        + 常见的复杂度量级并不多：`O(1)`、`O(logn)`、`O(n)`、`O(nlogn)`、`O(n^2)`/`O(n^3)`/.../`O(n^k)`、`O(2^n)`、`O(n!)`
        + 上述复杂度粗略地分为两类：*多项式量级*和*非多项式量级*。
            * 其中，非多项式量级只有两个：`O(2^n)` 和 `O(n!)`
                - 我们把时间复杂度为非多项式量级的算法问题叫作*NP（Non-Deterministic Polynomial，非确定多项式）问题*
                - 当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长
        + 主要看几种常见的*多项式时间复杂度*
            * 常数阶`O(1)`：只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)
            * 对数阶`O(logn)`、线性对数阶`O(nlogn)`
                - 实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。
                - 由于对数之间是可以互相转换的，`log3(n)`(此处为以3为底n的对数，由于编辑器排版问题展示不好位置) 就等于 `log3(2) * log2(n)`，所以 `O(log3n) = O(C * log2n)`，其中 `C=log3(2)` 是一个常量
                    + 根据换底公式：`log3(n) = log2(n)/log2(3) = log3(2) * log2(n)`，所以有上面的转换
                - 在采用大 O 标记复杂度的时候，可以忽略系数，即 `O(Cf(n)) = O(f(n))`，所以，`O(log2(n))` 就等于 `O(log3(n))`，因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 `O(logn)`
                - 如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了，O(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是 O(nlogn)
            * `O(m+n)`、`O(m*n)`
                - 跟前面都不一样的时间复杂度，代码的复杂度由两个数据的规模来决定
                - 我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个
    - *空间复杂度分析*
        + 类比一下时间复杂度，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系
        + 常见的空间复杂度就是 `O(1)`、`O(n)`、`O(n^2)`，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多
    - 四个复杂度分析方面的知识点
        + *最好情况时间复杂度（best case time complexity）*
            * 在最理想的情况下，执行这段代码的时间复杂度。
        + *最坏情况时间复杂度（worst case time complexity）*
            * 在最糟糕的情况下，执行这段代码的时间复杂度
        + *平均情况时间复杂度（average case time complexity）*
            * 最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，我们需要引入另一个概念：平均情况时间复杂度，后面简称为平均时间复杂度
            * 平均时间复杂度 的全称应该叫`加权平均时间复杂度`或者`期望时间复杂度`(将各种情况发生的概率考虑进去)
        + *均摊时间复杂度（amortized time complexity）*
            * 大部分情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限
    - [03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？](https://time.geekbang.org/column/article/40036)
    - [04 | 复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度](https://time.geekbang.org/column/article/40447)
* [05 | 数组：为什么很多编程语言中数组都从0开始编号？](https://time.geekbang.org/column/article/40961)
    - 数组（Array）是一种线性表数据结构。它用一组`连续`的内存空间，来存储一组具有`相同类型`的数据
    - 数组为了保证内存数据连续性，会导致插入、删除操作比较低效
        + 插入时间复杂度分析
            * 假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位
                - 如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 `O(1)`
                - 但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 `O(n)`
                - 因为我们在每个位置插入元素的`概率是一样的`，所以平均情况时间复杂度为 (1+2+…n)/n=`O(n)`
            * 若数组中的数据是*有序*的，则在某位置插入一个新元素时，就必须按照刚才的方法搬移 k 之后的数据
            * 但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合，则可`改进`为：
                - 在这种情况下，如果要将某个数据插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，*直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置*
                - 利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 `O(1)`。这个处理思想在快排中也会用到
        + 删除操作复杂度分析
            * 跟插入数据类似，如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了
                - 和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 `O(1)`；如果删除开头的数据，则最坏情况时间复杂度为 `O(n)`；平均情况时间复杂度也为 `O(n)`(删除末尾只操作1个数，删除开头操作n个数，第二个n-1...，删除每个元素概率为1/n，`(1+...+n)/n=(n+1)n/2n=(n+1)/2`)
            * 实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？
                - 可以*先记录*下已经删除的数据。每次的删除操作*并不是真正地搬移数据，只是记录数据已经被删除*。当数组没有更多空间存储数据时，我们再*触发执行一次*真正的删除操作，这样就大大减少了删除操作导致的数据搬移。(JVM 标记清除垃圾回收算法与之类似)
    - 开篇的问题：为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？
        + 从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”
            * 如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 `type_size` 的位置，a[k]的内存地址只需要用这个公式：
                - `a[k]_address = base_address + k * type_size`
            * 如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为
                - `a[k]_address = base_address + (k-1)*type_size`
            * 从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于CPU来说，就是多了一次减法指令。为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。
    - 在平时的业务开发中，我们可以直接使用编程语言提供的容器类(如Java的ArrayList)，但是，如果是特别底层的开发，直接使用数组可能会更合适
* [06 | 链表（上）：如何实现LRU缓存淘汰算法?](https://time.geekbang.org/column/article/41013)
    - 链表 Linked list
        + 相关概念：结点、后继指针next、头结点、尾结点
    - 缓存淘汰算法，常见策略：
        + 先进先出策略FIFO(First In, First Out)
        + 最少使用策略LFU(Least Frequently Used)
        + 最近最少使用策略LRU(Least Recently Used)
    - 针对链表的插入和删除，时间复杂度为`O(1)`
        + 只需要考虑相邻结点的指针改变
    - 三种最常见的链表结构
        + 单链表
            * 想访问第k个元素，只能根据指针一个结点一个结点依次遍历，时间复杂度`O(n)`
        + 双向链表
            * 每个结点不止有后继指针next，还有一个前驱指针prev指向前面的结点
            * 从结构上来看，双向链表可以支持`O(1)`时间复杂度的情况下找到前驱结点
                - 这也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效
                - 单链表的插入、删除已经是O(1)时间复杂度了，还能再怎么高效？
                - 删除结点：
                    + `情况1.` 删除结点中“值等于某个给定值”的结点：尽管单纯的删除操作时间复杂度是O(1)，但**遍历查找**的时间是主要的耗时点，对应的时间复杂度为`O(n)`，根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的*总时间复杂度*为`O(n)`
                    + `情况2.` 删除给定指针指向的结点：已经找到了某个结点q，但是要删除q需要知道它的前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表(`O(n)`)，而对于双向链表就不需要遍历了(`O(1)`)
                - 插入结点
                    + 和删除结点类似，对于单链表需要遍历其前驱结点以便插入，而双向链表则不需要
            * 除了插入、删除操作有优势之外，对于一个*有序链表*，双向链表的按值查询的效率也要比单链表高一些。
                - 因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据
                - Java的`LinkedHashMap`就用了双向链表这个结构(此外还用到散列表)
            * `空间换时间`的设计思想
                - 当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。
                - 相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路
                - 缓存就是利用了空间换时间的设计思想
        + 循环链表
            * 单链表的尾结点指针指向空地址，而循环链表的尾节点指向链表的头结点
            * 和单链表相比，循环链表的优点是从链尾到链头比较方便
            * 和双向链表整合在一起，就是双向循环链表
    - 链表和数组
        + 它们插入、删除、随机访问操作的时间复杂度正好相反
        + 数组使用连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高
        + 数组的缺点是大小固定，一经声明就要占用整块连续内存空间，过大可能没有足够内存来分配，过小可能不够用而只能申请更大空间进行拷贝；
        + 链表本身没有大小的限制，天然地支持动态扩容，链接中描述为这是它与数组最大的区别
    - 解答开篇问题：基于*链表*实现 LRU 缓存淘汰算法
        + 维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。
        + 当有一个新数据被访问时，就从链表头开始遍历链表
            * a. 若链表中有该数据(表示已缓存在链表中)，则将该结点删除，并插入到链表头部
            * b. 若链表中未缓存在链表中
                - 缓存未满时：将该结点插入到链表头部
                - 缓存已满：则删除链表尾结点，将新的结点插入到链表头部
        + 时间复杂度：遍历`O(n)`，结点操作`O(1)`，因此缓存访问的时间复杂度为`O(n)`
            * 优化：可引入*散列表（Hash table）*(后续)，记录每个数据的位置，将复杂度降到`O(1)`
        + 基于*数组*实现LRU缓存淘汰算法的思路：
            * 维护一个数组缓存访问的数据，越靠近数组开始位置的元素为越早访问
            * 有新数据访问时，从数组尾部开始查找
                - 若该数据已缓存在数组中，则将其删除(?涉及后面的数据搬移)，新增到数组尾
                - 若不在，直接在数组尾新增
                    + 缓存满时，删除数组首结点，元素全部前移
            * 删除结点搬移数据想想都觉得太暴力了，查找`O(n)`，搬移`O(n)`，复杂度是链表2倍，虽然最后还是为`O(n)`
            * 优化思路：
                - 由于每次删除结点搬移其后面的数据太耗时，用一个新的数组Arr保存要删除结点的位置(需要额外空间了，空间换时间)
                - 还是上面的逻辑走，发现要删除结点则先不做删除，而是记录位置到Arr，待缓存的空间不够时再做一次性删除和搬移处理
                - 复杂度：
                    + 由于需要额外空间，可指定固定的空间，当空间满时也触发批量删除，固定空间则空间复杂度也为`O(1)`
                    + 时间复杂度：还是需要查找和搬移，不过性能比上面要好些，虽然还是`O(n)`
    - 思考题：如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？
        + 解法
            * a. 将链表各结点的值复制到数组中，再从首尾依次取结点比较是否相同，有不同则非回文
                - 空间复杂度O(n)，时间复杂度 复制O(n)+判断O(n/2)=O(n)
            * b. 双指针法，定义两个指针以步长1和以步长2移动，直到找到中间结点，中间结点之后的部分反转，再各自比较两部分，最后再反转还原
                - 空间复杂度O(1), 时间复杂度 中间结点O(n/2)+反转O(n/2)+比较O(n/2)+反转O(n/2) = O(n)
        + 代码参考github：[linkedlist_palindromic](https://github.com/xiaodongQ/LeetCode/blob/master/datastruct_algo/linkedlist_palindromic.go)
        + LeetCode有该题：[234. 回文链表](https://leetcode-cn.com/problems/palindrome-linked-list/submissions/)
    - 关于链表的更多操作可见LeetCode
        + 参考：[链表标签](https://leetcode-cn.com/problemset/all/?topicSlugs=linked-list)
    - 几个写链表代码相关的技巧
        + [07 | 链表（下）：如何轻松写出正确的链表代码？](https://time.geekbang.org/column/article/41149)
        + 技巧一：理解指针或引用的含义
        + 技巧二：警惕指针丢失和内存泄漏
            * a和b结点间插入结点x，假设p指向a: 则先`x->next = p->next`，再`p->next = x`，若顺序反了则链表后面的部分访问不到
            * 删除结点，对该结点的内存手动释放(C/C++里，有垃圾回收的语言如Java/Go则不需要)
        + 技巧三：利用哨兵简化实现难度
            * 不用哨兵的场景，针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理
                - 向链表的结点p后插入一个结点：
                    + `new_node->next = p->next;`，`p->next = new_node;`，
                    + 但若是向空链表插入第一个结点，上面逻辑就不行了，需要：`if (head == null) { head = new_node; }`
                - 删除链表的结点p的后一个结点：
                    + `p->next = p->next->next;`
                    + 但若要删除结点的是链表所剩的最后一个结点，需要：`if (p->next == null) { head = null; }`
            * 引入哨兵结点(sentinel)
                - 不管链表是否为空，head指针都指向该哨兵结点。把这种有哨兵结点的链表叫*带头链表*(没有则称不带头链表)
                    + 哨兵节点不存任何数据
                - 插入
                    + 向空链表插入第一个结点则：`sentinel->next = new_node;`
                - 删除
                    + 删除只剩一个结点的链表结点，`sentinel->next = null;`
                - 因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了
        + 技巧四：重点留意边界条件处理
            * 如果链表为空时，代码是否能正常工作？
            * 如果链表只包含一个结点时，代码是否能正常工作？
            * 如果链表只包含两个结点时，代码是否能正常工作？
            * 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？
        + 技巧五：举例画图，辅助思考
            * 当写完代码之后，也可以举几个例子，画在纸上，照着代码走一遍，很容易就能发现代码中的 Bug
        + 技巧六：多写多练，没有捷径
            * 单链表反转、链表中环的检测、两个有序的链表合并、删除链表倒数第 n 个结点、求链表的中间结点
            * 链表操作相关的LeetCode个人练习链接：[链表相关操作](https://github.com/xiaodongQ/LeetCode/blob/master/linked_list/operate_linkedlist_test.go)
* [08 | 栈：如何实现浏览器的前进和后退功能？](https://time.geekbang.org/column/article/41222)
    - 栈既可以用数组来实现(*顺序栈*)，也可以用链表来实现(*链式栈*)。
    - 入栈和出栈只涉及栈顶个别数据操纵，时间复杂度为`O(1)`
    - 支持动态扩容的顺序栈
        + 当栈满了之后，就申请一个更大的数组，将原来的数据搬移到新数组中
        + 对于入栈操作来说，最好情况时间复杂度是 O(1)，最坏情况时间复杂度是 O(n)。
        + 平均情况下的时间复杂度，用`摊还分析法`分析：
            * 不满时，入栈`O(1)`
            * 当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存(假设满了扩展为2倍)，并且做 K 个数据的搬移操作，然后再入栈
            * 但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push(此处表示不涉及内存搬移的入栈操作) 操作就可以完成，这K次入栈总共涉及K个数据搬移和K次simple-push操作
            * 将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 `O(1)`。
    - 应用
        + 函数调用栈的应用
            * 参考示例链接中的代码调用，`main()`中调用了`add(int x, int y)`函数，`main`中定义的变量依次push入栈，然后是`add`的形参赋值后push入栈，而后add中定义的变量入栈，add中return时出栈
        + 表达式求值应用(编译器利用栈来实现表达式求值)
            * 编译器就是通过*两个栈*来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。
            * 我们从左向右遍历表达式，当遇到数字：我们就*直接压入操作数栈*；当遇到运算符：就与*运算符栈*的栈顶元素进行比较
                - 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；
                - 如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶*取 2 个操作数*，然后进行计算，*再把计算完的结果压入操作数栈*，继续比较。
                    + 注意此时遇到的运算符还没有压入栈，直到该运算符比运算符栈的栈顶运算符优先级高时，才将该运算符压入栈
            * 链接中用图示演示了一个简单示例：`3+5*8-6`，过程用文字说明如下(还是图示更清晰)：
                - `3`->操作数栈，`+`->运算符栈，`5`->操作数栈，`*`->运算符栈，`8`->操作数栈
                - `-`比运算符栈栈顶的`*`优先级低，则从操作数栈取`8`和`5`(出栈)，并取运算符栈栈顶`*`，得到`40`，压入操作数栈，此时操作数栈从栈顶向下为`40`，`3`
                - `-`和运算符栈当前栈顶`+`优先级相同，从操作数栈中取两个操作数`40`，`3`，取`+`，得到`43`(3+40=43，栈下面的元素作为操作符左边)压入操作数栈，此时操作数栈仅`43`一个栈帧
                - 将`-`压入运算符栈，`6`->操作数栈，表达式结束清空栈，从运算符栈取`-`，取两个操作数，得到`37`(43-6=37)
        + 括号匹配应用
            * 假设表达式中只包含三种括号，圆括号 ()、方括号[]和花括号{}，并且它们可以任意嵌套
                - 说明：`{[] ()[{}]}` 和 `[{()}([])]` 等为合法格式；`{[}()]` 或 `[({)]` 为不合法格式
            * 用栈来保存未匹配的左括号，从左到右依次扫描字符串。
                - 当扫描到左括号时，则将其压入栈中；
                - 当扫描到右括号时，从栈顶取出一个左括号
                - 如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。
                - 如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式
            * 当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。
    - 解答开篇问题：如何实现浏览器的前进和后退功能？
        + 使用两个栈，X 和 Y，我们把首次浏览的页面依次压入栈 X，当点击*后退*按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y
        + 当我们点击*前进*按钮时，我们依次*从栈 Y 中取出数据，放入栈 X 中*。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了
    - LeetCode：[栈相关题目](https://leetcode-cn.com/problemset/all/?topicSlugs=stack)
* [09 | 队列：队列在线程池等有限资源池中的应用](https://time.geekbang.org/column/article/41330)
    - 两个操作：入队`enqueue()` 和 出队`dequeue()`
    - 和栈类似，用数组实现的队列叫作`顺序队列`，用链表实现的队列叫作`链式队列`
    - 队列需要两个指针：一个是 head 指针，指向队头；一个是 tail 指针，指向队尾(tail不存数据)
        + 栈只需要一个栈顶指针
        + 入队时tail指针后移，出队时head指针后移
            * 入队时判断队列是否已满，出队时判断队列是否为空
    - 数组实现
        + 随着不停地进行入队、出队操作，head 和 tail(注意tail不存数据) 都会持续往后移动，当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了，如何解决？
            * 入队(enqueue)时，若队尾没有空闲空间了(tail==n)，则将数据搬移到队列头(需要队头有空闲空间，通过队头指针判断是否为第一个位置)
            * 入队O(1) (摊还分析法，head位置分别为1、n/2、n-1，搬移一次可支持后续n-head次入队)，出队O(1)
    - 链表实现
        + `enqueue`: tail->next=new_node, tail = tail->next
        + `dequeue`: head = head->next
    - 循环队列(可参考链接中的图示，更直观一些)
        + 避免上面数组实现方式的数据搬移
        + 最关键的是，确定好队空和队满的判定条件
            * 用数组实现的非循环队列中，队满条件：`tail==n`，队空：`head==tail`
            * 数组实现的循环队列中，队满条件：`(tail+1)%n==head`，队空还是：`head==tail`
                - 当队列满时，tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间
        + `enqueue`: 判是否满 `if ((tail+1)%n == head)`，入队后 `tail = (tail+1)%n`
        + `dequeue`: 判是否空 `if (head == tail)`，出队后 `head = (head+1)%n`
    - 应用
        + 阻塞队列(生产者 - 消费者模型)
            * 在队列基础上增加了阻塞操作
                - 队列为空的时候，从队头取数据会被阻塞
                - 如果队列已经满了，那么插入数据的操作就会被阻塞
        + 并发队列
            * 在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题
            * 线程安全的队列我们叫作并发队列
                - 最简单直接的方式是给`enqueue`和`dequeue`操作加锁，但锁粒度大并发度会降低，同一时刻只允许一个存或取操作
                - 基于数组的循环队列，利用 `CAS` 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因
            * 后续讲 Disruptor时，会再涉及并发队列的应用
    - 开篇问题：线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？
        + 一般有两种处理策略
            * 第一种是非阻塞的处理方式，直接拒绝任务请求；
            * 另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理
        + 阻塞两种实现方式
            * 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长
                - 所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的
            * 基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理
                - 需要设置一个合理的队列大小。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能
    - 对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队
        + 如数据库连接池
    - 笔记
        + C++里面的`condition_variable`，参考：[笔记](https://github.com/xiaodongQ/devNoteBackup/blob/master/%E5%90%84%E8%AF%AD%E8%A8%80%E8%AE%B0%E5%BD%95/C%2B%2B.md)，(搜`std::condition_variable`章节)
        + linux下的 `pthread_cond_t`，参考：[笔记](https://github.com/xiaodongQ/devNoteBackup/blob/master/%E5%90%84%E8%AF%AD%E8%A8%80%E8%AE%B0%E5%BD%95/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B.md)，(搜`## 条件变量(condition variable)`章节)
* [11 | 排序（上）：为什么插入排序比冒泡排序更受欢迎？](https://time.geekbang.org/column/article/41802)
    - 最经典的、最常用的一些排序算法：
        + 冒泡排序、插入排序、选择排序、
            * `O(n^2)`
        + 归并排序、快速排序、
            * `O(nlogn)`
        + 计数排序、基数排序、桶排序
            * `O(n)`
    - 排序算法的**执行效率**，一般从这几个方面来衡量：
        + 1. 最好情况、最坏情况、平均情况时间复杂度
            * 同时要说出对应情况下要排序的原始数据是什么样的
                - 对于要排序的数据，有的接近有序，有的完全无序
        + 2. 时间复杂度的系数、常数 、低阶
            * 时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶
            * 但是实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据
            * 所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来
        + 3. 比较次数和交换（或移动）次数
            * 在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去
    - 排序算法的**内存消耗**
        + 针对排序算法的空间复杂度，还引入了一个新的概念，`原地排序（Sorted in place）`
        + 原地排序算法，就是特指空间复杂度是 `O(1)` 的排序算法
    - 排序算法的**稳定性**
        + 如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变，那么这个排序就是稳定的
        + 为什么要考察排序算法的稳定性呢？
            * 演示数据结构和算法时，很多是用整数举例，但真正的软件开发中，要排序的对象往往是一组对象，而只是根据某个key排序
            * 如果要依据两个要素的顺序排列，如示例中的 先按时间先后再按金额大小，第一遍稳定的排序算法之后已经是有时间先后了，第二遍稳定排序后，先后顺序并不会改变；若第二遍排序不稳定，则按金额排序后相同金额的顺序可能发生变化
    - **冒泡排序**（Bubble Sort）
        + 每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。
        + 一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作
        + e.g.
            * `data := [...]int{3, 5, 4, 1, 2, 6}` 数组从小到大排列，第一次循环(冒泡)，依次比较相邻元素，会将6存在最后
            * 循环n次，遍历一遍所有元素进行：`if (data[j]>data[j+1]) swap`
                - 注意：每次内层遍历时，并不需要`len(data)-1`次，而是`len(data)-1-i`次(假设外层遍历下标i)，前面冒泡的数据并不必要再做比较
        + 对于上面的过程可以优化：
            * 当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作
        + 算法分析
            * 原地排序，空间复杂度：`O(1)`
            * 相同元素值时不做交换，所以冒泡排序是稳定的排序算法
            * 时间复杂度(需要分情况)：
                - 最好情况时间复杂度：数据已经有序，只需要冒泡一次，所以为`O(n)`
                - 最坏情况时间复杂度：数据刚好是倒序，要冒泡n次，为`O(n^2)`
                - 平均情况下的时间复杂度：`O(n^2)`
        + 平均情况时间复杂度分析过程：
            * 有序度和逆序度
                - 有序度是数组中具有有序关系的元素对的个数
                    + 倒序排列的数组，有序度为0
                    + 完全有序的数组的有序度叫作满有序度，其有序度为 `C(n,2)=n(n-1)/2`
                - 逆序度和有序度正好相反
                - 逆序度 = 满有序度 - 有序度
            * 冒泡排序包含两个原子操作：比较和交换。每次交换，有序度就加1
            * 不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是`n*(n-1)/2 – 初始有序度`
                - 最坏情况下，初始状态的有序度是 `0`，所以要进行 `n*(n-1)/2` 次交换
                - 最好情况下，初始状态的有序度是 `n*(n-1)/2`，就不需要进行交换，即`0`次交换
                - 取个中间值 `n*(n-1)/4`，来表示初始有序度既不是很高也不是很低的平均情况
            * 平均情况下，需要 `n*(n-1)/4` 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是`O(n^2)`，所以平均情况下的时间复杂度就是 `O(n^2)`
            * 这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟概率论的定量分析太复杂，不太好用。
    - **插入排序**(Insert Sort)
        + 将数据分为两个区间：已排序区间 和 未排序区间
            * 初始已排序区间就只有第一个元素
        + 插入算法的核心思想是：
            * 取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。
                - 找到插入点之后，插入点之后的元素顺序都向后移动一位，这样才能腾出位置插入
            * 重复这个过程，直到未排序区间中元素为空，算法结束
        + 移动次数也等于逆序度
        + 算法分析
            * 原地排序，`O(1)`
            * 稳定
            * 时间复杂度
                - 最好情况：完全有序，n次比较，不用移动，`O(n)`
                - 最坏情况：完全逆序，每次都要在头部插入数据，数据后移，`O(n^2)`
                - 平均情况时间复杂度：`O(n^2)`
                    + 每次插入数据的平均时间复杂度为`O(n)`(之前数组章节有分析，考虑概率)，因此n次插入平均`O(n^2)`
                    + 数组插入元素的分析，参考前面章节：[05 | 数组：为什么很多编程语言中数组都从0开始编号？]
    - **选择排序**(Selection Sort)
        + 选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间
            * 初始已排序区间为空，和插入排序有所区别
        + 但是选择排序每次会从未排序区间中找到最小的元素，将其放到*已排序区间的末尾*
            * 找出最小元素后，和未排序区间的第一个元素交换位置
        + 算法分析
            * 原地排序，`O(1)`
            * 不稳定
                - 每次找未排序区间中的最小元素 和 前面的元素交换位置，这样破坏了稳定性(查看参考链接中的图示更直观)
            * 时间复杂度
                - 最好、最坏、平均情况，都是`O(n^2)`
                    + 因为每次都要选择剩下最小的元素 和 前面数据交换
    - 解答开篇：插入排序和冒泡排序的时间复杂度相同，都是`O(n^2)`，都是原地排序算法，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？
        + 冒泡排序和插入排序，移动数据的次数是同样的，都等于原始数据的逆序度(假设逆序度为`K`)
            * 冒泡排序需要交换数据，每次交换需要三次赋值操作，需要`3*K`个单位时间
            * 而插入排序，每次移动数据只要一个赋值操作，仅需要`K`个单位时间
        + 所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是`O(n^2)`，但是如果我们希望把性能优化做到极致，那肯定首选插入排序
            * 插入排序的算法思路也有很大的优化空间，上面只是最基础的一种，优化方案如 希尔排序
            * 希尔排序
                - 希尔排序是希尔（Donald Shell）于1959年提出的一种排序算法。
                - 希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序
                - 希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。
                - 实现见参考链接
                - [图解排序算法(二)之希尔排序](https://www.cnblogs.com/chengxiao/p/6104371.html)
    - 这三种排序算法，对于*小规模数据*的排序，用起来非常高效。但是在*大规模数据*排序的时候，这个时间复杂度还是稍微有点高，所以我们更倾向于用下一节要讲的时间复杂度为 `O(nlogn)` 的排序算法
* [12 | 排序（下）：如何用快排思想在O(n)内查找第K大元素？](https://time.geekbang.org/column/article/41913)
    - 上章节的冒泡、插入、选择排序，时间复杂度都是`O(n^2)`，比较高，适合小规模数据的排序
    - 本章节介绍复杂度都为`O(nlogn)`的 *归并排序* 和 *快速排序*，这两种算法适合大规模的数据排序，比上面三种排序算法更常用。
    - 归并排序和快速排序都用到了分治思想
        + 分治算法一般都是用递归来实现的
        + 分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突
    - **归并排序**(Merge Sort)
        + 如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了
        + 要想写出归并排序的代码，先要写出归并排序的递推公式：
            * 递推公式：`merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))` (传入数组len-1，左右边界均闭区间)
            * 终止条件：`p >= r`时，不用再继续分解了
            * 合并函数`merge()`的实现：
                - 新建一个临时空间，依次取两个分区(各自游标i, j)中的数据进行比较，小的数存入到临时空间，并将对应游标+1，最后临时空间数据就是有序的了，将该范围数据拷贝到原数据对应的区间位置
            * 测试示例参考：MergeSort、MergeSort2，[LeetCode/sort/sort_algo_test.go](https://github.com/xiaodongQ/LeetCode/blob/master/sort/sort_algo_test.go)
                - 注意传入下标取到相等
        + 算法分析
            * 稳定排序
                - 取决于`merge()`函数，合并过程中，有相同元素则将前面的数据先放到临时空间中，即可保证相对顺序不变
            * 时间复杂度`O(nlogn)`
                - 归并排序涉及递归，时间复杂度的分析稍微有点复杂
                - 不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式
                - **递归代码的时间复杂度计算：**
                - 假设对 n 个元素进行归并排序需要的时间是 `T(n)`，那分解成两个子数组排序的时间都是 `T(n/2)`。我们知道，`merge()` 函数合并两个有序子数组的时间复杂度是 `O(n)`，归并排序的时间复杂度的计算公式就是：
                    + `T(1) = C；` n=1时，只需要常量级的执行时间，所以表示为C。
                    + `T(n) = 2*T(n/2) + n； n>1` (后面的n是merge)
                    + 展开: `T(n) = 2 * (2*T(n/4)+n/2) + n = 4*T(n/4)+2n = ... = 2^k * T(n/(2^k)) + k*n`
                    + 当`T(n/(2^k))=T(1)`，即`n/(2^k) = 1`，`2^k=n`，此时`k = log2(n)`，此时`T(n) = Cn+nlog2(n)`
                    + 所以用`大O`标记法表示 T(n)就等于`O(nlogn)`，归并排序时间复杂度为`O(nlogn)`
                - 归并排序的执行效率和要排序的原始数据的有序程度无关，最好、最坏、平均情况，均为`O(nlogn)`
            * 空间复杂度`O(n)`
                - 合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间
                - 尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。
                - 临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 `O(n)`
    - **快速排序**(Quick Sort)
        + 如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。
        + 可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了
        + 递推公式
            * `quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)`
            * 终止条件：`p >= r`
        + 归并排序中有一个 `merge()` 合并函数，快速排序中有一个 `partition()` 分区函数
            * 分区函数`partition()`的实现：
                - 取区间最后一个元素作为pivot中心点，开始遍历区间并记录一个游标i，小于pivot的则和游标i所处的记录交换位置，并将游标+1，最后一次将pivot(即最后元素)和游标处元素交换位置
        + 算法分析
            * 原地排序
            * 不稳定
            * 时间复杂度
                - 如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间(*最好情况*)，则快排和归并排序的递推求解公式是一样的，同`O(nlogn)`
                    + `T(1) = C；` `n=1`时，只需要常量级的执行时间，所以表示为C。
                    + `T(n) = 2*T(n/2) + n； n>1`
                - 公式成立的前提是每次分区操作，我们选择的 pivot 都很合适，正好能将大区间对等地一分为二。但实际上这种情况是很难实现的
                - 考虑*最坏情况*，原数组完全有序。则需要进行大约 `n` 次分区操作，才能完成快排的整个过程。每次分区平均要扫描大约 `n/2` 个元素，这种情况下，快排的时间复杂度就从 `O(nlogn)` 退化成了 `O(n^2)`
                - *平均情况*，假设每次分区操作都将区间分成大小为 9:1 的两个小区间，则
                    + `T(1) = C；` n=1时，只需要常量级的执行时间，所以表示为C。
                    + `T(n) = T(n/10) + T(9*n/10) + n； n>1`
                    + 递推求解的过程非常复杂，直接给出下面的结论：
                - `T(n)` 在大部分情况下的时间复杂度都可以做到 `O(nlogn)`，只有在极端情况下，才会退化到 `O(n^2)`
                    + 而且也有很多方法将退化到`O(n^2)`的概率降到很低，后面章节会有介绍
    - 解答开篇：如何用快排思想在`O(n)`时间复杂度内求无序数组中的第 K 大元素
        + 比如，`4， 2， 5， 12， 3` 这样一组数据，第 3 大元素就是 4
        + 选择数组区间 A[0…n-1]的最后一个元素 A[n-1]作为 pivot，对数组 A[0…n-1]原地分区，这样数组就分成了三部分，A[0…p-1]、A[p]、A[p+1…n-1]。
        + 如果 `p+1=K`，那 A[p]就是要求解的元素；如果 `K>p+1`, 说明第 K 大元素出现在 A[p+1…n-1]区间，我们再按照上面的思路递归地在 A[p+1…n-1]这个区间内查找。同理，如果`K<p+1`，那我们就在 A[0…p-1]区间查找
        + 复杂度分析：
            * 第一次分区查找，我们需要对大小为 n 的数组执行分区操作，需要遍历 n 个元素。第二次分区查找，我们只需要对大小为 n/2 的数组执行分区操作，需要遍历 n/2 个元素。依次类推，分区遍历元素的个数分别为、n/2、n/4、n/8、n/16.……直到区间缩小为 1
            * 把每次分区遍历的元素个数加起来，就是：`n+n/2+n/4+n/8+…+1`，等比数列求和得到`2n-1`，因此时间复杂度为`O(n)`
        + 代码查看：[LeetCode/sort/sort_algo_test.go ](https://github.com/xiaodongQ/LeetCode/blob/master/sort/sort_algo_test.go)，搜索：TestFindKTop
        + 暴力方法：
            * 每次取数组最小值，移动到数组最前面，然后在剩下数据中再找最小值移动，依次类推，K次即得到第K大的数。复杂度`O(K*n)`，K比较小时时间复杂度确实为`O(n)`，但若K=n/2或n，则为`O(n^2)`
* [13 | 线性排序：如何根据年龄给100万用户数据排序？](https://time.geekbang.org/column/article/42038)
    - 讲三种时间复杂度是 `O(n)` 的排序算法：桶排序、计数排序、基数排序
    - 因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作**线性排序**（Linear sort）
        + 之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作
        + 这几种排序算法对要排序的数据要求很苛刻，重点是掌握其使用场景
    - **桶排序**(Bucket Sort)
        + 核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了
        + 时间复杂度为`O(n)`，分析：
            * 如果要排序的数据有 n 个，我们把它们*均匀*地划分到 m 个桶内，每个桶里就有 `k=n/m` 个元素。
            * 每个桶内部使用快速排序，时间复杂度为 `O(k * logk)`
            * m 个桶排序的时间复杂度就是 `O(m * k * logk)`，因为 k=n/m，所以整个桶排序的时间复杂度就是 `O(n*log(n/m))`。
            * 当桶的个数 m 接近数据个数 n 时，`log(n/m)` 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 `O(n)`
        + 实际上，桶排序对要排序数据的要求是非常苛刻的
            * 首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序
            * 其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 `O(nlogn)` 的排序算法了
        + 桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。
            * 比如说有 10GB 的订单数据，希望按订单金额（假设金额都是正整数）进行排序，但是内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。
            * 解决：
                - 先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。
                - 将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推
                - 理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，我们就可以将这 100 个小文件*依次*放到内存中，用快排来排序
                - 等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了
            * 不过，订单按金额并不一定是均匀分布的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大。针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，我们就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元…901 元到 1000 元，直到所有的文件都能读入内存为止
    - **计数排序**（Counting sort）
        + 计数排序其实是桶排序的一种特殊情况
        + 当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间
        + 计数排序跟桶排序非常类似，只是桶的大小粒度不一样
        + e.g. 高考查分数系统显示成绩以及所在省的排名。如果你所在的省有 50 万考生，如何通过成绩快速排序得出名次？
            * 考生的满分是 900 分，最小是 0 分，这个数据的范围很小，所以我们可以分成 901 个桶，对应分数从 0 分到 900 分。根据考生的成绩，我们将这 50 万考生划分到这 901 个桶里
            * 桶内的数据都是分数相同的考生，所以并不需要再进行排序。只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了 50 万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是 `O(n)`
        + 如何快速计算出，每个分数的考生在有序数组中对应的存储位置呢？
            * 简化上面的数据：
                - 8个考生，分数在0-5之间，考生分数(`A[8]`)分别为：{2，5，3，0，2，3，0，3}
                - 进行计数排序，6个桶(`C[6]`)分别存分数0-5，则桶内容为：{2, 0, 2, 3, 0, 1} (C[0]存分数为0的考生个数，依次类推)
                - 排序之后的数组为`R[8]`，取桶内各分数计数的项填充，为：{0, 0, 2, 2, 3, 3, 3, 5} (排序后的结果，过程在下面)
            * 思路：
                - 对`C[6]`数组求和，则`C[k]`就为小于等于分数k的考生个数，得到新的`C[6]`为：{2, 2, 4, 7, 7, 8}
            * 从后到前依次扫描数组`A`，扫描到n时就从数组`C`取下标为n的值，e.g. 扫描到3，取`C[3]`得到7，即分数小于等于3的数有7个，即3是数组`R`的第7个元素(下标为6)，3放入数组`R`中，小于等于3的元素就剩下6个，`C[3]`需要减1，变成6，即`C`数组为{2, 2, 4, 6, 7, 8}。完成步骤后`R`为{x, x, x, x, x, x, 3, x}
                - 依此类推，继续从后到前扫描数组`A`：
                - 扫描到0，C[0]得到2，小于等于0的有2个，即0为R的第2个元素，存入到R[1]，并将C[0]减1，`C`变成{1, 2, 4, 6, 7, 8}
                    + 完成步骤后`R`为{x, 0, x, x, x, x, 3, x}
                - 扫描到3，C[3]得到6，为R的第6个元素，存入到R[5]，C[3]减1，`C`变成{1, 2, 4, 5, 7, 8}
                    + 完成步骤后`R`为{x, 0, x, x, x, 3, 3, x}
                - 扫描到2，C[2]得到4，2为R的第4个元素，存入到R[3]，`C`变成{1, 2, 3, 5, 7, 8}
                    + 完成步骤后`R`为{x, 0, x, 2, x, 3, 3, x}
                - 依次扫描完数组`A`，数组`R`就是有序的
                    + 0: `R`{0, 0, x, 2, x, 3, 3, x}，`C`{0, 2, 3, 5, 7, 8}
                    + 3: `R`{0, 0, x, 2, 3, 3, 3, x}，`C`{0, 2, 3, 4, 7, 8}
                    + 5: `R`{0, 0, x, 2, 3, 3, 3, 5}, `C`{0, 2, 3, 4, 7, 7}
                    + 2: `R`{0, 0, 2, 2, 3, 3, 3, 5}, `C`{0, 2, 2, 4, 7, 7}
        + 利用另外一个数组来计数的实现方式，这也是为什么这种排序算法叫计数排序的原因
        + 总结
            * 计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。
            * 而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数
    - **基数排序**（Radix sort）
        + 排序问题：假设我们有 10 万个手机号码(11位手机号)，希望将这 10 万个手机号码从小到大排序，有什么比较快速的排序方法呢？
            * 快排，`O(nlogn)`
            * 桶排序、计数排序，由于11位数据量太大，不合适
        + `O(n)`的排序思路
            * 先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了
            * 根据每一位来排序，可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 `O(n)`
            * 如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 `O(k*n)`。当 k 不大的时候，比如手机号码排序的例子，k 最大就是 11，所以基数排序的时间复杂度就近似于 `O(n)`
        + 有时候要排序的数据并不都是等长的，可以把所有的数据补齐到相同长度，位数不够的可以在后面补“0”
            * e.g. 牛津字典所有英文单词排序，最大长度个45字母，不足45字母的单词，在后面补"0"，不影响原有相对顺序(ASCII字母都大于"0")
        + 总结
            * 基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。
            * 除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到`O(n)` 了
    - 解答开篇：如何根据年龄给 100 万用户排序？
        * 归并、快排，`O(nlogn)`
        * 假设最小1岁，最大年龄120岁，遍历100万用户，按年龄划分到120个桶里，再依次扫描取出即可
            + 桶排序，放到120个桶里，依次扫描保存到有序位置
            + 计数排序(数据范围不大，先新建100万用户的存储空间，再找位置进行填入)：120桶中，只是记录数量，然后用计数排序的方法，从后往前遍历100万用户，算出存放到有序数组的位置进行存储
            + 感觉直接用桶排序即可，不必要用计数排序
    - 思考题：假设我们现在需要对 D，a，F，B，c，A，z这个字符串进行排序，要求将其中所有小写字母都排在大写字母的前面，但小写字母内部和大写字母内部不要求有序。比如经过排序之后为 a，c，z，D，F，B，A，这个如何来实现呢？如果字符串中存储的不仅有大小写字母，还有数字。要将小写字母的放到前面，大写字母放在最后，数字放在中间，不用排序算法，又该怎么解决呢？
        * 桶排序，3个桶，小写、大写、数字，分完后依次读取，空间复杂度`O(3n)`，时间`O(2n)` (若算趋势即为`O(n)`)
        * 快排思想，中心点假设为`Z`，<='Z'的分一个区，>'Z'的分一个区，若还有数字，则<='Z'的再进行一次分区，<='9'(ASCII 'a'>'A')
* [14 | 排序优化：如何实现一个通用的、高性能的排序函数？](https://time.geekbang.org/column/article/42359)
    - 线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法
    - 如果对小规模数据进行排序，可以选择时间复杂度是 `O(n^2)` 的算法；如果对大规模数据进行排序，时间复杂度是 `O(nlogn)` 的算法更加高效
    - 所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 `O(nlogn)` 的排序算法来实现排序函数
        + 归并排序并不是原地排序算法，空间复杂度是 `O(n)`
        + 快速排序比较适合来实现排序函数
    - 快速排序在最坏情况下的时间复杂度是 `O(n^2)`，如何优化快速排序：
        + 如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为 `O(n^2)`
            * 这种 `O(n^2)` 时间复杂度出现的主要原因还是因为分区点选的不够合理
            * 最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多
        + 两个比较常用、比较简单的分区算法
            * 三数取中法
                - 从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点
                - 但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”
            * 随机法
                - 随机法就是每次从要排序的区间中，随机选择一个元素作为分区点
                - 这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的
        + 为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：
            * 第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。
            * 第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。(归纳法？)
    - Glibc 中的 `qsort()`
        + `qsort()` 会优先使用归并排序来排序输入数据，因为归并排序的空间复杂度是` O(n)`，所以对于小数据量的排序，比如 1KB、2KB 等，归并排序额外需要 1KB、2KB 的内存空间，这个问题不大
            * 空间换时间
        + 要排序的数据量比较大的时候，`qsort()` 会改为用快速排序算法来排序
            * `qsort()` 选择分区点的方法就是“三数取中法”(Go里面的排序中的快排部分，也是用三数取中法，对数据量>40时有其他处理)
        + 对于递归太深会导致堆栈溢出的问题，`qsort()` 是通过自己实现一个堆上的栈，手动模拟递归来解决的
        + `qsort()` 并不仅仅用到了归并排序和快速排序，它还用到了插入排序。在快速排序的过程中，当要排序的区间中，元素的个数小于等于 4 时，`qsort()` 就退化为插入排序，不再继续用递归来做快速排序
            * 在小规模数据面前，`O(n^2)` 时间复杂度的算法并不一定比 `O(nlogn)` 的算法执行时间长
            * 在大 O 复杂度表示法中，我们会省略低阶、系数和常数，也就是说，`O(nlogn)` 在没有省略低阶、系数、常数之前可能是 `O(knlogn + c)`，而且 k 和 c 有可能还是一个比较大的数
    - Go中的`Sort` (sort包中)
        + 也是用到快排，除此之外还用到堆排序、插入排序(希尔排序)
        + `func quickSort(data Interface, a, b, maxDepth int)`
            * maxDepth为(数据长度n)：`2 * ceil(lg(n+1))`
                - 即 2 * 以2为底的n+1的对数(不为整数则向上取整)，实现时循环右移一位(即除2)提高效率
                - 用maxDepth来判断排序中，是否需要切换到堆排序
            * 当本次传入数据数据范围`>12`，
            * `func doPivot(data Interface, lo, hi int) (midlo, midhi int)` 分区函数
                - 选取pivot中心点，用三数取中法 `func medianOfThree(data Interface, m1, m0, m2 int)`
                    * m1开始，m0中间，m2结束(`len-1`)
                    * 首先判断m1和m0，若`m1位置的数<m0位置的数`(简写为`m1<m0`)，则交换m1和m0，即保证 `开始m1>=中间m0`
                    * 而后判断开始m1和结束m2，保证 `开始m1<=结束m2`且`开始m1>=中间m0`
                        + 若`m1>m2`则m1和m2要交换位置，然后m1还要和m0再比较一次，因为上一步骤的相对顺序在本次交换之后有可能改变了
                    * 最后结果是：开始m1的位置，存的是三个位置中中间的数，且`中间位置m0<=结束位置m2`
                    * 中心点取m1开始位置，即中间值
                - 分区操作有点复杂。。。考虑性能上的提升做了一些处理，自我觉得可读性很不好
        + Go里面使用排序的方式：
            * `Sort`函数的定义：`func Sort(data Interface)`
                - 用的`Interface`是sort中定义的接口类型(`type Interface interface`)，所以可以传不同类型的数据进行排序
            * 如果不是基本类型，则需要自己实现自定义类型(`struct`)对`Interface`中方法的绑定，其中包含3个方法：
                - `Len() int`，元素个数
                - `Less(i, j int) bool`，判断`data[i]<data[j]`
                - `Swap(i, j int)`，交换位置
            * sort标准库里面实现了几种基本类型的接口
                - `func Ints(a []int) { Sort(IntSlice(a)) }`
                    + 针对int类型的slice排序，`IntSlice`定义为：`type IntSlice []int`
                    + 是否已排序(升序) `func IntsAreSorted(a []int) bool { return IsSorted(IntSlice(a)) }`
                - `func Float64s(a []float64) { Sort(Float64Slice(a)) }`
                    + 针对float64类型的slice排序
                    + 是否已排序 `func Float64sAreSorted(a []float64) bool { return IsSorted(Float64Slice(a)) }`
                - `func Strings(a []string) { Sort(StringSlice(a)) }`
                    + 针对string类型的slice排序，用string的`==`, `>`, `<` (注意`strings.Compare()`主要是为了保证包的完备性，一般比内建的`==`等效率低)
                    + 是否已排序 `func StringsAreSorted(a []string) bool { return IsSorted(StringSlice(a)) }`
            * `Sort`默认是升序排序，若要降序排序，则调整一下：`Less(i, j int) bool`
                - sort包里对这个操作做了封装：
                    + `func Reverse(data Interface) Interface` 会返回一个`return &reverse{data}`
                    + `reverse struct`内部类型对原Interface类型进行封装(将其作为`reverse struct`的唯一成员)，然后将`Less`方法调整后的实现绑定到`reverse`类型上，这样调用`Less`时，就会用到新的`Less`(内部类型中的`Less`会隐藏，可进一步了解：嵌套匿名接口，可用来实现继承)
                    + `Interface`的其他方法(`Len`，`Swap`)，`reverse`还是存在的
                - 使用方式：
                    + `sort.Sort( sort.Reverse(sort.IntSlice(xdIntList)) )`
                    + 用`sort.Reverse`来封装一下原始数据，然后调用`sort.Sort`即可进行降序排序
            * 自定义类型的排序
                - 参考：[go语言的排序、结构体排序](https://www.cnblogs.com/wangchaowei/p/7802811.html)
                - `sort.Sort(XdSlice(a))` 定义一个`XdSlice struct`类型，然后实现接口`Len`、`Less`、`Swap`
* [15 | 二分查找（上）：如何用最省内存的方式实现快速查找功能？](https://time.geekbang.org/column/article/42520)
    - **二分查找**(Binary Search)针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0
    - `n/(2^k) = 1`即 `k = log(n)`时，完成最后元素的查找，即时间复杂度`O(logn)`
        + 二分查找是我们目前为止遇到的第一个时间复杂度为 `O(logn)` 的算法
        + `O(logn)` 这种对数时间复杂度。这是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级 `O(1)` 的算法还要高效
    - 二分查找应用场景的局限性
        + 首先，二分查找依赖的是顺序表结构，简单点说就是数组
            * 不能依赖其他数据结构，如链表，需要支持按下标直接访问元素
        + 其次，二分查找针对的是有序数据
            * 如果数据没有序，我们需要先排序
            * 若针对的是一组静态的数据，没有频繁地插入、删除，我们可以进行一次排序，多次二分查找。这样排序的成本可被均摊，二分查找的边际成本就会比较低。
            * 但是，若数据集合有频繁的插入和删除操作，要想用二分查找，要么每次插入、删除操作之后保证数据仍然有序，要么在每次二分查找之前都先进行排序。针对这种动态数据集合，维护有序的成本是很高的
            * 所以，二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中。
                - 对于动态数据集合的快速查找，后续二叉树中会介绍
        + 再次，数据量太小不适合二分查找
            * 只有数据量比较大的时候，二分查找的优势才会比较明显
            * 不过，如果数据之间的比较操作非常耗时，不管数据量大小，都推荐使用二分查找
        + 最后，数据量太大也不适合二分查找
            * 二分查找的底层需要依赖数组这种数据结构，而数组为了支持随机访问的特性，要求内存空间连续，对内存的要求比较苛刻
    - 解答开篇：如何在 1000 万个整数中快速查找某个整数？(内存限制100MB)
        + 一个整数8字节，1000万则占用 `8*1000*10000`，大概80MB内存未超过限制，对数据在内存中进行排序，而后用二分查找
        + 二分查找底层依赖的是数组，除了数据本身之外，不需要额外存储其他信息，是最省内存空间的存储方式，所以刚好能在限定的内存大小下解决这个问题
            * 后续讲到的二叉树、散列表，均需要比较多的额外内存空间，100MB就不够了
* [16 | 二分查找（下）：如何快速定位IP对应的省份地址？](https://time.geekbang.org/column/article/42733)
    - 通过 IP 地址来查找 IP 归属地的功能，是通过维护一个很大的 IP 地址库来实现的。地址库中包括 IP 地址范围和归属地的对应关系。
        + e.g. 当我们想要查询 202.102.133.13 这个 IP 地址的归属地时，我们就在地址库中搜索，发现这个 IP 地址落在[202.102.133.0, 202.102.133.255]这个地址范围内，那我们就可以将这个 IP 地址范围对应的归属地“山东东营市”显示给用户了
        + 假设我们有 12 万条这样的 IP 区间与归属地的对应关系，如何快速定位出一个 IP 地址的归属地呢？
            * 在庞大的地址库中逐一比对 IP 地址所在的区间，是非常耗时的
    - 二分查找的变形问题(基于最简单的情况变形：数据集有序且不重复)
        + 变体一：查找第一个值等于给定值的元素
        + 变体二：查找最后一个值等于给定值的元素
        + 变体三：查找第一个大于等于给定值的元素
        + 变体四：查找最后一个小于等于给定值的元素
    - 解答开篇：
        + IP 地址可以转化为 32 位的整型数，将起始地址，按照对应的整型值的大小关系，从小到大进行排序
        + 这个问题就可以转化为第四种变形问题“在有序数组中，查找最后一个小于等于某个给定值的元素”了，如果不在，就返回未查找到
* [17 | 跳表：为什么Redis一定要用跳表来实现有序集合？](https://time.geekbang.org/column/article/42896)
    - 二分查找底层依赖的是数组随机访问的特性，但是若数据存储在链表中，就真的无法使用二分查找吗？
    - 实际上，只需对链表稍加改造，就可以支持类似“二分”的查找算法，改造后的数据结构叫做 **跳表**(Skip list)
        + 跳表是一种各方面性能都比较优秀的动态数据结构，可以支持快速的插入、删除、查找操作，写起来也不复杂，甚至可以替代红黑树（Red-black tree）
        + Redis 中的有序集合（Sorted Set）就是用跳表来实现的
    - 理解跳表
        + 对于一个单链表来讲，即便链表中存储的数据是有序的，如果我们要想在其中查找某个数据，也只能从头到尾遍历链表。这样查找效率就会很低，时间复杂度会很高，是 `O(n)`
        + 提高查找效率：
            * 对链表建立一级“索引”：每两个结点提取一个结点到上一级，我们把抽出来的那一级叫作`索引`或`索引层`
            * 如果我们现在要查找某个结点，比如 16。我们可以先在索引层遍历，当遍历到索引层中值为 13 的结点时，我们发现下一个结点是 17，那要查找的结点 16 肯定就在这两个结点之间
            * 然后我们通过索引层结点的 down 指针，下降到原始链表这一层，继续遍历
        + 可以看出：加了一层索引之后，查找一个结点需要遍历的结点个数减少了，也就是说查找效率提高了
            * 再加索引效率还会提升，以空间换时间
        + 这种链表加多级索引的结构，就是跳表
    - 时间复杂度分析(查找的复杂度)
        + 假设链表里有n个数据，每两个结点会抽出一个结点作为上一级索引的结点，那第一级索引的结点个数大约就是 n/2，第二级索引的结点个数大约就是 n/4，第三级索引的结点个数大约就是 n/8，依次类推，也就是说，第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2，那第 k级索引结点的个数就是`n/(2^k)`
        + 假设索引有 h 级，最高级的索引有 2 个结点。通过上面的公式，我们可以得到 `n/(2^h)=2`，从而求得 `h = logn - 1` (以2为底)
        + 如果包含原始链表这一层，整个跳表的高度就是 `logn`
        + 在跳表中查询某个数据的时候，如果每一层都要遍历 m 个结点，那在跳表中查询一个数据的时间复杂度就是 `O(m*logn)`
            * 按照前面这种索引结构，我们每一级索引都最多只需要遍历 3 个结点，也就是说 `m=3`
            * 每一级索引都最多只需要遍历 3 个结点
                - 假设遍历x，遍历到y结点后，发现`x>y`，而`x<z`，则通过y的down指针下降到下一层索引
                - 依次类推每一级索引最多只要遍历3个结点(包含y和z结点)
        + 通过上面的分析，我们得到 m=3，所以在跳表中查询任意数据的时间复杂度就是 `O(logn)`
            * 实现了和二分查找一样的时间复杂度
            * 这种效率的提升，前提是建立了很多索引，是以空间换时间的方式
    - 空间复杂度分析
        + 假设原始链表大小为 n，那第一级索引大约有 n/2 个结点，第二级索引大约有 n/4 个结点，以此类推，每上升一级就减少一半，直到剩下 2 个结点，这是一个等比数列
            * `n/2 + n/4 + ... + 4 + 2 = (n/2)(1-(1/2)^k)/(1-1/2) = n - n/2^k`
            * 其中k，根据等比数列的通项公式 `n/2 * (1/2^(k-1)) = 2`，`n/2 = 2^k`，得到 `k = logn - 1`
            * 所以求和得到：`n - n/2^k = n - 2`
        + 所以，跳表的空间复杂度是 `O(n)`。也就是说，如果将包含 n 个结点的单链表构造成跳表，我们需要额外再用接近 n 个结点的存储空间
        + 前面都是每两个结点抽一个结点到上级索引，如果每三个结点或五个结点，抽一个结点到上级索引，是不是就不用那么多索引结点了呢
            * 第一级索引需要大约 n/3 个结点，第二级索引需要大约 n/9 个结点。每往上一级，索引结点个数都除以 3
            * 为了方便计算，我们假设最高一级的索引结点个数是 1。我们把每级索引的结点个数都写下来，也是一个等比数列
            * `n/3 + n/9 + ... + 9 + 3 + 1 = n/2`
            * 尽管空间复杂度还是 `O(n)`，但比上面的每两个结点抽一个结点的索引构建方法，要减少了一半的索引结点存储空间
        + 实际上，在软件开发中，我们不必太在意索引占用的额外空间。
            * 在讲数据结构和算法时，我们习惯性地把要处理的数据看成整数，但是在实际的软件开发中，原始链表中存储的有可能是很大的对象，
            * 而索引结点只需要存储关键值和几个指针，并不需要存储对象，所以当对象比索引结点大很多时，那索引占用的额外空间就可以忽略了
    - 高效的动态插入和删除
        + 实际上，跳表这个动态数据结构，不仅支持查找操作，还支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 `O(logn)`
        + 分析
            * 插入：需要先找到要插入的位置，查找`O(logn)`，确定好位置后只需要`O(1)`，所以插入的时间复杂度为`O(logn)`
            * 删除：在查找(`O(logn)`)要删除的结点的时候，要获取前驱结点，双向链表则操作只要`O(1)`，所以删除的时间复杂度为`O(logn)`
    - 跳表索引动态更新
        + 不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某2个索引结点之间数据非常多的情况，极端情况下，跳表还会退化成单链表
        + 需要某种手段来维护索引与原始链表大小之间的平衡，如果链表中结点多了，索引结点就相应地增加一些，避免复杂度退化，以及查找、插入、删除操作性能下降
        + 跳表是通过随机函数来维护前面提到的“平衡性”
            * 通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中(从底层第一次索引算做第一层)
            * 随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化
            * (可了解Redis 中关于有序集合的跳表实现)
            * 红黑树、AVL 树这样平衡二叉树，它们是通过左右旋的方式保持左右子树的大小平衡（后面章节会介绍）
    - 解答开篇：为什么 Redis 要用跳表来实现有序集合，而不是红黑树？
        + Redis 中的有序集合是通过跳表来实现的(其实还用到了散列表，后面章节介绍，暂忽略)
        + Redis 中的有序集合支持的核心操作主要有下面这几个：
            * 插入一个数据
            * 删除一个数据
            * 查找一个数据
            * 按照区间查找数据（比如查找值在[100, 356]之间的数据）
            * 迭代输出有序序列
        + 其中，插入、删除、查找以及迭代输出有序序列这几个操作，红黑树也可以完成，时间复杂度跟跳表是一样的。但是，按照区间来查找数据这个操作，红黑树的效率没有跳表高
            * 对于按照区间查找数据这个操作，跳表可以做到 `O(logn)` 的时间复杂度定位区间的起点，然后在原始链表中顺序往后遍历就可以了
        + 另外，比起红黑树来说好懂，可读性好；还有，跳表更加灵活，它可以通过改变索引构建策略，有效平衡执行效率和内存消耗。
        + 不过，跳表也不能完全替代红黑树。
            * 因为红黑树比跳表的出现要早一些，很多编程语言中的 Map 类型都是通过红黑树来实现的。
            * 我们做业务开发的时候，直接拿来用就可以了，不用费劲自己去实现一个红黑树，但是跳表并没有一个现成的实现，所以在开发中，如果你想使用跳表，必须要自己实现。
* [18 | 散列表（上）：Word文档中的单词拼写检查功能是如何实现的？](https://time.geekbang.org/column/article/64233)
    - **散列表**（Hash Table），我们平时也叫它“哈希表”或者“Hash 表”
    - 散列表用的是数组支持按照下标随机访问数据的特性(`O(1)`)，所以散列表其实就是数组的一种扩展，由数组演化而来。
        + 可以说，如果没有数组，就没有散列表
    - 编号转化为数组下标的散列思想：
        + 编号叫做 `键(Key)` 或 `关键字`
        + 编号转化为数组下标的映射方法叫做 `散列函数` 或 `Hash函数`、`哈希函数`
        + 散列函数计算得到的值叫做 `散列值` 或 `Hash值`、`哈希值`
    - 通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。
        + 当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。
    - 散列函数设计的基本要求
        + 散列函数计算得到的散列值是一个非负整数
        + 如果 key1 = key2，那 hash(key1) == hash(key2)
        + 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)
            * 在真实的情况下，要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的
            * 即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种散列冲突
            * 而且，因为数组的存储空间有限，也会加大散列冲突的概率
    - 散列冲突
        + 再好的散列函数也无法避免散列冲突
        + 为了解决散列冲突问题，常用方法有两类：
            * `开放寻址法`（open addressing）
                - 如果出现了散列冲突，就重新探测一个空闲位置，将其插入。
                - 对于探测方法，先讲一个比较简单的探测方法：`线性探测`（Linear Probing）
                    + 往散列表中*插入*数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，则从此位置依次往后找空闲位置。若遍历到尾部都没有找到空闲的位置，再从表头开始找
                    + 在散列表中*查找*元素，通过散列函数求出要查找的值对应的散列值，然后和下标为散列值的元素比较，若相等则找到；若不相等，则继续顺序查找，直到有空闲位置还没有找到，则说明不存在该元素
                    + *删除*元素，找到要删除的元素，置为特殊的标记deleted (不能直接置空，否则会影响查找)，查找时若碰到deleted标记，则继续往下探测
                    + 线性探测法其实存在很大问题。当散列表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。最坏时间复杂度`O(n)`
                - 除了线性探测方法之外，还有另外两种比较经典的探测方法，`二次探测`（Quadratic probing）和`双重散列`（Double hashing）
                - 二次探测（Quadratic probing）
                    + 所谓二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+1^2，hash(key)+2^2……
                - 双重散列（double hashing）
                    + 所谓双重散列，意思就是不仅要使用一个散列函数。我们使用*一组*散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置
                - 不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。用`装载因子`（load factor）来表示空位的多少
                    + 计算公式：`散列表的装载因子=填入表中的元素个数/散列表的长度`
                    + 装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降
            * `链表法`（chaining）
                - 链表法是一种更加常用的散列冲突解决办法
                    + 相比开放寻址法，它要简单很多
                - 在散列表中，每个“`桶`（bucket）”或者“`槽`（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中(可参考链接中的图示，更直观)
                - 当*插入*的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是`O(1)`
                - 当*查找*、*删除*一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除
                    + 这两个操作的时间复杂度跟链表的长度 k 成正比，也就是 `O(k)`
                    + 对于散列比较均匀的散列函数来说，理论上讲，`k=n/m`，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。
    - 解答开篇：Word 文档中单词拼写检查功能是如何实现的？
        + 常用的英文单词有 20 万个左右，假设单词的平均长度是 10 个字母，平均一个单词占用 10 个字节的内存空间，那 20 万英文单词大约占 2MB 的存储空间，就算放大 10 倍也就是 20MB，这个大小完全可以放在内存里面。所以我们可以用散列表来存储整个英文单词词典。
        + 当用户输入某个英文单词时，我们拿用户输入的单词去散列表中查找。如果查到，则说明拼写正确；如果没有查到，则说明拼写可能有误，给予提示。
        + 借助散列表这种数据结构，我们就可以轻松实现快速判断是否存在拼写错误。
    - 思考题
        + 1. 假设我们有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？
            * 遍历 10 万条数据，以 URL 为 key，访问次数为 value，存入散列表，同时记录下访问次数的最大值 K，时间复杂度 `O(N)`。
            * 如果K不是很大，可以使用桶排序，时间复杂度`O(N)`。如果K非常大（比如大于10万），就使用快速排序，复杂度`O(NlogN)`。
        + 2. 有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？
            * 以第一个字符串数组构建散列表，key 为字符串，value 为出现次数。再遍历第二个字符串数组，以字符串为 key 在散列表中查找，如果 value 大于零，说明存在相同字符串，另外保存起来。时间复杂度 `O(N)`
* [19 | 散列表（中）：如何打造一个工业级水平的散列表？](https://time.geekbang.org/column/article/64586)
    - 根据上章节学习可知，散列表的查询效率并不能笼统地说成是 `O(1)`。它跟散列函数、装载因子、散列冲突等都有关系
        + 如果散列函数设计得不好，或者装载因子过高，都可能导致散列冲突发生的概率升高，查询效率下降
    - 散列表碰撞攻击
        + 在极端情况下，有些恶意的攻击者，还有可能通过精心构造的数据，使得所有的数据经过散列函数之后，都散列到同一个槽里。若用的是基于链表的方式解决散列冲突，查找的时间复杂度就从`O(1)`退化到`O(n)`了
        + 如果散列表中有 10 万个数据，退化后的散列表查询的效率就下降了 10 万倍。更直接点说，如果之前运行 100 次查询只需要 0.1 秒，那现在就需要 1 万秒
        + 这样就有可能因为查询操作消耗大量 CPU 或者线程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的
    - 那么如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？
        + 首先，散列函数的设计不能太复杂
        + 其次，散列函数生成的值要尽可能随机并且均匀分布，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况
    - 散列函数的设计方法
        + 数据分析法、直接寻址法、平方取中法、折叠法、随机数法等
        + 关于散列函数的设计，我们要尽可能让散列后的值随机且均匀分布，这样会尽可能地减少散列冲突，即便冲突之后，分配到每个槽内的数据也比较均匀。
        + 除此之外，散列函数的设计也不能太复杂，太复杂就会太耗时间，也会影响散列表的性能。
    - 装载因子过大怎么办？
        + 针对散列表，当装载因子过大时，我们也可以进行动态扩容，重新申请一个更大的散列表，将数据搬移到这个新散列表中
            * 但是，针对散列表的扩容，数据搬移操作比数组要复杂很多。因为散列表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置
        + 复杂度分析
            * 插入一个数据，`最好情况`下，不需要扩容，最好时间复杂度是 `O(1)`
            * `最坏情况`下，散列表装载因子过高，启动扩容需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是`O(n)`
            * 用摊还分析法，`均摊情况`下，时间复杂度接近最好情况，就是 `O(1)`
        + 对于动态散列表，随着数据的删除，散列表中的数据会越来越少，空闲空间会越来越多。如果我们对空间消耗非常敏感，我们可以在装载因子小于某个值之后，启动动态缩容；当然，如果我们更加在意执行效率，能够容忍多消耗一点内存空间，那就可以不用费劲来缩容了
        + 当散列表的装载因子超过某个阈值时，就需要进行扩容。装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重
            * 装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1
    - 如何避免低效地扩容？
        + 为了解决一次性扩容耗时过多的情况，我们可以将扩容操作穿插在插入操作的过程中，分批完成。当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中
        + 当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。
        + 对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找
        + 通过这样均摊的方法，将一次性扩容的代价，均摊到多次插入操作中，就避免了一次性扩容耗时过多的情况。这种实现方式，任何情况下，插入一个数据的时间复杂度都是 `O(1)`。
    - 如何选择冲突解决方法？
        + 上一节讲了两种主要的散列冲突的解决办法：`开放寻址法`和`链表法`。
            * 这两种冲突解决办法在实际的软件开发中都非常常用
            * 比如，Java 中 `LinkedHashMap` 就采用了链表法解决冲突
            * `ThreadLocalMap` 是通过线性探测的开放寻址法来解决冲突
        + 两种方法的优劣势和适用场景
        + 开放寻址法
            * 开放寻址法的优点
                - 开放寻址法不像链表法需要拉很多链表，其散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度
                - 而且，这种方法实现的散列表，序列化起来比较简单(链表法包含指针，序列化起来就没那么容易)
            * 开放寻址法的缺点
                - 用开放寻址法解决冲突的散列表，删除数据的时候比较麻烦，需要特殊标记已经删除掉的数据
                - 而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，装载因子的上限不能太大。这也导致这种方法比链表法更浪费内存空间
            * 总结：
                - 当数据量比较小、装载因子小的时候，适合采用开放寻址法。
                - 这也是 Java 中的`ThreadLocalMap`使用开放寻址法解决散列冲突的原因
        + 链表法
            * 优点
                - 链表法对内存的利用率比开放寻址法要高。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好
                - 链表法比起开放寻址法，对大装载因子的容忍度更高
                    + 开放寻址法只能适用装载因子小于 1 的情况。接近 1 时(存放散列表的数组空间快满了)，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多
                    + 但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10(链表法每个桶/槽对应一条链表)，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多
            * 缺点
                - 链表因为要存储指针，所以对于比较小的对象的存储，是比较消耗内存的，还有可能会让内存的消耗翻倍
                    + 当然，如果我们存储的是大对象，也就是说要存储的对象的大小远远大于一个指针的大小（4 个字节或者 8 个字节），那链表中指针的内存消耗在大对象面前就可以忽略了
                - 而且，因为链表中的结点是零散分布在内存中的，不是连续的，所以对 CPU 缓存是不友好的，这方面对于执行效率也有一定的影响
            * 实际上，我们对链表法稍加改造，可以实现一个更加高效的散列表。
                - 那就是，我们将链表法中的链表改造为其他高效的动态数据结构，比如跳表、红黑树
                - 这样，即便出现散列冲突，极端情况下，所有的数据都散列到同一个桶内，那最终退化成的散列表的查找时间也只不过是 `O(logn)`。这样也就有效避免了前面讲到的散列碰撞攻击
            * 总结：
                - 基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，
                - 而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表
    - 工业级散列表举例分析(Java 中的 `HashMap`)
        + 1. 初始大小
            * HashMap 默认的初始大小是 16，当然这个默认值是可以设置的，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数，这样会大大提高 `HashMap` 的性能
        + 2. 装载因子和动态扩容
            * 最大装载因子默认是 `0.75`，当 `HashMap` 中元素个数超过 `0.75*capacity`（capacity 表示散列表的容量）的时候，就会启动扩容，每次扩容都会扩容为原来的两倍大小
        + 3. 散列冲突解决方法
            * HashMap 底层采用链表法来解决冲突。即使负载因子和散列函数设计得再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响 HashMap 的性能
            * 于是，在 JDK1.8 版本中，为了对 HashMap 做进一步优化，引入了红黑树。而当链表长度太长（默认超过 8）时，链表就转换为红黑树。
                - 可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。
        + 4. 散列函数
            * 散列函数的设计并不复杂，追求的是简单高效、分布均匀
            * `int hash(Object key) { int h = key.hashCode()； return (h ^ (h >>> 16)) & (capicity -1); }`
                - capicity表示散列表的大小
                - hashCode() 返回的是 Java 对象的 hash code
    - 解答开篇：如何设计的一个工业级的散列函数？
        + 首先思考，何为一个工业级的散列表？工业级的散列表应该具有哪些特性？
            * 支持快速的查询、插入、删除操作；
            * 内存占用合理，不能浪费过多的内存空间；
            * 性能稳定，极端情况下，散列表的性能也不会退化到无法接受的情况。
        + 为了实现这样一个散列表，从这三个方面来考虑设计思路：
            * 设计一个合适的散列函数；
            * 定义装载因子阈值，并且设计动态扩容策略；
            * 选择合适的散列冲突解决方法。
        + 具体如何选择，还要结合具体的业务场景、具体的业务数据来具体分析。
    - 课后思考：在你熟悉的编程语言中，哪些数据类型底层是基于散列表实现的？散列函数是如何设计的？散列冲突是通过哪种方法解决的？是否支持动态扩容呢？
        + [golang map源码详解](https://juejin.im/entry/5a1e4bcd6fb9a045090942d8)
        + [STL源码剖析 [容器]（十六）[stl_map.h]](https://blog.csdn.net/langb2014/article/details/48131975?utm_medium=distribute.pc_relevant.none-task-blog-baidujs-1)
* [20 | 散列表（下）：为什么散列表和链表经常会一起使用？](https://time.geekbang.org/column/article/64858)
    - 有两种数据结构，散列表和链表，经常会被放在一起使用
    - 前面的章节涉及的散列表和链表的组合使用：
        + 链表那一节讲到如何用链表来实现 LRU 缓存淘汰算法，但是链表实现的 LRU 缓存淘汰算法的时间复杂度是 `O(n)`，当时提到了，通过散列表可以将这个时间复杂度降低到 `O(1)`。
        + 在跳表那一节提到 Redis 的有序集合是使用跳表来实现的，跳表可以看作一种改进版的链表。也提到Redis 有序集合不仅使用了跳表，还用到了散列表
        + 除此之外，如果你熟悉 Java 编程语言，你会发现 LinkedHashMap 这样一个常用的容器，也用到了散列表和链表两种数据结构
    - LRU 缓存淘汰算法
        + 回顾之前说的实现：需要维护一个按照访问时间从大到小有序排列的链表结构
            * 因为缓存大小有限，当缓存空间不够，需要淘汰一个数据的时候，就直接将链表头部的结点删除
            * 当要缓存某个数据的时候，先在链表中查找这个数据。如果没有找到，则直接将数据放到链表的尾部；如果找到了，我们就把它移动到链表的尾部。因为查找数据需要遍历链表，所以单纯用链表实现的 LRU 缓存淘汰算法的时间复杂很高，是 `O(n)`
        + 总结一下一个缓存（cache）系统主要包含下面这几个操作：
            * 往缓存中添加一个数据；
            * 从缓存中删除一个数据；
            * 在缓存中查找一个数据。
        + 这三个操作都要涉及“查找”操作，如果单纯地采用链表的话，时间复杂度只能是 `O(n)`。
            * 如果我们将散列表和链表两种数据结构组合使用，可以将这三个操作的时间复杂度都降低到 `O(1)`
        + 具体结构：
            * 使用`双向链表`存储数据，链表每个结点包含`存储数据`（data）、`前驱指针`（prev）、`后继指针`（next）之外，还新增了一个特殊的字段 `hnext`
            * 因为此处的散列表是通过链表法解决散列冲突的，所以每个结点会在两条链中(双向链表 和 散列表中的链)
            * `hnext` 指针是为了将结点串在散列表的拉链中
        + 查找
            * `O(1)`，当找到数据之后，还需要将它移动到双向链表的尾部
        + 删除
            * 找到数据所在的结点，然后将结点删除，查找`O(1)`，由于是双向链表，通过前驱指针获取到前驱结点，因此删除结点`O(1)`
        + 添加
            * 先看这个数据是否已经在缓存中。
            * 如果已经在其中，需要将其移动到双向链表的尾部；
            * 如果不在其中，还要看缓存有没有满，如果满了，则将双向链表头部的结点删除，然后再将数据放到链表的尾部；如果没有满，就直接将数据放到链表的尾部
* [21 | 哈希算法（上）：如何防止数据库中的用户信息被脱库？](https://time.geekbang.org/column/article/65312)
    - 哈希(Hash、散列)定义和原理：将任意长度的二进制值串映射为固定长度的二进制值串，这个映射的规则就是哈希算法，而通过原始数据映射之后得到的二进制值串就是哈希值。
    - 需要满足的几点要求：
        + 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；
        + 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同；
        + 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；
        + 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。
    - 哈希算法的应用非常非常多，链接选择了最常见的七个，分别是：
        + 安全加密、唯一标识、数据校验、散列函数、负载均衡、数据分片、分布式存储。
    - 应用一：安全加密
        + 最常用于加密的哈希算法是 `MD5`（MD5 Message-Digest Algorithm，MD5 消息摘要算法）和 `SHA`（Secure Hash Algorithm，安全散列算法）
        + 除了这两个之外，当然还有很多其他加密算法，比如 `DES`（Data Encryption Standard，数据加密标准）、`AES`（Advanced Encryption Standard，高级加密标准）
        + 实际上，不管是什么哈希算法，我们只能尽量减少碰撞冲突的概率，理论上是没办法做到完全不冲突的
            * 基于鸽巢原理（也叫抽屉原理）：如果有 10 个鸽巢，有 11 只鸽子，那肯定有 1 个鸽巢中的鸽子数量多于 1 个，换句话说就是，肯定有 2 只鸽子在 1 个鸽巢内
            * 哈希算法产生的哈希值的长度是固定且有限的，如MD5的哈希值是固定的128位字符串，最多能表示 `2^128` 个数据，而我们要哈希的数据是无穷的，基于鸽巢原理，若要对`2^128+1`个数据求哈希，就必然会存在哈希值相同的情况
            * 即便哈希算法存在冲突，但是在有限的时间和资源下，哈希算法还是被很难破解的
    - 应用二：唯一标识
        + 如果要在海量的图库中，搜索一张图是否存在，我们不能单纯地用图片的元信息（比如图片名称）来比对，因为有可能存在名称相同但图片内容不同，或者名称不同图片内容相同的情况。那我们该如何搜索呢？
        + 可以给每一个图片取一个唯一标识，或者说信息摘要
            * 比如，我们可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识
            * 为了进一步提高效率，把每个图片的唯一标识，和相应的图片文件在图库中的路径信息，都存储在散列表中
            * 当要查看某个图片是不是在图库中的时候，我们先通过哈希算法对这个图片取唯一标识，然后在散列表中查找是否存在这个唯一标识
    - 应用三：数据校验
        + 电驴这样的BT下载软件基于P2P协议，如从多个机器上并行下载一个2GB的电影，这个电影文件可能会被分割成很多文件块（比如可以分成 100 块，每块大约 20MB）。等所有的文件块都下载完成之后，再组装成一个完整的电影文件就行了
            * 由于网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的
            * 为了校验文件块的安全、正确、完整，有多种校验方法。可以通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中
            * 当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对，如果不同则说明文件块不完整或被篡改，需要重新从其他宿主机下载该文件块
    - 应用四：散列函数
        + 散列函数也是哈希算法的一种应用
        + 相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决
            * 不仅如此，散列函数对于散列算法计算得到的值，是否能反向解密也并不关心
            * 散列函数中用到的散列算法，更加关注散列后的值是否能平均分布，也就是，一组数据是否能均匀地散列在各个槽中
            * 除此之外，散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率
* [22 | 哈希算法（下）：哈希算法在分布式系统中有哪些应用？](https://time.geekbang.org/column/article/67388)
    - 上一节讲了哈希算法的四个应用，它们分别是：安全加密、数据校验、唯一标识、散列函数。本节再来看剩余三种应用：
        + 负载均衡、数据分片、分布式存储
        + 这三个应用都跟分布式系统有关
    - 应用五：负载均衡
        + 负载均衡算法有很多，比如轮询、随机、加权轮询等
        + 那如何才能实现一个`会话粘滞`（session sticky）的负载均衡算法呢？也就是说，我们需要*在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上*
        * 最直接的方法就是，维护一张映射关系表，这张表的内容是客户端 IP 地址或者会话 ID 与服务器编号的映射关系
            * 客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。
            * 这种方法简单直观，但也有几个弊端：
                - 如果客户端很多，映射表可能会很大，比较浪费内存空间；
                - 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大；
        + 借助哈希算法，这些问题都可以非常完美地解决。
            * 可以通过哈希算法，对客户端IP地址或者会话ID计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号
                - 相同的客户端IP算出的哈希值是一样的，这样列表大小比较稳定时就会路由到同一个服务器上，即便变化也能大体上路由到同一个目的地址
            * 这样，就可以把同一个 IP 过来的所有请求，都路由到同一个后端服务器上
    - 应用六：数据分片
        + 1. 如何统计“搜索关键词”出现的次数？
            * 假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？
            * 有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长
            * 可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度
                - 用 n 台机器并行处理
                - 从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟n取模，最终得到的值，就是应该被分配到的机器编号(循环读取文件中每个搜索记录，对其hash后对n求余，而后将该记录放到余数对应机器)
                - 这样，哈希值相同的搜索关键词就被分配到了同一个机器上
                - 每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果
                - 实际上，这里的处理过程也是 `MapReduce` 的基本设计思想
        + 2. 如何快速判断图片是否在图库中？
            * 上节中给出了一种方法：对图片部分数据求hash后构建散列表，进行索引
            * 假设现在我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的
                - 因为单台机器的内存有限，而1亿张图片构建散列表显然远远超过了单台机器的内存上限，1亿(`10^8`)，假设hash值16字节，单独存hash就需要1.6GB内存，还没算散列表实现和路径等其他信息
            * 同样可以对数据进行分片，然后采用多机处理。
                - 准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。
                - 当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模
                - 假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找。
            * 估算一下，给这 1 亿张图片构建散列表大约需要多少台机器：
                - 散列表中每个数据单元包含两个信息，*哈希值和图片文件的路径*。假设我们通过 MD5 来计算哈希值，那长度就是 128 比特，也就是 16 字节。文件路径长度的上限是256字节，我们可以假设平均长度是128字节。如果我们用`链表法`来解决冲突，那还需要存储指针，指针只占用 8 字节。所以，散列表中每个数据单元就占用 152 字节（这里只是估算，并不准确）。
                - 假设一台机器的内存大小为 2GB，散列表的装载因子为 0.75，那一台机器可以给大约 1000 万（`2GB*0.75/152`）张图片构建散列表
                - 所以，如果要对 1 亿张图片构建索引，需要大约十几台机器
                - 在工程中，这种估算还是很重要的，能让我们事先对需要投入的资源、资金有个大概的了解，能更好地评估解决方案的可行性。
            * 实际上，针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU 等资源的限制。
    - 应用七：分布式存储
        + 现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存
        + 由于有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上
        + 该如何决定将哪个数据放到哪个机器上呢？
            * 前面数据分片的思想：即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号
            * 但是，如果数据增多，原来的10个机器已经无法承受了，那就需要扩容了，假设扩容到11个机器，原来的数据通过10来取模，而现在通过11取模分配到另外的机器，会导致之前的缓存失效。这样，所有的数据请求都会穿透缓存，直接去请求数据库。这样就可能发生雪崩效应，压垮数据库
        + 需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。使用：`一致性哈希算法`
            * 假设我们有 k 个机器，数据的哈希值的范围是[0, MAX]。我们将整个范围划分成 m 个小区间（m 远大于 k），每个机器负责 `m/k` 个小区间
            * 当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中
            * 这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。
            * 除了上面讲到的分布式缓存，实际上，一致性哈希算法的应用非常广泛，在很多分布式存储系统中，都可以见到一致性哈希算法的影子
        * 关于一致性哈希，可参考：[白话解析：一致性哈希算法 consistent hashing](http://www.zsythink.net/archives/1182)
            - 链接里也展示了哈希后对机器数取模的演化，机器数量变化后缓存失效雪崩的情况
            - 上面的方法是对服务器的数量进行取模，而一致性哈希算法是对`2^32`取模
            - 通过`hash（服务器的IP地址）% 2^32`把缓存服务器映射到hash环上，使用同样的方法(`hash（图片片段数据） %  2^32`)，我们也可以将需要缓存的对象映射到hash环上
            - 一致性哈希算法判断一个对象应该被缓存到哪台服务器上：
                + 将缓存服务器与被缓存对象都映射到hash环上以后，从被缓存对象的位置出发，沿顺时针方向遇到的第一个服务器，就是当前对象将要缓存于的服务器
                + 由于被缓存对象与服务器hash后的值是固定的，所以，在服务器不变的情况下，一张图片必定会被缓存到固定的服务器上，那么，当下次想要访问这张图片时，只要再次使用相同的算法进行计算，即可算出这个图片被缓存在哪个服务器上，直接去对应的服务器查找对应的图片即可。
                + 当某机器故障或者新增机器时，依旧是在hash环上按顺时针的方式判断缓存到哪台服务器上
            - 防止hash环的偏斜：
                + 在实际的映射中，服务器可能会被映射得并不均匀，极端情况下顺指针方向上是同一个服务器hash结点，这种情况被称为hash环的偏斜
                + 通过将现有的物理节点通过虚拟的方法复制出来，这些由实际节点虚拟复制而来的节点被称为"虚拟节点"。
                + "虚拟节点"是"实际节点"（实际的物理服务器）在hash环上的复制品,一个实际节点可以对应多个虚拟节点。引入虚拟节点的概念后，缓存的分布就均衡多了
            - 可以参考链接中的图示，更清晰直观
    - 总结：
        + 在负载均衡应用中，利用哈希算法替代映射表，可以实现一个会话粘滞的负载均衡策略。
        + 在数据分片应用中，通过哈希算法对处理的海量数据进行分片，多机分布式处理，可以突破单机资源的限制。
        + 在分布式存储应用中，利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题
    - 哈希算法还有很多其他的应用，比如网络协议中的 CRC 校验、Git commit id 等等
* [23 | 二叉树基础（上）：什么样的二叉树适合用数组来存储？](https://time.geekbang.org/column/article/67856)
    - `树`(Tree)，非线性表结构
    - 概念
        + 每个元素叫做`节点`
        + `父节点`、`子节点`、`兄弟节点`
        + 没有父节点的节点叫作`根节点`，没有子节点的节点叫作`叶子节点`或者`叶节点`
        + `高度`（Height）
            * 结点的高度：结点到叶子结点的最长路径(边数)
        + `深度`（Depth）
            * 结点的深度：根结点到这个节点所经历的边的个数
        + `层`（Level）
            * 结点的层数：结点的深度 + 1
        + `树的高度 = 根节点的高度`
    - `二叉树`（Binary Tree）
        + 树结构多种多样，不过最常用还是二叉树
        + 二叉树每个节点最多有两个子节点：`左子节点`和`右子节点`
        + 叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作`满二叉树`
        + 叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫作`完全二叉树`
            * 链接中列出了几个树的图示，能更直观地区分是否为完全二叉树
        + 如何表示（或者存储）一棵二叉树？
            * 有两种方法，一种是基于指针或者引用的二叉`链式存储法`，一种是基于数组的`顺序存储法`
        + `链式存储法`
            * 每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针
            * 这种存储方式比较常用。大部分二叉树代码都是通过这种结构来实现的
        + `顺序存储法`
            * 如果节点 X 存储在数组中下标为 `i` 的位置，下标为 `2 * i` 的位置存储的就是左子节点，下标为 `2 * i + 1` 的位置存储的就是右子节点。反过来，下标为 `i/2` 的位置存储就是它的父节点
            * 通过这种方式，我们只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为 1 的位置），这样就可以通过下标计算，把整棵树都串起来
                - 数组存储时，完全二叉树仅仅“浪费”了一个下标为 0 的存储位置
                - 如果是非完全二叉树，其实会浪费比较多的数组存储空间。
            * 所以，如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。
                - 因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。
                - 这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。
                - (堆其实就是一种完全二叉树，最常用的存储方式就是数组)
        + 二叉树的遍历
            * `前序遍历`、`中序遍历`和`后序遍历`
                - 其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序
                - 此外，还有`按层遍历`
            * `前序遍历`是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树
                - `preOrder(Node* root){ if(root==NULL) return; print root; preOrder(root->left); preOrder(root->right); }`
            * `中序遍历`是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树
                - `inOrder(Node* root){ if(root==NULL) return; inOrder(root->left); print root; inOrder(root->right); }`
            * `后序遍历`是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身
                - `postOrder(Node* root){ if(root==NULL) return; postOrder(root->left); postOrder(root->right); print root; }`
            * 实际上，二叉树的前、中、后序遍历就是一个递归的过程
            * 时间复杂度
                - 每个节点最多会被访问两次(函数递归的出栈和入栈)，所以遍历操作的时间复杂度，跟节点的个数 n 成正比，也就是说二叉树遍历的时间复杂度是`O(n)`
            * `按层遍历`
                - [二叉树按层遍历](https://blog.csdn.net/chenmeng930601/article/details/74330545)
                - 方法1：使用一个队列实现，需要记录队列中当前层的节点个数。
                    + 从第一层(根节点)开始，入队，n=1
                    + 出队n(此时为1)个节点，获取其左、右子树，进行入队，n=2(假设两个子树都有) (此时已完成了第一层遍历)
                    + 出队n=2个节点，并将出队的每个节点的左、右子树入队，n=4(假设两个子树都有) (此时已完成第二层遍历)
                    + 依次出队后将子树入队，直到某次入队的数量为0，则说明每层都遍历完了
                    + 空间复杂度`O(n)`
                    + 时间复杂度`O(n)`，和前面的遍历一样，递归处理每个节点时函数出栈入栈，最多2次
                - 方法2:使用两个队列实现
                    + 根节点入queue1
                    + 出queue1，并将其左、右子树入queue2 (结束后queue1为空)
                    + 出queue2，并将各节点左右子树入queue1 (结束后queue2为空)
                    + 循环交替操作，直到入队数量为0，则说明每层都遍历完了
                    + 空间复杂度`O(2n)`，即`O(n)`
                    + 时间复杂度`O(n)`同方法1
* [24 | 二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树？](https://time.geekbang.org/column/article/68334)
    - 本节介绍一种特殊的二叉树：`二叉查找树`(Binary Search Tree)
    - 二叉查找树最大的特点就是，支持动态数据集合的快速插入、删除、查找操作
        + 前面章节说过，散列表也是支持这些操作的，并且散列表的这些操作比二叉查找树更高效，时间复杂度是 `O(1)`
        + 既然有了这么高效的散列表，使用二叉树的地方是不是都可以替换成散列表呢？有没有哪些地方是散列表做不了，必须要用二叉树来做的呢？
    - 二叉查找树是二叉树中最常用的一种类型，也叫`二叉搜索树`。
        + 二叉查找树要求：在树中的任意一个节点，其左子树中的每个节点的值(包含子树的子树)，都要小于这个节点的值，而右子树节点的值都大于这个节点的值
        + 二叉查找树的`查找`操作
            * 先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找
        + 二叉查找树的`插入`操作
            * 类似查找操作。新插入的数据一般都是在叶子节点上，所以只需要从根节点开始，依次比较要插入的数据和节点的大小关系
            * 如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，比节点数据小则判断操作左子树。
        + 二叉查找树的`删除`操作
            * 相对于查找和插入，删除比较复杂。针对要删除节点的子节点个数的不同，我们需要分三种情况来处理。
                - 第一种情况是，如果要删除的节点*没有*子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null
                - 第二种情况是，如果要删除的节点只有*一个*子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了
                - 第三种情况是，如果要删除的节点有*两个*子节点，这就比较复杂了。
                    + 需要找到这个节点的*右子树*中的*最小节点*，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。(见参考链接的图示)
            * 实际上，关于二叉查找树的删除操作，还有个非常简单、取巧的方法，就是单纯将要删除的节点标记为“已删除”，但是并不真正从树中将这个节点去掉。
                - 这样原本删除的节点还需要存储在内存中，比较浪费内存空间，但是删除操作就变得简单了很多。
                - 而且，这种处理方法也并没有增加插入、查找操作代码实现的难度。
        + 除了插入、删除、查找操作之外，二叉查找树中还可以支持快速地查找最大节点和最小节点、前驱节点和后继节点
        + 二叉查找树除了支持上面几个操作之外，还有一个重要的特性：就是**中序遍历二叉查找树**，可以输出有序的数据序列，时间复杂度是 `O(n)`，非常高效
            * 因此，二叉查找树也叫作`二叉排序树`
        + 代码操作：[tree_algo_test.go](https://github.com/xiaodongQ/LeetCode/blob/master/tree/tree_algo_test.go)
    - 支持重复数据的二叉查找树
        + 很多时候，在实际的软件开发中，我们在二叉查找树中存储的是一个包含很多字段的对象，利用对象的某个字段作为键值（key）来构建二叉查找树。我们把对象中的其他字段叫作`卫星数据`
        + 第一种方法比较容易。
            * 二叉查找树中每一个节点不仅存储一个数据，通过链表和支持动态扩容的数组等数据结构，把值相同的数据都存储在同一个节点上
        + 第二种方法比较不好理解，不过更加优雅
            * 每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树(也就是说，把这个新插入的数据当作大于这个节点的值来处理)
            * 当要`查找`数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来
            * 对于`删除`操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。
    - 二叉查找树的时间复杂度分析
        + 对于同一组数据，构造不同的二叉查找树。它们的查找、插入、删除操作的执行效率都是不一样的
            * 如图示中的一根长链条，根节点的左右子树极度不平衡，已经退化成了链表，所以查找的时间复杂度就变成了`O(n)`
        + 上面分析了一种最糟糕的情况，现在来分析一个最理想的情况，二叉查找树是一棵完全二叉树（或满二叉树）
            * 不管操作是插入、删除还是查找，时间复杂度其实都跟树的高度成正比，也就是 `O(height)`
        + 现在问题就转变成另外一个了，也就是，如何求一棵包含 n 个节点的完全二叉树的高度？
            * 包含 n 个节点的完全二叉树中，第一层包含 `1` 个节点，第二层包含 `2` 个节点，第三层包含 `4` 个节点，依次类推，下面一层节点个数是上一层的 2 倍，第 K 层包含的节点个数就是 `2^(K-1)`
            * 不过，对于完全二叉树来说，最后一层的节点个数有点儿不遵守上面的规律了。它包含的节点个数在 `1` 个到 `2^(L-1)` 个之间（我们假设最大层数是 L）
            * 所以结点个数n满足：`n >= 1+2+4+8+...+2^(L-2)+1` && `n <= 1+2+4+8+...+2^(L-2)+2^(L-1)`
            * 借助等比数列求和得到：L 的范围是`[log2(n+1), log2(n) +1]`
                - `2^(L-1) - 1 + 1 <= n`，则 `L <= log2(n) + 1`
                - `2^L - 1 >= n`，则 `L >= log2(n+1)`
            * 完全二叉树的层数小于等于 `log2(n) +1`，也就是说，完全二叉树的高度小于等于 `log2(n)` (高度为 最大层数-1)
        + 显然，极度不平衡的二叉查找树，它的查找性能肯定不能满足我们的需求。需要构建一种不管怎么删除、插入数据，在任何时候，都能保持任意节点左右子树都比较平衡的二叉查找树：`平衡二叉查找树`
            * 平衡二叉查找树的高度接近 `logn`，所以插入、删除、查找操作的时间复杂度也比较稳定，是 `O(logn)`
    - 解答开篇：散列表的插入、删除、查找操作的时间复杂度可以做到常量级的`O(1)`，非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 `O(logn)`，相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢？
        + 第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 `O(n)` 的时间复杂度内，输出有序的数据序列。
        + 第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 `O(logn)`。
        + 第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 `logn`小，所以实际的查找速度可能不一定比 `O(logn)` 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
        + 第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。
            * 比如散列函数的设计、冲突解决办法、扩容、缩容等。
            * 平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定
        + 最后，为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。
        + 综合这几点，平衡二叉查找树在某些方面还是优于散列表的，所以，这两者的存在并不冲突。
            * 我们在实际的开发过程中，需要结合具体的需求来选择使用哪一个
    - 思考：如何通过编程，求出一棵给定二叉树的确切高度
        + 根节点高度 = `max(左子树高度, 右子树高度) + 1`
* [25 | 红黑树（上）：为什么工程中都用红黑树这种二叉树？](https://time.geekbang.org/column/article/68638)
    - 二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于 `log2(n)` 的情况，从而导致各个操作的效率下降。
        + 极端情况下，二叉树会退化为链表，时间复杂度会退化到 `O(n)`。
        + 需要设计一种平衡二叉查找树，解决这个复杂度退化的问题
        + 在工程中，很多用到平衡二叉查找树的地方都会用`红黑树`
    - `平衡二叉查找树`
        + 平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于 1。
            * 完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树
        + 平衡二叉查找树不仅满足上面平衡二叉树的定义，还满足二叉查找树的特点
        + 最先被发明的平衡二叉查找树是`AVL` 树，它严格符合我刚讲到的平衡二叉查找树的定义，即任何节点的左右子树高度相差不超过 1，是一种高度平衡的二叉查找树
            * [AVL树](https://zh.wikipedia.org/wiki/AVL%E6%A0%91)
            * 在计算机科学中，AVL树是最早被发明的自平衡二叉查找树。在AVL树中，任一节点对应的两棵子树的最大高度差为1，因此它也被称为高度平衡树。查找、插入和删除在平均和最坏情况下的时间复杂度都是`O(logn)`
            * 增加和删除元素的操作则可能需要借由一次或多次树旋转，以实现树的重新平衡
            * AVL树得名于它的发明者G. M. Adelson-Velsky和Evgenii Landis，他们在1962年的论文《An algorithm for the organization of information》中公开了这一数据结构
        + 但是很多平衡二叉查找树其实并没有严格符合上面的定义（树中任意一个节点的左右子树的高度相差不能大于 1），如红黑树，它从根节点到各个叶子节点的最长路径，可能比最短路径大一倍
            * 如果我们现在设计一个新的平衡二叉查找树，只要树的高度不比 `log2(n)` 大很多（比如树的高度仍然是对数量级的），尽管它不符合我们前面讲的严格的平衡二叉查找树的定义，但我们仍然可以说，这是一个合格的平衡二叉查找树
            * 平衡二叉查找树其实有很多，比如，`Splay Tree`（伸展树）、`Treap`（树堆）等，但是我们提到平衡二叉查找树，听到的基本都是红黑树
        + `红黑树`的英文是“`Red-Black Tree`”，简称 `R-B Tree`
            * 它是一种不严格的平衡二叉查找树
            * 红黑树中的节点，一类被标记为黑色，一类被标记为红色。
            * 除此之外，一棵红黑树还需要满足这样几个要求：
                - 根节点是黑色的；
                - 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
                    + 主要是为了简化红黑树的代码实现而设置的
                - 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
                - 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；
        + 红黑树的高度分析(参考链接中的图示更清晰)
            * 若将红色节点从红黑树中去掉，红色节点删除之后，有些节点就没有父节点了，它们会直接拿这些节点的祖父节点（父节点的父节点）作为父节点。所以，之前的二叉树就变成了四叉树
                - 从四叉树中取出某些节点，放到叶节点位置，四叉树就变成了完全二叉树。所以，仅包含黑色节点的四叉树的高度，比包含相同节点个数的完全二叉树的高度还要小
                - 而完全二叉树的高度近似`log2(n)`，这里的四叉“黑树”的高度要低于完全二叉树，所以去掉红色节点的“黑树”的高度也不会超过`log2(n)`
            * 现在把红色节点加回去，由于在红黑树中，红色节点不能相邻，有一个红色节点就要至少有一个黑色节点，将它跟其他红色节点隔开
                - 红黑树中包含最多黑色节点的路径不会超过`log2(n)`，所以加入红色节点之后，最长路径不会超过`2log2(n)`，也就是说，红黑树的高度近似 `2log2(n)`。
            * 所以，红黑树的高度只比高度平衡的AVL树的高度（`log2n`）仅仅大了一倍，在性能上，下降得并不多。
                - 这样推导出来的结果不够精确，实际上红黑树的性能更好
    - 解答开篇：平衡二叉查找树有很多种，为什么在工程中大家都喜欢用红黑树这种平衡二叉查找树？
        + 前面提到 `Treap`、`Splay Tree`，绝大部分情况下，它们操作的效率都很高，但是也无法避免极端情况下时间复杂度的退化。尽管这种情况出现的概率不大，但是对于单次操作时间非常敏感的场景来说，它们并不适用
        + `AVL` 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，`AVL`树为了维持这种高度的平衡，就要付出更多的代价。
            * 每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用 `AVL` 树的代价就有点高了。
        + 红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比 `AVL` 树要低
        + 所以，红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。
    - 因为红黑树是一种性能非常稳定的二叉查找树，所以，在工程中，但凡是用到动态插入、删除、查找数据的场景，都可以用到它。
        + 不过，它实现起来比较复杂，如果自己写代码实现，难度会有些高，这个时候，我们其实更倾向用`跳表`来替代它
* [26 | 红黑树（下）：掌握这些技巧，你也可以实现一个红黑树](https://time.geekbang.org/column/article/68976)
    - 实现红黑树的基本思想
        + 实际上，红黑树的平衡过程跟魔方复原非常神似，大致过程就是：遇到什么样的节点排布，我们就对应怎么去调整。只要按照这些固定的调整规则来操作，就能将一个非平衡的红黑树调整成平衡的
        + 再列一下上节提到的红黑树的要求
            * 根节点是黑色的；
            * 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
            * 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
            * 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；
        + 在插入、删除节点的过程中，第三、第四点要求可能会被破坏，而今天要讲的“平衡调整”，实际上就是要把被破坏的第三、第四点恢复过来
    - 两个重要操作(链接中演示了操作)
        + `左旋`（rotate left），全称为围绕某个节点的左旋
        + `右旋`（rotate right），全称为围绕某个节点右旋
    - 插入操作的平衡调整
        + 红黑树规定，插入的节点必须是红色的。而且，二叉查找树中新插入的节点都是放在叶子节点上
            * 所以有这样两种特殊情况
    - 先mark再翻篇。。。
        + 红黑树这里阻塞了好多天，可能老师开篇就说比较难，有点畏难的心态(心态不对，应该先趟过去试一下)
        + 今天看了插入和大概删除的平衡调整，看能看懂，按固定规则操作就可以，觉得再看这个章节就好多了
        + 跟厚书读薄，再从薄读厚一个道理，由面到点，再由点深入，过程可能很长，因为可以随时回头深入，不同时候有不同理解，有了解就有了回头的坐标。
* [27 | 递归树：如何借助树来求解递归算法的时间复杂度？](https://time.geekbang.org/column/article/69388)
    - 把递归一层一层的分解过程画成图，它其实就是一棵树。我们给这棵树起一个名字，叫作**递归树**
    - 如何用递归树来求解时间复杂度
        + 递归代码画成递归树
        + 归并排序：每层都为n(n/2+n/2 或者 n/4+n/4 + n/4+n/4 或 ...)，和数据规模有关，满二叉树层数logn，所以时间复杂度O(nlogn)
    - 实战一：分析快速排序的时间复杂度
    - 实战二：分析斐波那契数列的时间复杂度
    - 实战三：分析全排列的时间复杂度
    - 两种递归代码的时间复杂度分析方法
        + 有些代码比较适合用`递推公式`来分析，比如归并排序的时间复杂度、快速排序的最好情况时间复杂度
        + 有些比较适合采用`递归树`来分析，比如快速排序的平均时间复杂度
        + 而有些可能两个都不怎么适合使用，比如二叉树的递归前中后序遍历
* [28 | 堆和堆排序：为什么说堆排序没有快速排序快？](https://time.geekbang.org/column/article/69913)
    - **堆**（Heap），堆这种数据结构的应用场景非常多，最经典的莫过于堆排序了。
        + 堆排序是一种原地的、时间复杂度为 `O(nlogn)` 的排序算法
    - 只要满足这两点，它就是一个`堆`
        + 堆是一个完全二叉树
            * 完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列
        + 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值
            * 对于每个节点的值都大于等于子树中每个节点值的堆，我们叫做“`大顶堆`”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫做“`小顶堆`”
    - 对于同一组数据，可以构建多种不同形态的堆
    - 如何实现一个堆？
        + 用数组来存储完全二叉树是非常节省存储空间的。
            * 因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点
            * 数组中下标为 `i` 的节点的左子节点，就是下标为 `i∗2` 的节点
            * 右子节点就是下标为 `i∗2+1` 的节点，父节点就是下标为 `i​/2` 的节点
            * 注意此处及下面的分析中，堆中的数据是从数组下标为 `1` 的位置开始存储。
                - 那如果从 `0` 开始存储，实际上处理思路是没有任何变化的，唯一变化的，可能就是，代码实现的时候，计算子节点和父节点的下标的公式改变了
                - 如果节点的下标是 i，那左子节点的下标就是 `2∗i+1`，右子节点的下标就是 `2∗i+2`，父节点的下标就是 `i/2−1​`。
        + 1. 往堆中`插入`一个元素
            * 新插入的元素放到堆的最后，不符合堆的特性，需要进行调整，让其重新满足堆的特性，这个过程我们起了一个名字，就叫做`堆化（heapify）`
                - 堆化实际上有两种，从下往上和从上往下
                - 堆化非常简单，就是顺着节点所在的路径，向上或者向下，对比，然后交换
            * `从下往上`：让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系
        + 2. 删除堆顶元素
            * 假设为大顶堆，当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除
                - 不过这种方法有点问题，就是最后堆化出来的堆并不满足完全二叉树的特性
            * *解决*：我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是`从上往下`的堆化方法
                - 因为我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的“空洞”
                - 所以这种方法堆化之后的结果，肯定满足完全二叉树的特性
        + 复杂度
            * 堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是 `O(logn)`
                - 一个包含 n 个节点的完全二叉树，树的高度不会超过 `log2(​n)`
            * 插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 `O(logn)`
    - 如何基于堆实现排序？
        + 堆排序的过程大致分解成两个大的步骤，`建堆`和`排序`
        + `建堆`的过程，有两种思路
            * 第一种是借助前面讲的，在堆中插入一个元素的思路
                - 假设起初堆中只包含一个数据，就是下标为 1 的数据。然后，调用前面讲的插入操作，将下标从 2 到 n 的数据依次插入到堆中
            * 第二种实现思路，跟第一种截然相反
                - 第一种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是`从下往上`堆化
                - 而第二种实现思路，是从后往前处理数组，并且每个数据都是`从上往下`堆化
                - 对下标从 n/2​ 开始到 1 的数据进行堆化，下标是 n/2​+1 到 n 的节点是叶子节点，我们不需要堆化
                    + 实际上，对于完全二叉树来说，下标从 `n/2​+1` 到 `n` 的节点都是叶子节点
            * 建堆复杂度分析(第二种)
                - 每个节点堆化的时间复杂度是`O(logn)`，那`n/2​+1个`节点堆化的总时间复杂度是不是就是`O(nlogn)`呢？
                    + 实际上，堆排序的建堆过程的时间复杂度是 `O(n)`
                        * 因为叶子节点不需要堆化，所以需要堆化的节点从倒数第二层开始。每个节点堆化的过程中，需要比较和交换的节点个数，跟这个节点的高度 k 成正比
                        * 只需要将每个节点的高度求和，得出的就是建堆的时间复杂度
                        * 具体推导见链接
        + `排序`
            * 建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。
            * 数组中的第一个元素就是堆顶，也就是最大的元素。把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置
                - 这个过程有点类似上面讲的“删除堆顶元素”的操作
                - 当堆顶元素移除之后，我们把下标为 n 的元素放到堆顶，然后再通过堆化的方法，将剩下的 n−1 个元素重新构建成堆
                - 堆化完成之后，我们再取堆顶的元素，放到下标是`n−1`的位置，一直重复这个过程，直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了
        + 堆排序的时间复杂度、空间复杂度以及稳定性分析
            * 整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是`原地排序`算法
            * 堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是 `O(n)`，排序过程的时间复杂度是 `O(nlogn)`，所以，堆排序整体的时间复杂度是 `O(nlogn)`
            * 堆排序`不是稳定`的排序算法，因为在排序的过程，存在将堆的最后一个节点跟堆顶节点互换的操作，所以就有可能改变值相同数据的原始相对顺序
    - 解答开篇：在实际开发中，为什么快速排序要比堆排序性能好？
        + 第一点，堆排序数据访问的方式没有快速排序友好
            * 对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的
            * 这样对 CPU 缓存是不友好的(CPU缓存行)
        + 第二点，对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序
            * 对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换（或移动）。快速排序数据交换的次数不会比逆序度多
            * 但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。
                - 比如，对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了
* [29 | 堆的应用：如何快速获取到Top 10最热门的搜索关键词？](https://time.geekbang.org/column/article/70187)
    - 假设现在我们有一个包含 10 亿个搜索关键词的日志文件，如何能快速获取到热门榜 Top 10 的搜索关键词呢？
        + 这个问题就可以用堆来解决，这也是堆这种数据结构一个非常典型的应用
    - 堆这种数据结构几个非常重要的应用：
        + 优先级队列、求 Top K 和求中位数
    - 堆的应用一：`优先级队列`
        + 在优先级队列中，数据的出队顺序不是先进先出，而是按照优先级来，优先级最高的，最先出队
        + 实现一个优先级队列，用堆来实现是最直接、最高效的。这是因为，堆和优先级队列非常相似。
            * 往优先级队列中插入一个元素，就相当于往堆中插入一个元素；
            * 从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。
        + 优先级队列的应用场景非常多
            * 很多数据结构和算法都要依赖它。比如，赫夫曼编码、图的最短路径、最小生成树算法等等
            * 很多语言中，都提供了优先级队列的实现，比如，Java 的 `PriorityQueue`，C++ 的 `priority_queue` 等
        + 具体例子：1. 合并有序小文件
            * 问题：假设我们有 100 个小文件，每个文件的大小是 100MB，每个文件中存储的都是有序的字符串。我们希望将这些 100 个小文件合并成一个有序的大文件
            * 思路：
                - 有点像归并排序中的合并函数，从这100个文件中，各取第一个字符串，放入数组中，然后比较大小，把最小的那个字符串放入合并后的大文件中，并从数组中删除
                - 缺点：每次从数组中取最小字符串，都需要循环遍历整个数组，显然，这不是很高效
            * 可以利用优先级队列(或者说堆)：
                - 将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串
                - 将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将 100 个小文件中的数据依次放入到大文件中
                - 删除堆顶数据和往堆中插入数据的时间复杂度都是 `O(logn)`，相对上面每次从数组中找最小串(`O(n)`，n为个数，此处100)，高效得多
        + 具体例子：2. 高性能定时器
            * 问题：假设有一个定时器，定时器里维护了很多定时任务，每个任务都设定了一个要触发的时间点，需定期检查并触发执行相应任务
            * 思路：
                - 每隔比较短的时间(如1秒)就扫描一遍任务，看是否有任务达到设定的执行时间，有则拿出来执行
                - 缺点：每隔1秒扫描一次任务列表比较低效，原因有下面两点：
                - 第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的
                - 第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时
            * 利用优先级队列：
                - 按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务
                - 拿队首时间和当前时间相减，得到间隔`T`，则从当前到`T-1`这段时间，定时器都不需要做任何事情
                - 当`T`秒时间过去后，定时器取优先级队列的队首(小顶堆的堆顶)执行任务，然后继续计算新的队首(新堆顶)时间点和当前的时间差，继续等待
            * 这样，定时器既不用间隔 1 秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。
    - 堆的应用二：利用堆求 Top K
        + 这种求 Top K 的问题抽象成两类
            * 一类是针对静态数据集合，也就是说数据集合事先确定，不会再变
            * 另一类是针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中
        + 针对静态数据
            * 可以维护一个大小为 `K` 的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。
                - 如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。
                - 这样等数组中的数据都遍历完之后，堆中的数据就是前 K 大数据了
            * 遍历数组需要`O(n)`的时间复杂度，一次堆化操作需要`O(logK)`的时间复杂度，所以最坏情况下，n 个元素都入堆一次，时间复杂度就是`O(nlogK)`
        + 针对动态数据求得 Top K 就是实时 Top K
            * 一个数据集合中有两个操作：添加数据 和 询问当前的前K大数据
            * 如果每次询问前 K 大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是 `O(nlogK)`
            * 可以一直都维护一个 K 大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比，其操作和上面一样
                - 这样，无论何时查询当前的前K大数据，都可以立刻返回，时间复杂度为`O(1)`
    - 堆的应用三：利用堆求中位数
        + 中位数，顾名思义，就是处在中间位置的那个数，数据为奇数个则为从小到大的第`n/2`个数(从0开始)，偶数个则为`n/2`和`n/2 + 1`(任取一个，此处取`n/2`)
        + 对于一组静态数据
            * 中位数是固定的，可以先排序，再取`n/2`位置数据，每次询问直接返回该位置数据即可
            * 尽管排序的代价比较大，但是边际成本会很小
        + 但是对于一组动态数据
            * 每次询问还是用先排序的方法，效率就不高了
        + 可以利用两个堆，一个大顶堆、一个小顶堆，实现在动态数据集合中求中位数的操作
            * 大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据
                - 假设个数n 是偶数，我们从小到大排序，那前 n/2​ 个数据存储在大顶堆中，后 n/2 个数据存储在小顶堆中
                - 如果 n 是奇数，情况是类似的，大顶堆就存储 n/2+1 个数据，小顶堆中就存储 n/2 个数据
            * 这样，大顶堆中的堆顶元素就是我们要找的中位数
            * 对于动态数据，当新添加一个数据的时候，如何调整两个堆，让大顶堆中的堆顶元素继续是中位数呢？
                - 如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；否则，我们就将这个新数据插入到小顶堆。
            * 这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：如果 n 是偶数，两个堆中的数据个数都是 n/2；如果 n 是奇数，大顶堆有 n/2+1 个数据，小顶堆有n/2个数据
                - 可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定
            * 插入数据因为需要涉及堆化，所以时间复杂度变成了 `O(logn)`，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是 `O(1)`
        + 实际上，利用两个堆不仅可以快速求出中位数，还可以快速求其他百分位的数据，原理是类似的。
            * e.g. “如何快速求接口的 99% 响应时间？”
                - 如果有 n 个数据，将数据从小到大排列之后，99 百分位数大约就是第 `n*99%` 个数据，同类，80 百分位数大约就是第 `n*80%` 个数据
                - 维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是 n，大顶堆中保存 `n*99%` 个数据，小顶堆中保存 `n*1%`个数据。大顶堆堆顶的数据就是我们要找的 99% 响应时间。
                - 每次插入一个数据的时候，我们要判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系，然后决定插入到哪个堆中。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。
                - 为了保持大顶堆中的数据占 99%，小顶堆中的数据占 1%，在每次新插入数据之后，我们都要重新计算两个堆的个数是否还符合 99:1 这个比例。如果不满足，就将一个堆中的数据移动到另一个堆中，知道满足这个比例。
                - 每次插入数据可能涉及几个数据的堆化操作，时间复杂度为`O(logn)`，每次查询99%响应时直接返回大顶堆的堆顶数据即可，时间复杂度`O(1)`
    - 解答开篇：假设现在我们有一个包含 10 亿个搜索关键词的日志文件，如何快速获取到 Top 10 最热门的搜索关键词呢？
        + 有很多高级解决方法，如使用MapReduce等。但是若将处理的场景限定为单机，可使用的内存为1GB，该如何解决？
        + 思路：
            * 先统计每个搜索关键词出现的频率，利用散列表、平衡二叉查找树或其他一些支持快速查找、插入的数据结构来记录关键字及其出现的次数
            * 假设选用散列表，顺序扫描关键词统计各次数。然后用堆求Top K的方法，建立一个大小为10的小顶堆。依次遍历关键词和其次数，与堆顶的搜索关键词对比，如果出现次数比堆顶次数多，则删除堆顶关键词，并将该关键词加入到堆中。最后堆中就是出现次数最多的Top 10搜索关键词了。
        + 上面思路的问题：
            * 10亿条记录还是很多的，假设不重复的有1亿条，若每个关键词平均长度为50字节，就需要 50 * 10^8 = 5GB内存，而散列表要避免频繁冲突，不会选择太大的装载因子，所以消耗的内存更多了。超出了限制的1GB内存大小。
        + 解决：
            * 相同数据经过哈希算法得到的哈希值是一样的。我们可以根据哈希算法的这个特点，将 10 亿条搜索关键词先通过哈希算法分片到 10 个文件中
            * 创建 10 个空文件 00，01，02，……，09。我们遍历这 10 亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同 10 取模，得到的结果就是这个搜索关键词应该被分到的文件编号
            * 对这 10 亿个关键词分片之后，每个文件都只有 1 亿的关键词(哈希值%10，只是平均，不一定每个就是1亿条)，去除掉重复的，可能就只有 1000 万个，每个关键词平均 50 个字节，所以总的大小就是 500MB。1GB 的内存完全可以放得下。(此处只是分片到10个文件中，没有对重复关键词做什么处理)
            * 我们针对每个包含 1 亿条搜索关键词的文件，利用散列表和堆，分别求出 Top 10，然后把这个 10 个 Top 10 放在一块，然后取这 100 个关键词中，出现次数最多的 10 个关键词，这就是这 10 亿数据中的 Top 10 最频繁的搜索关键词了
    - 思考题：有一个访问量非常大的新闻网站，我们希望将点击量排名 Top 10 的新闻摘要，滚动显示在网站首页 banner 上，并且每隔 1 小时更新一次。如果你是负责开发这个功能的工程师，你会如何来实现呢？
        + 留言中的方案：
        + 对每篇新闻摘要计算一个hashcode，并建立摘要与hashcode的关联关系，使用map存储，以hashCode为key，新闻摘要为值
            * 或者若每个新闻有一个uuid，则不需要对摘要做哈希进行映射
        + 按每小时一个文件的方式记录下被点击的摘要的hashCode
        + 当一个小时结束后，上一个小时的文件被关闭，开始计算上一个小时的点击top10
        + 将hashcode分片到多个文件中，通过对hashCode取模运算，即可将相同的hashCode分片到相同的文件中
        + 针对每个文件取top10的hashCode，使用`Map<hashCode,int>`的方式，统计出所有的摘要点击次数，然后再使用小顶堆（大小为10）计算top10
        + 再针对所有分片计算一个总的top10,最后合并的逻辑也是使用小顶堆，计算top10
        + 如果仅展示前一个小时的top10,计算结束
        + 如果需要展示全天，需要与上一次的计算按hashCode进行合并，然后在这合并的数据中取top10
            * 和上一次的哈希合并，则同一篇文章次数会叠加，是合理的。根据合并的数据(`map<hash,int>`)再计算，而不是(不合并)直接拿int前10的hash
        + 在展示时，将计算得到的top10的hashcode，转化为新闻摘要显示即可
* [30 | 图的表示：如何存储微博、微信等社交网络中的好友关系？](https://time.geekbang.org/column/article/70537)
    - **图(Graph)**，涉及图的算法有很多，也非常复杂，比如图的搜索、最短路径、最小生成树、二分图等等
        + 和树比起来，这是一种更加复杂的非线性表结构
    - 树中的元素我们称为节点，图中的元素我们就叫做`顶点（vertex）`
    - 图中的一个顶点可以与任意其他顶点建立连接关系。我们把这种建立的关系叫做`边（edge）`
    - 生活中就有很多符合图这种结构的例子，社交网络就是一个非常典型的图结构
        + 拿*微信*举例子。我们可以把每个用户看作一个顶点。如果两个用户之间互加好友，那就在两者之间建立一条边。所以，整个微信的好友关系就可以用一张图来表示。其中，每个用户有多少个好友，对应到图中，就叫做`顶点的度（degree）`，就是跟顶点相连接的边的条数
        + *微博*的社交关系跟微信还有点不一样，或者说更加复杂一点。微博允许单向关注，也就是说，用户 A 关注了用户 B，但用户 B 可以不关注用户 A。
            * 引入`方向`的概念，如果用户 A 关注了用户 B，我们就在图中画一条从 A 到 B 的带箭头的边，来表示边的方向。如果用户 A 和用户 B 互相关注了，那我们就画一条从 A 指向 B 的边，再画一条从 B 指向 A 的边。
            * 把这种边有方向的图叫做“`有向图`”。把边没有方向的图就叫做“`无向图`”
            * 在有向图中，我们把度分为`入度（In-degree）`和`出度（Out-degree）`
            * 顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点。
        + 再来看另一种社交软件：*QQ*
            * QQ 不仅记录了用户之间的好友关系，还记录了两个用户之间的亲密度，如果两个用户经常往来，那亲密度就比较高；如果不经常往来，亲密度就比较低
            * 在图中记录这种好友关系的亲密度，要用到另一种图：`带权图（weighted graph）`，在带权图中，每条边都有一个`权重（weight）`，我们可以通过这个权重来表示 QQ 好友间的亲密度
    - 邻接矩阵存储方法：如何在内存中存储图这种数据结构呢？
        + 图最直观的一种存储方法就是，*邻接矩阵（Adjacency Matrix）*
            * 邻接矩阵的底层依赖一个二维数组。
                - 对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 `A[i][j]`和 `A[j][i]`标记为 1；
                - 对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 `A[i][j]`标记为 1
                    - 同理，如果有一条箭头从顶点 j 指向顶点 i 的边，我们就将 `A[j][i]`标记为 1
                - 对于带权图，数组中就存储相应的权重
            * 用邻接矩阵来表示一个图，虽然简单、直观，但是比较浪费存储空间
                - 对于无向图来说，如果 `A[i][j]`等于 1，那`A[j][i]`也肯定等于 1。实际上，我们只需要存储一个就可以了
                - 也就是说，无向图的二维数组中，如果将其用对角线划分为上下两部分，那我们只需要利用上面或者下面这样一半的空间就足够了，另外一半白白浪费掉了
                - 还有，如果我们存储的是`稀疏图`（Sparse Matrix），也就是说，顶点很多，但每个顶点的边并不多，那邻接矩阵的存储方法就更加浪费空间了
            * 但这也并不是说，邻接矩阵的存储方法就完全没有优点。首先，邻接矩阵的存储方式简单、直接，因为基于数组，所以在获取两个顶点的关系时，就非常高效。其次，用邻接矩阵存储图的另外一个好处是方便计算。这是因为，用邻接矩阵的方式存储图，可以将很多图的运算转换成矩阵之间的运算
        + 针对上面邻接矩阵比较浪费内存空间的问题，我们来看另外一种图的存储方法，*邻接表（Adjacency List）*
            * 每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点
            * 一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。
        + 邻接矩阵存储起来比较浪费空间，但是使用起来比较节省时间。相反，邻接表存储起来比较节省空间，但是使用起来就比较耗时间
            * 如果想确定是否存在一条从顶点 2 到顶点 4 的边，那我们就要遍历顶点 2 对应的那条链表，看链表中是否存在顶点 4。
            * 链表的存储方式对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了
            * 之前章节提到在基于链表法解决冲突的散列表中，如果链过长，为了提高查找效率，我们可以将链表换成其他更加高效的数据结构，比如平衡二叉查找树等。也可以将邻接表同散列表一样进行“改进升级”。
                - 可以将邻接表中的链表改成平衡二叉查找树。实际开发中，我们可以选择用红黑树。这样，我们就可以更加快速地查找两个顶点之间是否存在边
                - 当然，这里的二叉查找树可以换成其他动态数据结构，比如跳表、散列表等。
                - 除此之外，我们还可以将链表改成有序动态数组，可以通过二分查找的方法来快速定位两个顶点之间否是存在边
    - 解答开篇：如何存储微博、微信等社交网络中的好友关系？
        + 数据结构是为算法服务的，所以具体选择哪种存储方法，与期望支持的操作有关系。针对微博用户关系，假设我们需要支持下面这样几个操作：
            * 判断用户 A 是否关注了用户 B；(从A的关注列表里找是否有B)
            * 判断用户 A 是否是用户 B 的粉丝；(从B的粉丝列表里找是否有A)
            * 用户 A 关注用户 B
            * 用户 A 取消关注用户 B
            * 根据用户名称的首字母排序，分页获取用户的粉丝列表
            * 根据用户名称的首字母排序，分页获取用户的关注列表
        + 对于邻接矩阵和邻接表两种存储方法，因为社交网络是一张稀疏图，使用邻接矩阵存储比较浪费存储空间。所以，这里我们采用*邻接表*来存储。
            * 不过，用一个邻接表来存储这种有向图是不够的。我们去查找某个用户关注了哪些用户非常容易，但是如果要想知道某个用户都被哪些用户关注了，也就是用户的粉丝列表，是非常困难的。
            * 基于此，我们需要一个`逆邻接表`。邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系。
                - 逆邻接表中，每个顶点的链表中，存储的是指向这个顶点的顶点(而邻接表每个顶点的链表中，存储的就是这个顶点指向的顶点)
                - 如果要查找某个用户关注了哪些用户，我们可以在邻接表中查找；如果要查找某个用户被哪些用户关注了，我们从逆邻接表中查找。
            * 基础的邻接表不适合快速判断两个用户之间是否是关注和被关注关系，所以选择改进版本，将邻接表中的链表改为支持快速查找的动态数据结构。选择哪种：红黑树、跳表、有序动态数组还是散列表呢？
                - 因为我们需要按照用户名称的首字母排序，分页来获取用户的粉丝列表或者关注列表，用*跳表*这种结构再合适不过了。
                    + 这是因为，跳表插入、删除、查找都非常高效，时间复杂度是 `O(logn)`，空间复杂度上稍高，是 `O(n)`。
                    + 最重要的一点，跳表中存储的数据本来就是有序的了，分页获取粉丝列表或关注列表，就非常高效。
            * 如果对于小规模的数据，比如社交网络中只有几万、几十万个用户，我们可以将整个社交关系存储在内存中，上面的解决思路是没有问题的。但是如果像微博那样有上亿的用户，数据规模太大，我们就无法全部存储在内存中了。这个时候该怎么办呢？
                - 可以通过哈希算法等数据分片方式，将邻接表存储在*不同的机器上*(逆邻接表的处理方式也一样)
                    + 当要查询顶点与顶点关系的时候，我们就利用同样的哈希算法，先定位顶点所在的机器，然后再在相应的机器上查找
                - 除此之外，我们还有另外一种解决思路，就是利用*外部存储（比如硬盘）*，因为外部存储的存储空间要比内存会宽裕很多。
                    + *数据库*是我们经常用来持久化存储关系数据的，所以这里介绍一种数据库的存储方式。
                    + 为了高效地支持前面定义的操作，我们可以在表上建立多个索引
* [31 | 深度和广度优先搜索：如何找出社交网络中的三度好友关系？](https://time.geekbang.org/column/article/70891)
    - 在社交网络中，有一个`六度分割理论`，具体是说，你与世界上的另一个人间隔的关系不会超过六度，也就是说平均只需要六步就可以联系到任何两个互不相识的人。
    - 一个用户的一度连接用户很好理解，就是他的好友，二度连接用户就是他好友的好友，三度连接用户就是他好友的好友的好友。在社交网络中，我们往往通过用户之间的连接关系，来实现推荐“可能认识的人”这么一个功能。
    - 本篇的开篇问题是：给你一个用户，如何找出这个用户的所有三度（其中包含一度、二度和三度）好友关系？
    - `广度优先搜索（BFS）`
        + 广度优先搜索（Breadth-First-Search），我们平常都简称 `BFS`
        + 直观地讲，它其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。
        + 代码实现
            * `void bfs(int s, int t)`
            * s表示起始顶点，t表示终止顶点，搜索一条从 s 到 t 的路径。实际上，这样求得的路径就是从 s 到 t 的最短路径。
            * 里面有三个重要的辅助变量 `visited`、`queue`、`prev`
                - `visited` 用来记录已经被访问的顶点，用来避免顶点被重复访问
                - `queue` 是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。
                - `prev` 用来记录搜索路径。当我们从顶点 s 开始，广度优先搜索到顶点 t 后，prev 数组中存储的就是搜索的路径。不过，这个路径是反向存储的。`prev[w]`存储的是，顶点 w 是从哪个前驱顶点遍历过来的。
        + 复杂度分析
            * 最坏情况下，终止顶点离起始顶点很远，需要遍历整个图才能找到。这时每个顶点(`vertex`)都要进出一遍队列，每个边(`edge`)也都会被访问一次。所以广度优先遍历的*时间复杂度*为`O(V+E)`，其中，`V`表示顶点的个数，`E`表示边的个数。
            * 对于一个连通图来说，也就是说一个图中的所有顶点都是连通的，`E` 肯定要大于等于 `V-1`，所以，广度优先搜索的时间复杂度也可以简写为 `O(E)`。
            * 广度优先搜索的空间消耗主要在几个辅助变量 visited 数组、queue 队列、prev 数组上。这三个存储空间的大小都不会超过顶点的个数，所以*空间复杂度*是`O(V)`。
    - `深度优先搜索（DFS）`
        + 深度优先搜索（Depth-First-Search），简称 `DFS`。
        + 最直观的例子就是“走迷宫”。
            * 假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜索策略。
        + 搜索的起始顶点是 s，终止顶点是 t，我们希望在图中寻找一条从顶点 s 到顶点 t 的路径。如果映射到迷宫那个例子，s 就是你起始所在的位置，t 就是出口。
            * 可以看出，深度优先搜索找出来的路径，并不是顶点 s 到顶点 t 的最短路径。
            * 实际上，深度优先搜索用的是一种比较著名的算法思想，`回溯思想`。这种思想解决问题的过程，非常适合用递归来实现。
            * 深度优先搜索代码实现也用到了 `prev`、`visited` 变量以及 `print()` 函数，它们跟广度优先搜索代码实现里的作用是一样的
        + 复杂度分析
            * 每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的*时间复杂度*是 `O(E)`，E 表示边的个数。
            * 深度优先搜索算法的消耗内存主要是 visited、prev 数组和递归调用栈。visited、prev 数组的大小跟顶点的个数 V 成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的*空间复杂度*就是 `O(V)`。
    - 解答开篇：如何找出社交网络中某个用户的三度好友关系？
        + 这个问题就非常适合用图的广度优先搜索算法来解决，因为广度优先搜索是层层往外推进的。
        + 首先，遍历与起始顶点最近的一层顶点，也就是用户的一度好友，然后再遍历与用户距离的边数为 2 的顶点，也就是二度好友关系，以及与用户距离的边数为 3 的顶点，也就是三度好友关系。
        + 只需要稍加改造一下广度优先搜索代码，用一个数组来记录每个顶点与起始顶点的距离，非常容易就可以找出三度好友关系。
    - 小结
        + 广度优先搜索和深度优先搜索是图上的两种最常用、最基本的搜索算法，比起其他高级的搜索算法，比如 `A*`、`IDA*`等，要简单粗暴，没有什么优化，所以，也被叫作`暴力搜索算法`。
        + 所以，这两种搜索算法仅适用于状态空间不大，也就是说图不大的搜索。
* [32 | 字符串匹配基础（上）：如何借助哈希算法实现高效字符串匹配？](https://time.geekbang.org/column/article/71187)
    - 字符串匹配算法很多
        + 本节涉及两种比较简单的、好理解的，它们分别是：`BF` 算法和 `RK` 算法
        + 下一节，会涉及两种比较难理解、但更加高效的，它们是：`BM` 算法和 `KMP` 算法
        + 这两节讲的都是`单模式串匹配`的算法，也就是一个串跟一个串进行匹配
        + 后面章节涉及的`多模式串匹配`算法，也就是在一个串中同时查找多个串，它们分别是 `Trie` 树和 `AC` 自动机
    - `RK` 算法是 `BF` 算法的改进，它巧妙借助了前面讲过的哈希算法，让匹配的效率有了很大的提升
    - `BF` 算法
        + BF 算法中的 BF 是 `Brute Force` 的缩写，中文叫作暴力匹配算法，也叫朴素匹配算法
        + 定义两个概念(主串、模式串)：在字符串 A 中查找字符串 B，那字符串 A 就是`主串`，字符串 B 就是`模式串`。我们把主串的长度记作`n`，模式串的长度记作`m`。因为我们是在主串中查找模式串，所以 `n>m`。
        + 作为最简单、最暴力的字符串匹配算法，BF算法的思想可以用一句话来概括，那就是，我们在主串中，检查起始位置分别是 0、1、2…n-m 且长度为 m 的 n-m+1 个子串，看有没有跟模式串匹配的。
        + 每次都比对 m 个字符，要比对 n-m+1 次，所以，这种算法的最坏情况时间复杂度是`O(n*m)`
        + 尽管理论上，BF 算法的时间复杂度很高，是 O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。
            * 第一，实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把m个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是 O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。
            * 第二，朴素字符串匹配算法思想简单，代码实现也非常简单。简单意味着不容易出错，如果有bug也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常说的KISS（Keep it Simple and Stupid）设计原则。
        + 所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了。
    - `RK` 算法
        + RK 算法的全称叫 `Rabin-Karp` 算法，是由它的两位发明者 Rabin 和 Karp 的名字来命名的。
        + 是刚刚讲的 BF 算法的升级版
        + 对朴素的字符串匹配算法稍加改造，引入哈希算法，时间复杂度立刻就会降低
        + RK算法的思路是这样的：
            * 通过哈希算法对主串中的n-m+1个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。
                - 先不考虑哈希冲突，下面进行说明
            * 如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了
            * 因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了
        + 通过哈希算法计算子串的哈希值的时候，我们需要遍历子串中的每个字符。尽管模式串与子串比较的效率提高了，但是，算法整体的效率并没有提高。通过下面对哈希算法的设计可以提高哈希算法计算子串哈希值的效率
        + 哈希算法设计：
            * 假设要匹配的字符串的字符集中只包含 `K` 个字符，我们可以用一个 `K 进制数`来表示一个子串，这个 K 进制数转化成十进制数，作为子串的哈希值
                - e.g. 假设字符串中只包含a～z这26个小写字符，用二十六进制来表示一个字符串，对应的哈希值就是二十六进制数转化成十进制的结果
                - 计算成十进制时，可以通过查表的方法来提高效率。事先计算好26^0、26^1、26^2……26^(m-1)，并且存储在一个长度为 m 的数组中，公式中的“次方”就对应数组的下标，直接使用，省去了计算的时间。
            * 很容易就能得出这样的规律：相邻两个子串 s[i-1]和 s[i]（i 表示子串在主串中的起始位置，子串的长度都为 m），对应的哈希值计算公式有交集，也就是说，可以使用 s[i-1]的哈希值很快的计算出 s[i]的哈希值。
        + 复杂度分析
            * 通过设计上面特殊的哈希算法，只需要扫描一遍主串就能计算出所有子串的哈希值了，所以这部分的时间复杂度是`O(n)`。
            * 模式串哈希值与每个子串哈希值之间的比较的时间复杂度是 `O(1)`，总共需要比较 n-m+1 个子串的哈希值，所以，这部分的时间复杂度也是 `O(n)`
            * 所以，RK 算法整体的时间复杂度就是 `O(n)`。
        + 还有一个问题：若模式串很长，相应的主串中的子串也会很长，通过上面的哈希算法计算得到的哈希值就可能很大，可能会超过整型数据可以表示的范围
            * 刚刚设计的哈希算法是`没有`散列冲突的(K进制唯一表示一个数)
            * 实际上，为了能将哈希值落在整型数据范围内，可以牺牲一下，允许哈希冲突
            * 比如a对应1，b对应2，以此类推，z对应26。可以把字符串中每个字母对应的数字相加，最后得到的和作为哈希值
                - 最简单的设计方法，这种哈希算法的哈希冲突概率也是挺高的
            * 比如将每一个字母从小到大对应一个素数，而不是 1，2，3……这样的自然数，这样冲突的概率就会降低一些
            * 当发现一个子串的哈希值跟模式串的哈希值相等的时候，需要再对比一下子串和模式串本身
            * 所以，哈希算法的冲突概率要相对控制得低一些，如果存在大量冲突，就会导致 RK 算法的时间复杂度退化，效率下降。极端情况下，如果存在大量的冲突，每次都要再对比子串和模式串本身，那时间复杂度就会退化成 `O(n*m)`
            * 但也不要太悲观，一般情况下，冲突不会很多，RK 算法的效率还是比 BF 算法高的。
* [33 | 字符串匹配基础（中）：如何实现文本编辑器中的查找功能？](https://time.geekbang.org/column/article/71525)
    - 文本编辑器中的查找替换功能，比如，在 Word 中把一个单词统一替换成另一个，它是怎么实现的呢？
        + 用上一节讲的 `BF` 算法和 `RK` 算法，也可以实现这个功能，但是在某些极端情况下，`BF` 算法性能会退化的比较严重，而 `RK` 算法需要用到哈希算法，而设计一个可以应对各种类型字符的哈希算法并不简单
    - `BM（Boyer-Moore）`算法
        + 它是一种非常高效的字符串匹配算法，有实验统计，它的性能是著名的`KMP` 算法的 3 到 4 倍
        + BM 算法的核心思想
            * 把模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当遇到不匹配的字符时，BF 算法和 RK 算法的做法是，模式串往后滑动一位，然后从模式串的第一个字符开始重新匹配。
        + BM 算法原理分析
            * BM 算法包含两部分，分别是`坏字符规则`（bad character rule）和`好后缀规则`（good suffix shift）
            * 1. `坏字符规则`
                - BM 算法的匹配顺序比较特别，它是按照模式串下标从大到小的顺序，*倒着匹配*的
                - 从模式串的末尾往前倒着匹配，当我们发现某个字符没法匹配的时候。我们把这个没有匹配的字符叫作`坏字符`（**主串**中的字符）
                - 当发生不匹配的时候，我们把坏字符对应的模式串中的字符下标记作`si`。如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作`xi`。如果不存在，我们把 `xi` 记作 `-1`。那模式串往后移动的位数就等于 `si-xi` (这里的下标都是字符在模式串的下标)
                    + 如果坏字符在模式串里多处出现，那我们在计算`xi`的时候，选择*最靠后*的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过
                - 利用坏字符规则，BM 算法在最好情况下的时间复杂度非常低，是`O(n/m)`
                    + 比如，主串是 `aaabaaabaaabaaab`，模式串是`aaaa`。每次比对，模式串都可以直接后移四位(此处坏字符为`b`，主串中的坏字符对应模式串的位置`si`为`3`，坏字符在模式串的位置`xi`，因为在模式串中不存在所以`xi`为`-1`，所以移动4位)，所以，匹配具有类似特点的模式串和主串的时候，BM 算法非常高效
                - 不过，单纯使用坏字符规则还是不够的。因为根据 `si-xi` 计算出来的移动位数，有可能是负数，比如主串是 `aaaaaaaaaaaaaaaa`，模式串是 `baaa`。(坏字符为`a`，其对应模式串位置`si`为`0`，坏字符在模式串中位置`xi`，最靠后的为`3`，所以`si-xi`为`-3`)不但不会向后滑动模式串，还有可能倒退。
                - 所以，BM 算法还需要用到“`好后缀规则`”。
            * 2. `好后缀规则`
                - 好后缀规则实际上跟坏字符规则的思路很类似
                - 示例见参考链接，当模式串滑动到图中的位置的时候，模式串和主串有 2 个字符是匹配的，倒数第 3 个字符发生了不匹配的情况
                - 把已经匹配的 `bc`(链接中的示例) 叫作`好后缀`，记作`{u}`，拿它在模式串中查找，
                    + 如果找到了另一个跟`{u}`相匹配的子串`{u*}`，那我们就将模式串滑动到子串`{u*}`与主串中`{u}`对齐的位置
                    + 如果在模式串中找不到另一个等于`{u}`的子串，我们就直接将模式串，滑动到主串中`{u}`的后面，因为之前的任何一次往后滑动，都没有匹配主串中`{u}`的情况
                        * 但是如果我们将模式串移动到好后缀的后面，如链接中所示，那就会错过模式串和主串可以匹配的情况
                        * 当模式串滑动到前缀与主串中{u}的后缀有部分重合的时候，并且重合的部分相等的时候，就有可能会存在完全匹配的情况。
                        * 针对这种情况，我们不仅要看好后缀在模式串中，是否有另一个匹配的子串，我们还要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的
* [34 | 字符串匹配基础（下）：如何借助BM算法轻松理解KMP算法？](https://time.geekbang.org/column/article/71845)
    - 在所有的字符串匹配算法里，要说最知名的一种的话，那就非 `KMP` 算法莫属。很多时候，提到字符串匹配，我们首先想到的就是 `KMP` 算法。
    - `KMP` 算法基本原理
        + KMP 算法是根据三位作者（D.E.Knuth，J.H.Morris 和 V.R.Pratt）的名字来命名的，算法的全称是 Knuth Morris Pratt 算法，简称为 KMP 算法。
        + KMP 算法的核心思想，跟上一节讲的 `BM` 算法非常相近。我们假设主串是 `a`，模式串是 `b`。在模式串与主串匹配的过程中，当遇到不可匹配的字符的时候，我们希望找到一些规律，可以将模式串往后多滑动几位，跳过那些肯定不会匹配的情况
    - 待定
* [35 | Trie树：如何实现搜索引擎的搜索关键词提示功能？](https://time.geekbang.org/column/article/72414)
    - `Trie树`，也叫“字典树”。顾名思义，它是一个树形结构。
        + 它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。
    - 示例
        + 有 6 个字符串，它们分别是：how，hi，her，hello，so，see。我们希望在里面多次查找某个字符串是否存在
        + 可以先对这 6 个字符串做一下预处理，组织成 `Trie` 树的结构，之后每次查找，都是在 Trie 树中进行匹配查找
    - `Trie` 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起
        + 其中，根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串（注意：红色节点并不都是叶子节点）
        + Trie 树是一个多叉树
    - Trie 树主要有两个操作
        + 一个是将字符串集合`构造`成 Trie 树
            * `Trie`树构造过程的每一步，都相当于往 Trie 树中插入一个字符串。当所有字符串都插入完成之后，Trie 树就构造好了。
            * 26个字母，多叉树节点的子节点部分可用：`TrieNode children[26];` 表示
        + 另一个是在 Trie 树中`查询`一个字符串。
            * 当我们在 Trie 树中查找一个字符串的时候，比如查找字符串“her”，那我们将要查找的字符串分割成单个的字符 h，e，r，然后从 Trie 树的根节点开始匹配
    - 复杂度分析
        + `构建` Trie 树的过程，需要扫描所有的字符串，时间复杂度是 `O(n)`（n 表示所有字符串的长度和）。但是一旦构建成功之后，后续的查询操作会非常高效。
        + 构建好 Trie 树后，在其中`查找`字符串的时间复杂度是 `O(k)`，k 表示要查找的字符串的长度
    - 内存问题
        + 如果字符串中包含从 a 到 z 这 26 个字符，那每个节点都要存储一个长度为 26 的数组，并且每个数组元素要存储一个 8 字节指针（或者是 4 字节，这个大小跟 CPU、操作系统、编译器等有关）。
            * 而且，即便一个节点只有很少的子节点，远小于 26 个，比如 3、4 个，我们也要维护一个长度为 26 的数组。
        + 如果要构造的Trie树的字符串中不仅包含小写字母，还包含大写字母、数字、甚至是中文，那需要的存储空间就更多了。
            * 所以，也就是说，在某些情况下，Trie 树不一定会节省存储空间。
            * 在重复的前缀并不多的情况下，Trie 树不但不能节省内存，还有可能会浪费更多的内存
        + 为了解决这个内存问题，可以稍微牺牲一点查询的效率，将每个节点中的数组换成其他数据结构，来存储一个节点的子节点指针。
            * 比如使用有序数组、跳表、散列表、红黑树等。
            * 假设我们用有序数组，数组中的指针按照所指向的子节点中的字符的大小顺序排列。查询的时候，我们可以通过二分查找的方法，快速查找到某个字符应该匹配的子节点的指针。但是，在往 Trie 树中插入一个字符串的时候，我们为了维护数组中数据的有序性，就会稍微慢了点。
    - Trie树对要处理的字符串有及其严苛的要求
        + 第一，字符串中包含的字符集不能太大
        + 第二，要求字符串的前缀重合比较多，不然空间消耗会变大很多
        + 第三，如果要用 Trie 树解决问题，那我们就要自己从零开始实现一个 Trie 树，还要保证没有 bug，这个在工程上是将简单问题复杂化，除非必须，一般不建议这样做。
        + 第四，我们知道，通过指针串起来的数据块是不连续的，而 Trie 树中用到了指针，所以，对缓存并不友好，性能上会打个折扣
    - 综合上面这几点，针对在一组字符串中查找字符串的问题，我们在工程中，更倾向于用`散列表`或者`红黑树`。因为这两种数据结构，我们都不需要自己去实现，直接利用编程语言中提供的现成类库就行了
        + Trie 树只是不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。
        + Trie 树比较适合的是`查找前缀匹配的字符串`，也就是类似开篇问题的那种场景。
        + 实际上，Trie 树的这个应用可以扩展到更加广泛的一个应用上，就是`自动输入补全`，比如输入法自动补全功能、IDE 代码编辑器自动补全功能、浏览器网址输入的自动补全功能等等。
* [36 | AC自动机：如何用多模式串匹配实现敏感词过滤功能？](https://time.geekbang.org/column/article/72810)
    - 如何才能实现一个高性能的敏感词过滤系统呢？这就要用到今天的`多模式串匹配`算法。
    - 基于`单模式串`和 `Trie` 树实现的敏感词过滤
        + 前面几节讲了好几种字符串匹配算法，有 BF 算法、RK 算法、BM 算法、KMP 算法，还有 Trie 树。前面四种算法都是单模式串匹配算法，只有 Trie 树是多模式串匹配算法
        + `单模式串匹配`算法，是在一个模式串和一个主串之间进行匹配，也就是说，在一个主串中查找一个模式串
        + `多模式串匹配`算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串
    - 经典的多模式串匹配算法：`AC 自动机`
        + AC 自动机算法，全称是 Aho-Corasick 算法
        + AC 自动机实际上就是在 `Trie` 树之上，加了类似 `KMP` 的 `next` 数组(参考前面KMP章节)，只不过此处的 `next` 数组是构建在树上罢了
    - 待定
* [37 | 贪心算法：如何用贪心算法实现Huffman压缩编码？](https://time.geekbang.org/column/article/73188)
    - 算法思想：贪心算法、分治算法、回溯算法、动态规划
    - `贪心算法（greedy algorithm）`
        + 贪心算法有很多经典的应用，比如霍夫曼编码（Huffman Coding）、Prim 和 Kruskal 最小生成树算法、还有 Dijkstra 单源最短路径算法
    - 关于贪心算法，文章中的例子
        + 5种豆子，每种总量和总价值都各不相同。为了让可容纳100KG物品的背包中所装物品的总价值最大，我们如何选择在背包中装哪些豆子？每种豆子又该装多少呢？
        + 只要先算一算每个物品的单价，按照单价由高到低依次来装就好了
        + 本质上借助的就是贪心算法
    - 贪心算法解决问题的步骤
        + 第一步，当我们看到这类问题的时候，首先要联想到贪心算法：针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大
        + 第二步，我们尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。
        + 第三步，我们举几个例子看下贪心算法产生的结果是否是最优的。
            * 大部分情况下，举几个例子验证一下就可以了。
            * 严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来说，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明
        + 实际上，用贪心算法解决问题的思路，并不总能给出最优解
            * 在这种情况下，贪心算法不工作的主要原因是，前面的选择，会影响后面的选择
            * 即便第一步选择最优的走法（边最短），但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了
    - 贪心算法实战分析
        + 对于贪心算法，如果死抠理论的话，确实很难理解透彻。掌握贪心算法的关键是多练习。只要多练习几道题，自然就有感觉了
        + 分糖果
            * 从n个孩子中，抽取一部分孩子分配糖果，每个孩子的需求量不同，如何分能让满足的孩子的个数（期望值）是最大的。限制值是糖果个数m小于孩子个数n
            * 每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案
        + 钱币找零
            * 不同面额的钱币，各自的张数不同，现在要用这些钱来支付 K 元，最少要用多少张纸币呢
            * 先用面值最大的来支付，如果不够，就继续用更小一点面值的，以此类推，最后剩下的用 1 元来补齐
                - 在贡献相同期望值（纸币数目）的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路
        + 区间覆盖
            * 有n个区间，从这 n 个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢
                - 这个处理思想在很多贪心算法问题中都有用到，比如任务调度、教师排课等等问题。
            * 这个问题的解决思路是这样的：
                - 我们假设这 n 个区间中最左端点是 lmin，最右端点是 rmax。这个问题就相当于，我们选择几个不相交的区间，从左到右将[lmin, rmax]覆盖上。
                - 我们按照起始端点从小到大的顺序对这 n 个区间排序
                - 每次选择的时候，选择左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间
            * 这实际上就是一种贪心的选择方法。
    - 解答开篇：如何用贪心算法实现霍夫曼编码？
        + 假设有一个包含 1000 个字符的文件，每个字符占 1 个 byte（1byte=8bits），存储这 1000 个字符就一共需要 8000bits，那有没有更加节省空间的存储方式呢？
        + 假设我们通过统计分析发现，这 1000 个字符中只包含 6 种不同字符，假设它们分别是 a、b、c、d、e、f。而 3 个二进制位（bit）就可以表示 8 个不同的字符，所以，为了尽量减少存储空间，每个字符我们用 3 个二进制位来表示
            * 3个二进制位表示：a(000)、b(001)、c(010)、d(011)、e(100)、f(101)
            * 则存储这1000个字符只要3000bit
        + `霍夫曼编码`更加节省空间
            * 霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在 20%～90% 之间
            * 霍夫曼编码不仅会考察文本中有多少个不同字符，还会考察*每个字符出现的频率*，根据频率的不同，选择不同长度的编码。用这种*不等长*的编码方法，来进一步增加压缩的效率
                - 根据贪心的思想，我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。
                - 为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况
            * 如何根据字符出现频率的不同，给不同的字符进行不同长度的编码呢？
                - 把每个字符看作一个节点，并且附带着把频率放到优先级队列中(此处为大顶堆)
                - 从队列中取出频率最小的两个节点 A、B，然后新建一个节点 C，把频率设置为两个节点的频率之和，并把这个新节点 C 作为节点 A、B 的父节点
                - 最后再把 C 节点放入到优先级队列中。
                - 重复这个过程，直到队列中没有数据。
                - 然后给每一条边加上画一个权值，指向左子节点的边我们统统标记为 `0`，指向右子节点的边，我们统统标记为 `1`，那从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。
                - 参考文章链接的图示更直观
    - 课后思考
        + 在一个非负整数 a 中，我们希望从中移除 k 个数字，让剩下的数字值最小，如何选择移除哪 k 个数字呢？
            * 由最高位开始，比较低一位数字，如高位大，则移除；若高位小，则向右移一位继续比较两个数字，直到高位大于低位才移除，循环k次
                - e.g. 4556847594546移除5位-》455647594546-》45547594546-》4547594546-》4447594546-》444594546
            * 评论中有种方法说用 Top K 排序，求出 K 个最大的数字再移除，是*不对*的
                - e.g. 3255155,删除4位,用topK最后结果是321,但实际最小是155
            * 评论中另外有种方法先算出移除k个数字，还剩下几位(n)，比如剩下3位，也*不对*
                - e.g. 3255155,删除4位,每次从前面3个中取最小的进行保留，循环直到剩下n(包括保留的那个)
                - 3255155-》25155-〉155
                - 1255255-》15255-〉155，应该是122，还是得用第一种方法
        + 假设有 n 个人等待被服务，但是服务窗口只有一个，每个人需要被服务的时间长度是不同的，如何安排被服务的先后顺序，才能让这 n 个人总的等待时间最短？
            * 每次选服务时长最短的服务。实际服务总时长都是一样的，所以要减少别人不必要的等待
* [38 | 分治算法：谈一谈大规模计算框架MapReduce中的分治思想](https://time.geekbang.org/column/article/73503)
    - MapReduce 是 Google 大数据处理的三驾马车之一，另外两个是 GFS 和 Bigtable
        + 它在倒排索引、PageRank 计算、网页分析等搜索引擎相关的技术中都有大量的应用。
    - `分治算法（divide and conquer）`的核心思想其实就是四个字，分而治之。
        + 也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解
        + 这个定义看起来有点类似递归的定义
            * 关于分治和递归的区别：分治算法是一种处理问题的思想，递归是一种编程技巧
            * 实际上，分治算法一般都比较适合用递归来实现
        + 分治算法的递归实现中，每一层递归都会涉及这样三个操作：
            * 分解：将原问题分解成一系列子问题；
            * 解决：递归地求解各个子问题，若子问题足够小，则直接求解；
            * 合并：将子问题的结果合并成原问题
        + 分治算法能解决的问题，一般需要满足下面这几个条件：
            * 原问题与分解成的小问题具有相同的模式
            * 原问题分解成的子问题可以独立求解，子问题之间没有相关性
                - 这一点是分治算法跟动态规划的明显区别
            * 具有分解终止条件，也就是说，当问题足够小时，可以直接求解
            * 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了
    - 分治算法应用举例分析
        + 如何编程求出一组数据的有序对个数或者逆序对个数？
            * 最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的
                - 对每个数字对应的个数k值求和，不过，这样操作的时间复杂度是 `O(n^2)`
            * 分治的思想：可以将数组分成前后两半 A1 和 A2，分别计算 A1 和 A2 的逆序对个数 K1 和 K2，然后再计算 A1 与 A2 之间的逆序对个数 K3。
                - 那数组 A 的逆序对个数就等于 K1+K2+K3。
                - 如何快速计算出两个子问题 A1 与 A2 之间的逆序对个数呢？可借助*归并排序*算法
                - 每次合并操作，都计算逆序对个数，把这些计算出来的逆序对个数求和，就是这个数组的逆序对个数了。
        + 二维平面上有 n 个点，如何快速计算出两个距离最近的点对？
        + 有两个 `n*n` 的矩阵 A，B，如何快速求解两个矩阵的乘积 `C=A*B`？
    - 分治思想在海量数据处理中的应用
        + 可以将海量的数据集合根据某种方法，划分为几个小的数据集合，每个小的数据集合单独加载到内存来解决，然后再将小数据集合合并成大数据集合
        + 利用这种分治的处理思路，不仅仅能克服内存的限制，还能利用多线程或者多机处理，加快处理的速度。
            - 如之前桶排序讲到的内存受限情况下给10GB订单排序(笔记内搜`有 10GB 的订单数据`)
            - 如果订单数据存储在类似 GFS 这样的分布式系统上，当10GB的订单被划分成多个小文件的时候，每个文件可以并行加载到多台机器上处理，最后再将结果合并在一起，这样并行处理的速度也加快了很多
* [39 | 回溯算法：从电影《蝴蝶效应》中学习回溯算法的核心思想](https://time.geekbang.org/column/article/74287)
    - 之前(第 31 节)讲的深度优先搜索算法利用的就是回溯算法思想
    - 回溯算法除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中
        + 比如正则表达式匹配、编译原理中的语法分析等
        + 除此之外，很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1 背包、图的着色、旅行商问题、全排列等等
    - 理解`回溯算法`
        + 笼统地讲，回溯算法很多时候都应用在“搜索”这类问题上：在一组可能的解中，搜索满足期望的解
        + 回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。
            - 面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。
        + 八皇后问题
            - 有一个 8x8 的棋盘，希望往里放 8 个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子
            - 八皇后问题就是期望找到所有满足这种要求的放棋子方式
            - 把这个问题划分成8个阶段，依次将8个棋子放到第一行、第二行、第三行……第八行。在放置的过程中，我们不停地检查当前放法，是否满足要求


