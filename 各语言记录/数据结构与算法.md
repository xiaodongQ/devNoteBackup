## 数据结构与算法之美

* 极客时间专栏：[数据结构与算法之美](https://time.geekbang.org/column/intro/126)
    - 学习过程中的代码实践，也放在LeetCode的练习里：[xiaodongQ/LeetCode](https://github.com/xiaodongQ/LeetCode)
* 复杂度分析
    - 复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半
    - 你可能会有些疑惑，我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？这种方式叫`事后统计法`。但是，这种统计方法有非常大的局限性。
        + 1. 测试结果非常依赖测试环境
            * 换到另一台机器上时，可能会有截然相反的结果
        + 2. 测试结果受数据规模的影响很大
            * 对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别
            * 除此之外，如果测试数据规模太小，测试结果可能无法真实地反应算法的性能
            * 我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法
    - `大 O 复杂度表示法`
        + 大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度(asymptotic time complexity)，简称`时间复杂度`
    - 如何分析一段代码的时间复杂度？三个比较**实用的方法**：
        + 1. 只关注循环执行次数最多的一段代码
        + 2. 加法法则：总复杂度等于量级最大的那段代码的复杂度
            * 如果 `T1(n)=O(f(n))`，`T2(n)=O(g(n))`；那么 `T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n)))`
        + 3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
            * 如果 `T1(n)=O(f(n))`，`T2(n)=O(g(n))`；那么 `T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))`
    - 几种常见时间复杂度实例分析
        + 常见的复杂度量级并不多：`O(1)`、`O(logn)`、`O(n)`、`O(nlogn)`、`O(n^2)`/`O(n^3)`/.../`O(n^k)`、`O(2^n)`、`O(n!)`
        + 上述复杂度粗略地分为两类：*多项式量级*和*非多项式量级*。
            * 其中，非多项式量级只有两个：`O(2^n)` 和 `O(n!)`
                - 我们把时间复杂度为非多项式量级的算法问题叫作*NP（Non-Deterministic Polynomial，非确定多项式）问题*
                - 当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长
        + 主要看几种常见的*多项式时间复杂度*
            * 常数阶`O(1)`：只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)
            * 对数阶`O(logn)`、线性对数阶`O(nlogn)`
                - 实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。
                - 由于对数之间是可以互相转换的，`log3(n)`(此处为以3为底n的对数，由于编辑器排版问题展示不好位置) 就等于 `log3(2) * log2(n)`，所以 `O(log3n) = O(C * log2n)`，其中 `C=log3(2)` 是一个常量
                    + 根据换底公式：`log3(n) = log2(n)/log2(3) = log3(2) * log2(n)`，所以有上面的转换
                - 在采用大 O 标记复杂度的时候，可以忽略系数，即 `O(Cf(n)) = O(f(n))`，所以，`O(log2(n))` 就等于 `O(log3(n))`，因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 `O(logn)`
                - 如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了，O(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是 O(nlogn)
            * `O(m+n)`、`O(m*n)`
                - 跟前面都不一样的时间复杂度，代码的复杂度由两个数据的规模来决定
                - 我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个
    - *空间复杂度分析*
        + 类比一下时间复杂度，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系
        + 常见的空间复杂度就是 `O(1)`、`O(n)`、`O(n^2)`，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多
    - 四个复杂度分析方面的知识点
        + *最好情况时间复杂度（best case time complexity）*
            * 在最理想的情况下，执行这段代码的时间复杂度。
        + *最坏情况时间复杂度（worst case time complexity）*
            * 在最糟糕的情况下，执行这段代码的时间复杂度
        + *平均情况时间复杂度（average case time complexity）*
            * 最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，我们需要引入另一个概念：平均情况时间复杂度，后面简称为平均时间复杂度
            * 平均时间复杂度 的全称应该叫`加权平均时间复杂度`或者`期望时间复杂度`(将各种情况发生的概率考虑进去)
        + *均摊时间复杂度（amortized time complexity）*
            * 大部分情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限
    - [03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？](https://time.geekbang.org/column/article/40036)
    - [04 | 复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度](https://time.geekbang.org/column/article/40447)
* [05 | 数组：为什么很多编程语言中数组都从0开始编号？](https://time.geekbang.org/column/article/40961)
    - 数组（Array）是一种线性表数据结构。它用一组`连续`的内存空间，来存储一组具有`相同类型`的数据
    - 数组为了保证内存数据连续性，会导致插入、删除操作比较低效
        + 插入时间复杂度分析
            * 假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位
                - 如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 `O(1)`
                - 但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 `O(n)`
                - 因为我们在每个位置插入元素的`概率是一样的`，所以平均情况时间复杂度为 (1+2+…n)/n=`O(n)`
            * 若数组中的数据是*有序*的，则在某位置插入一个新元素时，就必须按照刚才的方法搬移 k 之后的数据
            * 但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合，则可`改进`为：
                - 在这种情况下，如果要将某个数据插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，*直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置*
                - 利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 `O(1)`。这个处理思想在快排中也会用到
        + 删除操作复杂度分析
            * 跟插入数据类似，如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了
                - 和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 `O(1)`；如果删除开头的数据，则最坏情况时间复杂度为 `O(n)`；平均情况时间复杂度也为 `O(n)`(删除末尾只操作1个数，删除开头操作n个数，第二个n-1...，删除每个元素概率为1/n，`(1+...+n)/n=(n+1)n/2n=(n+1)/2`)
            * 实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？
                - 可以*先记录*下已经删除的数据。每次的删除操作*并不是真正地搬移数据，只是记录数据已经被删除*。当数组没有更多空间存储数据时，我们再*触发执行一次*真正的删除操作，这样就大大减少了删除操作导致的数据搬移。(JVM 标记清除垃圾回收算法与之类似)
    - 开篇的问题：为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？
        + 从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”
            * 如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 `type_size` 的位置，a[k]的内存地址只需要用这个公式：
                - `a[k]_address = base_address + k * type_size`
            * 如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为
                - `a[k]_address = base_address + (k-1)*type_size`
            * 从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于CPU来说，就是多了一次减法指令。为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。
    - 在平时的业务开发中，我们可以直接使用编程语言提供的容器类(如Java的ArrayList)，但是，如果是特别底层的开发，直接使用数组可能会更合适
* [06 | 链表（上）：如何实现LRU缓存淘汰算法?](https://time.geekbang.org/column/article/41013)
    - 链表 Linked list
        + 相关概念：结点、后继指针next、头结点、尾结点
    - 缓存淘汰算法，常见策略：
        + 先进先出策略FIFO(First In, First Out)
        + 最少使用策略LFU(Least Frequently Used)
        + 最近最少使用策略LRU(Least Recently Used)
    - 针对链表的插入和删除，时间复杂度为`O(1)`
        + 只需要考虑相邻结点的指针改变
    - 三种最常见的链表结构
        + 单链表
            * 想访问第k个元素，只能根据指针一个结点一个结点依次遍历，时间复杂度`O(n)`
        + 双向链表
            * 每个结点不止有后继指针next，还有一个前驱指针prev指向前面的结点
            * 从结构上来看，双向链表可以支持`O(1)`时间复杂度的情况下找到前驱结点
                - 这也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效
                - 单链表的插入、删除已经是O(1)时间复杂度了，还能再怎么高效？
                - 删除结点：
                    + `情况1.` 删除结点中“值等于某个给定值”的结点：尽管单纯的删除操作时间复杂度是O(1)，但**遍历查找**的时间是主要的耗时点，对应的时间复杂度为`O(n)`，根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的*总时间复杂度*为`O(n)`
                    + `情况2.` 删除给定指针指向的结点：已经找到了某个结点q，但是要删除q需要知道它的前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表(`O(n)`)，而对于双向链表就不需要遍历了(`O(1)`)
                - 插入结点
                    + 和删除结点类似，对于单链表需要遍历其前驱结点以便插入，而双向链表则不需要
            * 除了插入、删除操作有优势之外，对于一个*有序链表*，双向链表的按值查询的效率也要比单链表高一些。
                - 因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据
                - Java的`LinkedHashMap`就用了双向链表这个结构(此外还用到散列表)
            * `空间换时间`的设计思想
                - 当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。
                - 相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路
                - 缓存就是利用了空间换时间的设计思想
        + 循环链表
            * 单链表的尾结点指针指向空地址，而循环链表的尾节点指向链表的头结点
            * 和单链表相比，循环链表的优点是从链尾到链头比较方便
            * 和双向链表整合在一起，就是双向循环链表
    - 链表和数组
        + 它们插入、删除、随机访问操作的时间复杂度正好相反
        + 数组使用连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高
        + 数组的缺点是大小固定，一经声明就要占用整块连续内存空间，过大可能没有足够内存来分配，过小可能不够用而只能申请更大空间进行拷贝；
        + 链表本身没有大小的限制，天然地支持动态扩容，链接中描述为这是它与数组最大的区别
    - 解答开篇问题：基于*链表*实现 LRU 缓存淘汰算法
        + 维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。
        + 当有一个新数据被访问时，就从链表头开始遍历链表
            * a. 若链表中有该数据(表示已缓存在链表中)，则将该结点删除，并插入到链表头部
            * b. 若链表中未缓存在链表中
                - 缓存未满时：将该结点插入到链表头部
                - 缓存已满：则删除链表尾结点，将新的结点插入到链表头部
        + 时间复杂度：遍历`O(n)`，结点操作`O(1)`，因此缓存访问的时间复杂度为`O(n)`
            * 优化：可引入*散列表（Hash table）*(后续)，记录每个数据的位置，将复杂度降到`O(1)`
        + 基于*数组*实现LRU缓存淘汰算法的思路：
            * 维护一个数组缓存访问的数据，越靠近数组开始位置的元素为越早访问
            * 有新数据访问时，从数组尾部开始查找
                - 若该数据已缓存在数组中，则将其删除(?涉及后面的数据搬移)，新增到数组尾
                - 若不在，直接在数组尾新增
                    + 缓存满时，删除数组首结点，元素全部前移
            * 删除结点搬移数据想想都觉得太暴力了，查找`O(n)`，搬移`O(n)`，复杂度是链表2倍，虽然最后还是为`O(n)`
            * 优化思路：
                - 由于每次删除结点搬移其后面的数据太耗时，用一个新的数组Arr保存要删除结点的位置(需要额外空间了，空间换时间)
                - 还是上面的逻辑走，发现要删除结点则先不做删除，而是记录位置到Arr，待缓存的空间不够时再做一次性删除和搬移处理
                - 复杂度：
                    + 由于需要额外空间，可指定固定的空间，当空间满时也触发批量删除，固定空间则空间复杂度也为`O(1)`
                    + 时间复杂度：还是需要查找和搬移，不过性能比上面要好些，虽然还是`O(n)`
    - 思考题：如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？
        + 解法
            * a. 将链表各结点的值复制到数组中，再从首尾依次取结点比较是否相同，有不同则非回文
                - 空间复杂度O(n)，时间复杂度 复制O(n)+判断O(n/2)=O(n)
            * b. 双指针法，定义两个指针以步长1和以步长2移动，直到找到中间结点，中间结点之后的部分反转，再各自比较两部分，最后再反转还原
                - 空间复杂度O(1), 时间复杂度 中间结点O(n/2)+反转O(n/2)+比较O(n/2)+反转O(n/2) = O(n)
        + 代码参考github：[linkedlist_palindromic](https://github.com/xiaodongQ/LeetCode/blob/master/datastruct_algo/linkedlist_palindromic.go)
        + LeetCode有该题：[234. 回文链表](https://leetcode-cn.com/problems/palindrome-linked-list/submissions/)
    - 关于链表的更多操作可见LeetCode
        + 参考：[链表标签](https://leetcode-cn.com/problemset/all/?topicSlugs=linked-list)
    - 几个写链表代码相关的技巧
        + [07 | 链表（下）：如何轻松写出正确的链表代码？](https://time.geekbang.org/column/article/41149)
        + 技巧一：理解指针或引用的含义
        + 技巧二：警惕指针丢失和内存泄漏
            * a和b结点间插入结点x，假设p指向a: 则先`x->next = p->next`，再`p->next = x`，若顺序反了则链表后面的部分访问不到
            * 删除结点，对该结点的内存手动释放(C/C++里，有垃圾回收的语言如Java/Go则不需要)
        + 技巧三：利用哨兵简化实现难度
            * 不用哨兵的场景，针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理
                - 向链表的结点p后插入一个结点：
                    + `new_node->next = p->next;`，`p->next = new_node;`，
                    + 但若是向空链表插入第一个结点，上面逻辑就不行了，需要：`if (head == null) { head = new_node; }`
                - 删除链表的结点p的后一个结点：
                    + `p->next = p->next->next;`
                    + 但若要删除结点的是链表所剩的最后一个结点，需要：`if (p->next == null) { head = null; }`
            * 引入哨兵结点(sentinel)
                - 不管链表是否为空，head指针都指向该哨兵结点。把这种有哨兵结点的链表叫*带头链表*(没有则称不带头链表)
                    + 哨兵节点不存任何数据
                - 插入
                    + 向空链表插入第一个结点则：`sentinel->next = new_node;`
                - 删除
                    + 删除只剩一个结点的链表结点，`sentinel->next = null;`
                - 因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了
        + 技巧四：重点留意边界条件处理
            * 如果链表为空时，代码是否能正常工作？
            * 如果链表只包含一个结点时，代码是否能正常工作？
            * 如果链表只包含两个结点时，代码是否能正常工作？
            * 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？
        + 技巧五：举例画图，辅助思考
            * 当写完代码之后，也可以举几个例子，画在纸上，照着代码走一遍，很容易就能发现代码中的 Bug
        + 技巧六：多写多练，没有捷径
            * 单链表反转、链表中环的检测、两个有序的链表合并、删除链表倒数第 n 个结点、求链表的中间结点
            * 链表操作相关的LeetCode个人练习链接：[链表相关操作](https://github.com/xiaodongQ/LeetCode/blob/master/linked_list/operate_linkedlist_test.go)
* [08 | 栈：如何实现浏览器的前进和后退功能？](https://time.geekbang.org/column/article/41222)
    - 栈既可以用数组来实现(*顺序栈*)，也可以用链表来实现(*链式栈*)。
    - 入栈和出栈只涉及栈顶个别数据操纵，时间复杂度为`O(1)`
    - 支持动态扩容的顺序栈
        + 当栈满了之后，就申请一个更大的数组，将原来的数据搬移到新数组中
        + 对于入栈操作来说，最好情况时间复杂度是 O(1)，最坏情况时间复杂度是 O(n)。
        + 平均情况下的时间复杂度，用`摊还分析法`分析：
            * 不满时，入栈`O(1)`
            * 当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存(假设满了扩展为2倍)，并且做 K 个数据的搬移操作，然后再入栈
            * 但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push(此处表示不涉及内存搬移的入栈操作) 操作就可以完成，这K次入栈总共涉及K个数据搬移和K次simple-push操作
            * 将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 `O(1)`。
    - 应用
        + 函数调用栈的应用
            * 参考示例链接中的代码调用，`main()`中调用了`add(int x, int y)`函数，`main`中定义的变量依次push入栈，然后是`add`的形参赋值后push入栈，而后add中定义的变量入栈，add中return时出栈
        + 表达式求值应用(编译器利用栈来实现表达式求值)
            * 编译器就是通过*两个栈*来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。
            * 我们从左向右遍历表达式，当遇到数字：我们就*直接压入操作数栈*；当遇到运算符：就与*运算符栈*的栈顶元素进行比较
                - 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；
                - 如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶*取 2 个操作数*，然后进行计算，*再把计算完的结果压入操作数栈*，继续比较。
                    + 注意此时遇到的运算符还没有压入栈，直到该运算符比运算符栈的栈顶运算符优先级高时，才将该运算符压入栈
            * 链接中用图示演示了一个简单示例：`3+5*8-6`，过程用文字说明如下(还是图示更清晰)：
                - `3`->操作数栈，`+`->运算符栈，`5`->操作数栈，`*`->运算符栈，`8`->操作数栈
                - `-`比运算符栈栈顶的`*`优先级低，则从操作数栈取`8`和`5`(出栈)，并取运算符栈栈顶`*`，得到`40`，压入操作数栈，此时操作数栈从栈顶向下为`40`，`3`
                - `-`和运算符栈当前栈顶`+`优先级相同，从操作数栈中取两个操作数`40`，`3`，取`+`，得到`43`(3+40=43，栈下面的元素作为操作符左边)压入操作数栈，此时操作数栈仅`43`一个栈帧
                - 将`-`压入运算符栈，`6`->操作数栈，表达式结束清空栈，从运算符栈取`-`，取两个操作数，得到`37`(43-6=37)
        + 括号匹配应用
            * 假设表达式中只包含三种括号，圆括号 ()、方括号[]和花括号{}，并且它们可以任意嵌套
                - 说明：`{[] ()[{}]}` 和 `[{()}([])]` 等为合法格式；`{[}()]` 或 `[({)]` 为不合法格式
            * 用栈来保存未匹配的左括号，从左到右依次扫描字符串。
                - 当扫描到左括号时，则将其压入栈中；
                - 当扫描到右括号时，从栈顶取出一个左括号
                - 如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。
                - 如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式
            * 当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。
    - 解答开篇问题：如何实现浏览器的前进和后退功能？
        + 使用两个栈，X 和 Y，我们把首次浏览的页面依次压入栈 X，当点击*后退*按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y
        + 当我们点击*前进*按钮时，我们依次*从栈 Y 中取出数据，放入栈 X 中*。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了
    - LeetCode：[栈相关题目](https://leetcode-cn.com/problemset/all/?topicSlugs=stack)
* [09 | 队列：队列在线程池等有限资源池中的应用](https://time.geekbang.org/column/article/41330)
    - 两个操作：入队`enqueue()` 和 出队`dequeue()`
    - 和栈类似，用数组实现的队列叫作`顺序队列`，用链表实现的队列叫作`链式队列`
    - 队列需要两个指针：一个是 head 指针，指向队头；一个是 tail 指针，指向队尾(tail不存数据)
        + 栈只需要一个栈顶指针
        + 入队时tail指针后移，出队时head指针后移
            * 入队时判断队列是否已满，出队时判断队列是否为空
    - 数组实现
        + 随着不停地进行入队、出队操作，head 和 tail(注意tail不存数据) 都会持续往后移动，当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了，如何解决？
            * 入队(enqueue)时，若队尾没有空闲空间了(tail==n)，则将数据搬移到队列头(需要队头有空闲空间，通过队头指针判断是否为第一个位置)
            * 入队O(1) (摊还分析法，head位置分别为1、n/2、n-1，搬移一次可支持后续n-head次入队)，出队O(1)
    - 链表实现
        + `enqueue`: tail->next=new_node, tail = tail->next
        + `dequeue`: head = head->next
    - 循环队列(可参考链接中的图示，更直观一些)
        + 避免上面数组实现方式的数据搬移
        + 最关键的是，确定好队空和队满的判定条件
            * 用数组实现的非循环队列中，队满条件：`tail==n`，队空：`head==tail`
            * 数组实现的循环队列中，队满条件：`(tail+1)%n==head`，队空还是：`head==tail`
                - 当队列满时，tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间
        + `enqueue`: 判是否满 `if ((tail+1)%n == head)`，入队后 `tail = (tail+1)%n`
        + `dequeue`: 判是否空 `if (head == tail)`，出队后 `head = (head+1)%n`
    - 应用
        + 阻塞队列(生产者 - 消费者模型)
            * 在队列基础上增加了阻塞操作
                - 队列为空的时候，从队头取数据会被阻塞
                - 如果队列已经满了，那么插入数据的操作就会被阻塞
        + 并发队列
            * 在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题
            * 线程安全的队列我们叫作并发队列
                - 最简单直接的方式是给`enqueue`和`dequeue`操作加锁，但锁粒度大并发度会降低，同一时刻只允许一个存或取操作
                - 基于数组的循环队列，利用 `CAS` 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因
            * 后续讲 Disruptor时，会再涉及并发队列的应用
    - 开篇问题：线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？
        + 一般有两种处理策略
            * 第一种是非阻塞的处理方式，直接拒绝任务请求；
            * 另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理
        + 阻塞两种实现方式
            * 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长
                - 所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的
            * 基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理
                - 需要设置一个合理的队列大小。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能
    - 对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队
        + 如数据库连接池
    - 笔记
        + C++里面的`condition_variable`，参考：[笔记](https://github.com/xiaodongQ/devNoteBackup/blob/master/%E5%90%84%E8%AF%AD%E8%A8%80%E8%AE%B0%E5%BD%95/C%2B%2B.md)，(搜`std::condition_variable`章节)
        + linux下的 `pthread_cond_t`，参考：[笔记](https://github.com/xiaodongQ/devNoteBackup/blob/master/%E5%90%84%E8%AF%AD%E8%A8%80%E8%AE%B0%E5%BD%95/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B.md)，(搜`## 条件变量(condition variable)`章节)
* [11 | 排序（上）：为什么插入排序比冒泡排序更受欢迎？](https://time.geekbang.org/column/article/41802)
    - 最经典的、最常用的一些排序算法：
        + 冒泡排序、插入排序、选择排序、
            * `O(n^2)`
        + 归并排序、快速排序、
            * `O(nlogn)`
        + 计数排序、基数排序、桶排序
            * `O(n)`
    - 排序算法的**执行效率**，一般从这几个方面来衡量：
        + 1. 最好情况、最坏情况、平均情况时间复杂度
            * 同时要说出对应情况下要排序的原始数据是什么样的
                - 对于要排序的数据，有的接近有序，有的完全无序
        + 2. 时间复杂度的系数、常数 、低阶
            * 时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶
            * 但是实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据
            * 所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来
        + 3. 比较次数和交换（或移动）次数
            * 在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去
    - 排序算法的**内存消耗**
        + 针对排序算法的空间复杂度，还引入了一个新的概念，`原地排序（Sorted in place）`
        + 原地排序算法，就是特指空间复杂度是 `O(1)` 的排序算法
    - 排序算法的**稳定性**
        + 如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变，那么这个排序就是稳定的
        + 为什么要考察排序算法的稳定性呢？
            * 演示数据结构和算法时，很多是用整数举例，但真正的软件开发中，要排序的对象往往是一组对象，而只是根据某个key排序
            * 如果要依据两个要素的顺序排列，如示例中的 先按时间先后再按金额大小，第一遍稳定的排序算法之后已经是有时间先后了，第二遍稳定排序后，先后顺序并不会改变；若第二遍排序不稳定，则按金额排序后相同金额的顺序可能发生变化
    - **冒泡排序**（Bubble Sort）
        + 每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。
        + 一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作
        + e.g.
            * `data := [...]int{3, 5, 4, 1, 2, 6}` 数组从小到大排列，第一次循环(冒泡)，依次比较相邻元素，会将6存在最后
            * 循环n次，遍历一遍所有元素进行：`if (data[j]>data[j+1]) swap`
                - 注意：每次内层遍历时，并不需要`len(data)-1`次，而是`len(data)-1-i`次(假设外层遍历下标i)，前面冒泡的数据并不必要再做比较
        + 对于上面的过程可以优化：
            * 当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作
        + 算法分析
            * 原地排序，空间复杂度：`O(1)`
            * 相同元素值时不做交换，所以冒泡排序是稳定的排序算法
            * 时间复杂度(需要分情况)：
                - 最好情况时间复杂度：数据已经有序，只需要冒泡一次，所以为`O(n)`
                - 最坏情况时间复杂度：数据刚好是倒序，要冒泡n次，为`O(n^2)`
                - 平均情况下的时间复杂度：`O(n^2)`
        + 平均情况时间复杂度分析过程：
            * 有序度和逆序度
                - 有序度是数组中具有有序关系的元素对的个数
                    + 倒序排列的数组，有序度为0
                    + 完全有序的数组的有序度叫作满有序度，其有序度为 `C(n,2)=n(n-1)/2`
                - 逆序度和有序度正好相反
                - 逆序度 = 满有序度 - 有序度
            * 冒泡排序包含两个原子操作：比较和交换。每次交换，有序度就加1
            * 不管算法怎么改进，交换次数总是确定的，即为逆序度，也就是`n*(n-1)/2 – 初始有序度`
                - 最坏情况下，初始状态的有序度是 `0`，所以要进行 `n*(n-1)/2` 次交换
                - 最好情况下，初始状态的有序度是 `n*(n-1)/2`，就不需要进行交换，即`0`次交换
                - 取个中间值 `n*(n-1)/4`，来表示初始有序度既不是很高也不是很低的平均情况
            * 平均情况下，需要 `n*(n-1)/4` 次交换操作，比较操作肯定要比交换操作多，而复杂度的上限是`O(n^2)`，所以平均情况下的时间复杂度就是 `O(n^2)`
            * 这个平均时间复杂度推导过程其实并不严格，但是很多时候很实用，毕竟概率论的定量分析太复杂，不太好用。
    - **插入排序**(Insert Sort)
        + 将数据分为两个区间：已排序区间 和 未排序区间
            * 初始已排序区间就只有第一个元素
        + 插入算法的核心思想是：
            * 取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。
                - 找到插入点之后，插入点之后的元素顺序都向后移动一位，这样才能腾出位置插入
            * 重复这个过程，直到未排序区间中元素为空，算法结束
        + 移动次数也等于逆序度
        + 算法分析
            * 原地排序，`O(1)`
            * 稳定
            * 时间复杂度
                - 最好情况：完全有序，n次比较，不用移动，`O(n)`
                - 最坏情况：完全逆序，每次都要在头部插入数据，数据后移，`O(n^2)`
                - 平均情况时间复杂度：`O(n^2)`
                    + 每次插入数据的平均时间复杂度为`O(n)`(之前数组章节有分析，考虑概率)，因此n次插入平均`O(n^2)`
                    + 数组插入元素的分析，参考前面章节：[05 | 数组：为什么很多编程语言中数组都从0开始编号？]
    - **选择排序**(Selection Sort)
        + 选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间
            * 初始已排序区间为空，和插入排序有所区别
        + 但是选择排序每次会从未排序区间中找到最小的元素，将其放到*已排序区间的末尾*
            * 找出最小元素后，和未排序区间的第一个元素交换位置
        + 算法分析
            * 原地排序，`O(1)`
            * 不稳定
                - 每次找未排序区间中的最小元素 和 前面的元素交换位置，这样破坏了稳定性(查看参考链接中的图示更直观)
            * 时间复杂度
                - 最好、最坏、平均情况，都是`O(n^2)`
                    + 因为每次都要选择剩下最小的元素 和 前面数据交换
    - 解答开篇：插入排序和冒泡排序的时间复杂度相同，都是`O(n^2)`，都是原地排序算法，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？
        + 冒泡排序和插入排序，移动数据的次数是同样的，都等于原始数据的逆序度(假设逆序度为`K`)
            * 冒泡排序需要交换数据，每次交换需要三次赋值操作，需要`3*K`个单位时间
            * 而插入排序，每次移动数据只要一个赋值操作，仅需要`K`个单位时间
        + 所以，虽然冒泡排序和插入排序在时间复杂度上是一样的，都是`O(n^2)`，但是如果我们希望把性能优化做到极致，那肯定首选插入排序
            * 插入排序的算法思路也有很大的优化空间，上面只是最基础的一种，优化方案如 希尔排序
            * 希尔排序
                - 希尔排序是希尔（Donald Shell）于1959年提出的一种排序算法。
                - 希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序
                - 希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。
                - 实现见参考链接
                - [图解排序算法(二)之希尔排序](https://www.cnblogs.com/chengxiao/p/6104371.html)
    - 这三种排序算法，对于*小规模数据*的排序，用起来非常高效。但是在*大规模数据*排序的时候，这个时间复杂度还是稍微有点高，所以我们更倾向于用下一节要讲的时间复杂度为 `O(nlogn)` 的排序算法
* [12 | 排序（下）：如何用快排思想在O(n)内查找第K大元素？](https://time.geekbang.org/column/article/41913)
    - 上章节的冒泡、插入、选择排序，时间复杂度都是`O(n^2)`，比较高，适合小规模数据的排序
    - 本章节介绍复杂度都为`O(nlogn)`的 *归并排序* 和 *快速排序*，这两种算法适合大规模的数据排序，比上面三种排序算法更常用。
    - 归并排序和快速排序都用到了分治思想
        + 分治算法一般都是用递归来实现的
        + 分治是一种解决问题的处理思想，递归是一种编程技巧，这两者并不冲突
    - **归并排序**(Merge Sort)
        + 如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了
        + 要想写出归并排序的代码，先要写出归并排序的递推公式：
            * 递推公式：`merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))` (传入数组len-1，左右边界均闭区间)
            * 终止条件：`p >= r`时，不用再继续分解了
            * 合并函数`merge()`的实现：
                - 新建一个临时空间，依次取两个分区(各自游标i, j)中的数据进行比较，小的数存入到临时空间，并将对应游标+1，最后临时空间数据就是有序的了，将该范围数据拷贝到原数据对应的区间位置
            * 测试示例参考：MergeSort、MergeSort2，[LeetCode/sort/sort_algo_test.go](https://github.com/xiaodongQ/LeetCode/blob/master/sort/sort_algo_test.go)
                - 注意传入下标取到相等
        + 算法分析
            * 稳定排序
                - 取决于`merge()`函数，合并过程中，有相同元素则将前面的数据先放到临时空间中，即可保证相对顺序不变
            * 时间复杂度`O(nlogn)`
                - 归并排序涉及递归，时间复杂度的分析稍微有点复杂
                - 不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式
                - **递归代码的时间复杂度计算：**
                - 假设对 n 个元素进行归并排序需要的时间是 `T(n)`，那分解成两个子数组排序的时间都是 `T(n/2)`。我们知道，`merge()` 函数合并两个有序子数组的时间复杂度是 `O(n)`，归并排序的时间复杂度的计算公式就是：
                    + `T(1) = C；` n=1时，只需要常量级的执行时间，所以表示为C。
                    + `T(n) = 2*T(n/2) + n； n>1` (后面的n是merge)
                    + 展开: `T(n) = 2 * (2*T(n/4)+n/2) + n = 4*T(n/4)+2n = ... = 2^k * T(n/(2^k)) + k*n`
                    + 当`T(n/(2^k))=T(1)`，即`n/(2^k) = 1`，`2^k=n`，此时`k = log2(n)`，此时`T(n) = Cn+nlog2(n)`
                    + 所以用`大O`标记法表示 T(n)就等于`O(nlogn)`，归并排序时间复杂度为`O(nlogn)`
                - 归并排序的执行效率和要排序的原始数据的有序程度无关，最好、最坏、平均情况，均为`O(nlogn)`
            * 空间复杂度`O(n)`
                - 合并函数，在合并两个有序数组为一个有序数组时，需要借助额外的存储空间
                - 尽管每次合并操作都需要申请额外的内存空间，但在合并完成之后，临时开辟的内存空间就被释放掉了。在任意时刻，CPU 只会有一个函数在执行，也就只会有一个临时的内存空间在使用。
                - 临时内存空间最大也不会超过 n 个数据的大小，所以空间复杂度是 `O(n)`
    - **快速排序**(Quick Sort)
        + 如果要排序数组中下标从 p 到 r 之间的一组数据，我们选择 p 到 r 之间的任意一个数据作为 pivot（分区点）。遍历 p 到 r 之间的数据，将小于 pivot 的放到左边，将大于 pivot 的放到右边，将 pivot 放到中间。经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。
        + 可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了
        + 递推公式
            * `quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)`
            * 终止条件：`p >= r`
        + 归并排序中有一个 `merge()` 合并函数，快速排序中有一个 `partition()` 分区函数
            * 分区函数`partition()`的实现：
                - 取区间最后一个元素作为pivot中心点，开始遍历区间并记录一个游标i，小于pivot的则和游标i所处的记录交换位置，并将游标+1，最后一次将pivot(即最后元素)和游标处元素交换位置
        + 算法分析
            * 原地排序
            * 不稳定
            * 时间复杂度
                - 如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间(*最好情况*)，则快排和归并排序的递推求解公式是一样的，同`O(nlogn)`
                    + `T(1) = C；` `n=1`时，只需要常量级的执行时间，所以表示为C。
                    + `T(n) = 2*T(n/2) + n； n>1`
                - 公式成立的前提是每次分区操作，我们选择的 pivot 都很合适，正好能将大区间对等地一分为二。但实际上这种情况是很难实现的
                - 考虑*最坏情况*，原数组完全有序。则需要进行大约 `n` 次分区操作，才能完成快排的整个过程。每次分区平均要扫描大约 `n/2` 个元素，这种情况下，快排的时间复杂度就从 `O(nlogn)` 退化成了 `O(n^2)`
                - *平均情况*，假设每次分区操作都将区间分成大小为 9:1 的两个小区间，则
                    + `T(1) = C；` n=1时，只需要常量级的执行时间，所以表示为C。
                    + `T(n) = T(n/10) + T(9*n/10) + n； n>1`
                    + 递推求解的过程非常复杂，直接给出下面的结论：
                - `T(n)` 在大部分情况下的时间复杂度都可以做到 `O(nlogn)`，只有在极端情况下，才会退化到 `O(n^2)`
                    + 而且也有很多方法将退化到`O(n^2)`的概率降到很低，后面章节会有介绍
    - 解答开篇：如何用快排思想在`O(n)`时间复杂度内求无序数组中的第 K 大元素
        + 比如，`4， 2， 5， 12， 3` 这样一组数据，第 3 大元素就是 4
        + 选择数组区间 A[0…n-1]的最后一个元素 A[n-1]作为 pivot，对数组 A[0…n-1]原地分区，这样数组就分成了三部分，A[0…p-1]、A[p]、A[p+1…n-1]。
        + 如果 `p+1=K`，那 A[p]就是要求解的元素；如果 `K>p+1`, 说明第 K 大元素出现在 A[p+1…n-1]区间，我们再按照上面的思路递归地在 A[p+1…n-1]这个区间内查找。同理，如果`K<p+1`，那我们就在 A[0…p-1]区间查找
        + 复杂度分析：
            * 第一次分区查找，我们需要对大小为 n 的数组执行分区操作，需要遍历 n 个元素。第二次分区查找，我们只需要对大小为 n/2 的数组执行分区操作，需要遍历 n/2 个元素。依次类推，分区遍历元素的个数分别为、n/2、n/4、n/8、n/16.……直到区间缩小为 1
            * 把每次分区遍历的元素个数加起来，就是：`n+n/2+n/4+n/8+…+1`，等比数列求和得到`2n-1`，因此时间复杂度为`O(n)`
        + 代码查看：[LeetCode/sort/sort_algo_test.go ](https://github.com/xiaodongQ/LeetCode/blob/master/sort/sort_algo_test.go)，搜索：TestFindKTop
        + 暴力方法：
            * 每次取数组最小值，移动到数组最前面，然后在剩下数据中再找最小值移动，依次类推，K次即得到第K大的数。复杂度`O(K*n)`，K比较小时时间复杂度确实为`O(n)`，但若K=n/2或n，则为`O(n^2)`
* [13 | 线性排序：如何根据年龄给100万用户数据排序？](https://time.geekbang.org/column/article/42038)
    - 讲三种时间复杂度是 `O(n)` 的排序算法：桶排序、计数排序、基数排序
    - 因为这些排序算法的时间复杂度是线性的，所以我们把这类排序算法叫作**线性排序**（Linear sort）
        + 之所以能做到线性的时间复杂度，主要原因是，这三个算法是非基于比较的排序算法，都不涉及元素之间的比较操作
        + 这几种排序算法对要排序的数据要求很苛刻，重点是掌握其使用场景
    - **桶排序**(Bucket Sort)
        + 核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了
        + 时间复杂度为`O(n)`，分析：
            * 如果要排序的数据有 n 个，我们把它们*均匀*地划分到 m 个桶内，每个桶里就有 `k=n/m` 个元素。
            * 每个桶内部使用快速排序，时间复杂度为 `O(k * logk)`
            * m 个桶排序的时间复杂度就是 `O(m * k * logk)`，因为 k=n/m，所以整个桶排序的时间复杂度就是 `O(n*log(n/m))`。
            * 当桶的个数 m 接近数据个数 n 时，`log(n/m)` 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 `O(n)`
        + 实际上，桶排序对要排序数据的要求是非常苛刻的
            * 首先，要排序的数据需要很容易就能划分成 m 个桶，并且，桶与桶之间有着天然的大小顺序
            * 其次，数据在各个桶之间的分布是比较均匀的。如果数据经过桶的划分之后，有些桶里的数据非常多，有些非常少，很不平均，那桶内数据排序的时间复杂度就不是常量级了。在极端情况下，如果数据都被划分到一个桶里，那就退化为 `O(nlogn)` 的排序算法了
        + 桶排序比较适合用在外部排序中。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。
            * 比如说有 10GB 的订单数据，希望按订单金额（假设金额都是正整数）进行排序，但是内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。
            * 解决：
                - 先扫描一遍文件，看订单金额所处的数据范围。假设经过扫描之后我们得到，订单金额最小是 1 元，最大是 10 万元。
                - 将所有订单根据金额划分到 100 个桶里，第一个桶我们存储金额在 1 元到 1000 元之内的订单，第二桶存储金额在 1001 元到 2000 元之内的订单，以此类推
                - 理想的情况下，如果订单金额在 1 到 10 万之间均匀分布，那订单会被均匀划分到 100 个文件中，每个小文件中存储大约 100MB 的订单数据，我们就可以将这 100 个小文件*依次*放到内存中，用快排来排序
                - 等所有文件都排好序之后，我们只需要按照文件编号，从小到大依次读取每个小文件中的订单数据，并将其写入到一个文件中，那这个文件中存储的就是按照金额从小到大排序的订单数据了
            * 不过，订单按金额并不一定是均匀分布的。有可能某个金额区间的数据特别多，划分之后对应的文件就会很大。针对这些划分之后还是比较大的文件，我们可以继续划分，比如，订单金额在 1 元到 1000 元之间的比较多，我们就将这个区间继续划分为 10 个小区间，1 元到 100 元，101 元到 200 元，201 元到 300 元…901 元到 1000 元，直到所有的文件都能读入内存为止
    - **计数排序**（Counting sort）
        + 计数排序其实是桶排序的一种特殊情况
        + 当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间
        + 计数排序跟桶排序非常类似，只是桶的大小粒度不一样
        + e.g. 高考查分数系统显示成绩以及所在省的排名。如果你所在的省有 50 万考生，如何通过成绩快速排序得出名次？
            * 考生的满分是 900 分，最小是 0 分，这个数据的范围很小，所以我们可以分成 901 个桶，对应分数从 0 分到 900 分。根据考生的成绩，我们将这 50 万考生划分到这 901 个桶里
            * 桶内的数据都是分数相同的考生，所以并不需要再进行排序。只需要依次扫描每个桶，将桶内的考生依次输出到一个数组中，就实现了 50 万考生的排序。因为只涉及扫描遍历操作，所以时间复杂度是 `O(n)`
        + 如何快速计算出，每个分数的考生在有序数组中对应的存储位置呢？
            * 简化上面的数据：
                - 8个考生，分数在0-5之间，考生分数(`A[8]`)分别为：{2，5，3，0，2，3，0，3}
                - 进行计数排序，6个桶(`C[6]`)分别存分数0-5，则桶内容为：{2, 0, 2, 3, 0, 1} (C[0]存分数为0的考生个数，依次类推)
                - 排序之后的数组为`R[8]`，取桶内各分数计数的项填充，为：{0, 0, 2, 2, 3, 3, 3, 5}
            * 思路：
                - 对`C[6]`数组求和，则`C[k]`就为小于等于分数k的考生个数，得到新的`C[6]`为：{2, 2, 4, 7, 7, 8}
            * 从后到前依次扫描数组`A`，扫描到n时就从数组`C`取下标为n的值，e.g. 扫描到3，取`C[3]`得到7，即分数小于等于3的数有7个，即3是数组`R`的第7个元素(下标为6)，3放入数组`R`中，小于等于3的元素就剩下6个，`C[3]`需要减1，变成6，即`C`数组为{2, 2, 4, 6, 7, 8}。完成步骤后`R`为{x, x, x, x, x, x, 3, x}
                - 依此类推，继续从后到前扫描数组`A`：
                - 扫描到0，C[0]得到2，小于等于0的有2个，即0为R的第2个元素，存入到R[1]，并将C[0]减1，`C`变成{1, 2, 4, 6, 7, 8}
                    + 完成步骤后`R`为{x, 0, x, x, x, x, 3, x}
                - 扫描到3，C[3]得到6，为R的第6个元素，存入到R[5]，C[3]减1，`C`变成{1, 2, 4, 5, 7, 8}
                    + 完成步骤后`R`为{x, 0, x, x, x, 3, 3, x}
                - 扫描到2，C[2]得到4，2为R的第4个元素，存入到R[3]，`C`变成{1, 2, 3, 5, 7, 8}
                    + 完成步骤后`R`为{x, 0, x, 2, x, 3, 3, x}
                - 依次扫描完数组`A`，数组`R`就是有序的
                    + 0: `R`{0, 0, x, 2, x, 3, 3, x}，`C`{0, 2, 3, 5, 7, 8}
                    + 3: `R`{0, 0, x, 2, 3, 3, 3, x}，`C`{0, 2, 3, 4, 7, 8}
                    + 5: `R`{0, 0, x, 2, 3, 3, 3, 5}, `C`{0, 2, 3, 4, 7, 7}
                    + 2: `R`{0, 0, 2, 2, 3, 3, 3, 5}, `C`{0, 2, 2, 4, 7, 7}
        + 利用另外一个数组来计数的实现方式，这也是为什么这种排序算法叫计数排序的原因
        + 总结
            * 计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。
            * 而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数
    - **基数排序**（Radix sort）
        + 排序问题：假设我们有 10 万个手机号码(11位手机号)，希望将这 10 万个手机号码从小到大排序，有什么比较快速的排序方法呢？
            * 快排，`O(nlogn)`
            * 桶排序、计数排序，由于11位数据量太大，不合适
        + `O(n)`的排序思路
            * 先按照最后一位来排序手机号码，然后，再按照倒数第二位重新排序，以此类推，最后按照第一位重新排序。经过 11 次排序之后，手机号码就都有序了
            * 根据每一位来排序，可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到 `O(n)`
            * 如果要排序的数据有 k 位，那我们就需要 k 次桶排序或者计数排序，总的时间复杂度是 `O(k*n)`。当 k 不大的时候，比如手机号码排序的例子，k 最大就是 11，所以基数排序的时间复杂度就近似于 `O(n)`
        + 有时候要排序的数据并不都是等长的，可以把所有的数据补齐到相同长度，位数不够的可以在后面补“0”
            * e.g. 牛津字典所有英文单词排序，最大长度个45字母，不足45字母的单词，在后面补"0"，不影响原有相对顺序(ASCII字母都大于"0")
        + 总结
            * 基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果 a 数据的高位比 b 数据大，那剩下的低位就不用比较了。
            * 除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到`O(n)` 了
    - 解答开篇：如何根据年龄给 100 万用户排序？
        * 归并、快排，`O(nlogn)`
        * 假设最小1岁，最大年龄120岁，遍历100万用户，按年龄划分到120个桶里，再依次扫描取出即可
            + 桶排序，放到120个桶里，依次扫描保存到有序位置
            + 计数排序(数据范围不大，先新建100万用户的存储空间，再找位置进行填入)：120桶中，只是记录数量，然后用计数排序的方法，从后往前遍历100万用户，算出存放到有序数组的位置进行存储
            + 感觉直接用桶排序即可，不必要用计数排序
    - 思考题：假设我们现在需要对 D，a，F，B，c，A，z这个字符串进行排序，要求将其中所有小写字母都排在大写字母的前面，但小写字母内部和大写字母内部不要求有序。比如经过排序之后为 a，c，z，D，F，B，A，这个如何来实现呢？如果字符串中存储的不仅有大小写字母，还有数字。要将小写字母的放到前面，大写字母放在最后，数字放在中间，不用排序算法，又该怎么解决呢？
        * 桶排序，3个桶，小写、大写、数字，分完后依次读取，空间复杂度`O(3n)`，时间`O(2n)` (若算趋势即为`O(n)`)
        * 快排思想，中心点假设为`Z`，<='Z'的分一个区，>'Z'的分一个区，若还有数字，则<='Z'的再进行一次分区，<='9'(ASCII 'a'>'A')
* [14 | 排序优化：如何实现一个通用的、高性能的排序函数？](https://time.geekbang.org/column/article/42359)
    - 线性排序算法的时间复杂度比较低，适用场景比较特殊。所以如果要写一个通用的排序函数，不能选择线性排序算法
    - 如果对小规模数据进行排序，可以选择时间复杂度是 `O(n^2)` 的算法；如果对大规模数据进行排序，时间复杂度是 `O(nlogn)` 的算法更加高效
    - 所以，为了兼顾任意规模数据的排序，一般都会首选时间复杂度是 `O(nlogn)` 的排序算法来实现排序函数
        + 归并排序并不是原地排序算法，空间复杂度是 `O(n)`
        + 快速排序比较适合来实现排序函数
    - 快速排序在最坏情况下的时间复杂度是 `O(n^2)`，如何优化快速排序：
        + 如果数据原来就是有序的或者接近有序的，每次分区点都选择最后一个数据，那快速排序算法就会变得非常糟糕，时间复杂度就会退化为 `O(n^2)`
            * 这种 `O(n^2)` 时间复杂度出现的主要原因还是因为分区点选的不够合理
            * 最理想的分区点是：被分区点分开的两个分区中，数据的数量差不多
        + 两个比较常用、比较简单的分区算法
            * 三数取中法
                - 从区间的首、尾、中间，分别取出一个数，然后对比大小，取这 3 个数的中间值作为分区点
                - 但是，如果要排序的数组比较大，那“三数取中”可能就不够了，可能要“五数取中”或者“十数取中”
            * 随机法
                - 随机法就是每次从要排序的区间中，随机选择一个元素作为分区点
                - 这种方法并不能保证每次分区点都选的比较好，但是从概率的角度来看，也不大可能会出现每次分区点都选的很差的情况，所以平均情况下，这样选的分区点是比较好的
        + 为了避免快速排序里，递归过深而堆栈过小，导致堆栈溢出，我们有两种解决办法：
            * 第一种是限制递归深度。一旦递归过深，超过了我们事先设定的阈值，就停止递归。
            * 第二种是通过在堆上模拟实现一个函数调用栈，手动模拟递归压栈、出栈的过程，这样就没有了系统栈大小的限制。(归纳法？)
    - Glibc 中的 `qsort()`
        + `qsort()` 会优先使用归并排序来排序输入数据，因为归并排序的空间复杂度是` O(n)`，所以对于小数据量的排序，比如 1KB、2KB 等，归并排序额外需要 1KB、2KB 的内存空间，这个问题不大
            * 空间换时间
        + 要排序的数据量比较大的时候，`qsort()` 会改为用快速排序算法来排序
            * `qsort()` 选择分区点的方法就是“三数取中法”(Go里面的排序中的快排部分，也是用三数取中法，对数据量>40时有其他处理)
        + 对于递归太深会导致堆栈溢出的问题，`qsort()` 是通过自己实现一个堆上的栈，手动模拟递归来解决的
        + `qsort()` 并不仅仅用到了归并排序和快速排序，它还用到了插入排序。在快速排序的过程中，当要排序的区间中，元素的个数小于等于 4 时，`qsort()` 就退化为插入排序，不再继续用递归来做快速排序
            * 在小规模数据面前，`O(n^2)` 时间复杂度的算法并不一定比 `O(nlogn)` 的算法执行时间长
            * 在大 O 复杂度表示法中，我们会省略低阶、系数和常数，也就是说，`O(nlogn)` 在没有省略低阶、系数、常数之前可能是 `O(knlogn + c)`，而且 k 和 c 有可能还是一个比较大的数
    - Go中的`Sort` (sort包中)
        + 也是用到快排，除此之外还用到堆排序、插入排序(希尔排序)
        + `func quickSort(data Interface, a, b, maxDepth int)`
            * maxDepth为(数据长度n)：`2 * ceil(lg(n+1))`
                - 即 2 * 以2为底的n+1的对数(不为整数则向上取整)，实现时循环右移一位(即除2)提高效率
                - 用maxDepth来判断排序中，是否需要切换到堆排序
            * 当本次传入数据数据范围`>12`，
            * `func doPivot(data Interface, lo, hi int) (midlo, midhi int)` 分区函数
                - 选取pivot中心点，用三数取中法 `func medianOfThree(data Interface, m1, m0, m2 int)`
                    * m1开始，m0中间，m2结束(`len-1`)
                    * 首先判断m1和m0，若`m1位置的数<m0位置的数`(简写为`m1<m0`)，则交换m1和m0，即保证 `开始m1>=中间m0`
                    * 而后判断开始m1和结束m2，保证 `开始m1<=结束m2`且`开始m1>=中间m0`
                        + 若`m1>m2`则m1和m2要交换位置，然后m1还要和m0再比较一次，因为上一步骤的相对顺序在本次交换之后有可能改变了
                    * 最后结果是：开始m1的位置，存的是三个位置中中间的数，且`中间位置m0<=结束位置m2`
                    * 中心点取m1开始位置，即中间值
                - 分区操作有点复杂。。。考虑性能上的提升做了一些处理，自我觉得可读性很不好
        + Go里面使用排序的方式：
            * `Sort`函数的定义：`func Sort(data Interface)`
                - 用的`Interface`是sort中定义的接口类型(`type Interface interface`)，所以可以传不同类型的数据进行排序
            * 如果不是基本类型，则需要自己实现自定义类型(`struct`)对`Interface`中方法的绑定，其中包含3个方法：
                - `Len() int`，元素个数
                - `Less(i, j int) bool`，判断`data[i]<data[j]`
                - `Swap(i, j int)`，交换位置
            * sort标准库里面实现了几种基本类型的接口
                - `func Ints(a []int) { Sort(IntSlice(a)) }`
                    + 针对int类型的slice排序，`IntSlice`定义为：`type IntSlice []int`
                    + 是否已排序(升序) `func IntsAreSorted(a []int) bool { return IsSorted(IntSlice(a)) }`
                - `func Float64s(a []float64) { Sort(Float64Slice(a)) }`
                    + 针对float64类型的slice排序
                    + 是否已排序 `func Float64sAreSorted(a []float64) bool { return IsSorted(Float64Slice(a)) }`
                - `func Strings(a []string) { Sort(StringSlice(a)) }`
                    + 针对string类型的slice排序，用string的`==`, `>`, `<` (注意`strings.Compare()`主要是为了保证包的完备性，一般比内建的`==`等效率低)
                    + 是否已排序 `func StringsAreSorted(a []string) bool { return IsSorted(StringSlice(a)) }`
            * `Sort`默认是升序排序，若要降序排序，则调整一下：`Less(i, j int) bool`
                - sort包里对这个操作做了封装：
                    + `func Reverse(data Interface) Interface` 会返回一个`return &reverse{data}`
                    + `reverse struct`内部类型对原Interface类型进行封装(将其作为`reverse struct`的唯一成员)，然后将`Less`方法调整后的实现绑定到`reverse`类型上，这样调用`Less`时，就会用到新的`Less`(内部类型中的`Less`会隐藏，可进一步了解：嵌套匿名接口，可用来实现继承)
                    + `Interface`的其他方法(`Len`，`Swap`)，`reverse`还是存在的
                - 使用方式：
                    + `sort.Sort( sort.Reverse(sort.IntSlice(xdIntList)) )`
                    + 用`sort.Reverse`来封装一下原始数据，然后调用`sort.Sort`即可进行降序排序
            * 自定义类型的排序
                - 参考：[go语言的排序、结构体排序](https://www.cnblogs.com/wangchaowei/p/7802811.html)
                - `sort.Sort(XdSlice(a))` 定义一个`XdSlice struct`类型，然后实现接口`Len`、`Less`、`Swap`
* [15 | 二分查找（上）：如何用最省内存的方式实现快速查找功能？](https://time.geekbang.org/column/article/42520)
    - 二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0
    - `n/(2^k) = 1`即 `k = log(n)`时，完成最后元素的查找，即时间复杂度`O(logn)`
        + 二分查找是我们目前为止遇到的第一个时间复杂度为 `O(logn)` 的算法
        + `O(logn)` 这种对数时间复杂度。这是一种极其高效的时间复杂度，有的时候甚至比时间复杂度是常量级 `O(1)` 的算法还要高效






* 极客时间视频课程：[7天入门数据结构与算法](https://u.geekbang.org/lesson/7?article=159518)
    - [数据结构脑图](https://naotu.baidu.com/file/b832f043e2ead159d584cca4efb19703?token=7a6a56eb2630548c)
    - [算法脑图](https://naotu.baidu.com/file/0a53d3a5343bd86375f348b2831d3610?token=5ab1de1c90d5f3ec)

知识简洁记忆

听七牛云许式伟老师的架构课里说到，“架构能力的提升，本质上是对你的知识脉络的反复梳理与融会贯通的过程”。
目标是把这些内容先读厚，再读薄，融会贯通。

* 数据结构(分三大块，一维数据结构、二维数据结构、特殊数据结构)
    - 一维：
        + 基础型
            * 数组 array
            * 链表 linked list
        + 高级
            * 栈 stack
            * 队列 queue
            * 双端队列 deque
            * 集合 set
            * 映射 map (hash or map)
    - 二维(从一维泛化过来)：
        + 基础型
            * 树 tree
            * 图 graph
        + 高级(在树的基础上加了很多判断)
            * 二叉搜索树 binary search tree (red-black tree, AVL)
            * 堆 heap
            * 并查集 disjoint set
            * 字典树 Trie
    - 特殊(用于工程中特定的场景)
        + 位运算 Bitwise
        + 布隆过滤器 BloomFilter
        + LRU Cache 缓存
* 算法(8大点，前三点是算法最基础的地方，)
    - 分支 Branch: if-else/switch
    - 循环 Iteration: for/while loop
    - 递归 Recursion: Divide & Conquer/Backtrace
    - 搜索 Search: 深度优先搜索 Depth first search/广度优先搜索 Breadth first search/启发式搜索 A*
    - 动态规划 Dynamic Programming
    - 二分查找 Binary Search
    - 贪心 Greedy
    - 数学 Math, 几何 Geometry(英 /dʒiˈɒmətri/)
