## 数据结构与算法之美

* 极客时间专栏：[数据结构与算法之美](https://time.geekbang.org/column/intro/126)
    - 学习过程中的代码实践，也放在LeetCode的练习里：[xiaodongQ/LeetCode](https://github.com/xiaodongQ/LeetCode)
* 复杂度分析
    - 复杂度分析是整个算法学习的精髓，只要掌握了它，数据结构和算法的内容基本上就掌握了一半
    - 你可能会有些疑惑，我把代码跑一遍，通过统计、监控，就能得到算法执行的时间和占用的内存大小。为什么还要做时间、空间复杂度分析呢？这种分析方法能比我实实在在跑一遍得到的数据更准确吗？这种方式叫`事后统计法`。但是，这种统计方法有非常大的局限性。
        + 1. 测试结果非常依赖测试环境
            * 换到另一台机器上时，可能会有截然相反的结果
        + 2. 测试结果受数据规模的影响很大
            * 对同一个排序算法，待排序数据的有序度不一样，排序的执行时间就会有很大的差别
            * 除此之外，如果测试数据规模太小，测试结果可能无法真实地反应算法的性能
            * 我们需要一个不用具体的测试数据来测试，就可以粗略地估计算法的执行效率的方法
    - `大 O 复杂度表示法`
        + 大 O 时间复杂度实际上并不具体表示代码真正的执行时间，而是表示代码执行时间随数据规模增长的变化趋势，所以，也叫作渐进时间复杂度(asymptotic time complexity)，简称`时间复杂度`
    - 如何分析一段代码的时间复杂度？三个比较**实用的方法**：
        + 1. 只关注循环执行次数最多的一段代码
        + 2. 加法法则：总复杂度等于量级最大的那段代码的复杂度
            * 如果 `T1(n)=O(f(n))`，`T2(n)=O(g(n))`；那么 `T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n)))`
        + 3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
            * 如果 `T1(n)=O(f(n))`，`T2(n)=O(g(n))`；那么 `T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))`
    - 几种常见时间复杂度实例分析
        + 常见的复杂度量级并不多：`O(1)`、`O(logn)`、`O(n)`、`O(nlogn)`、`O(n^2)`/`O(n^3)`/.../`O(n^k)`、`O(2^n)`、`O(n!)`
        + 上述复杂度粗略地分为两类：*多项式量级*和*非多项式量级*。
            * 其中，非多项式量级只有两个：`O(2^n)` 和 `O(n!)`
                - 我们把时间复杂度为非多项式量级的算法问题叫作*NP（Non-Deterministic Polynomial，非确定多项式）问题*
                - 当数据规模 n 越来越大时，非多项式量级算法的执行时间会急剧增加，求解问题的执行时间会无限增长
        + 主要看几种常见的*多项式时间复杂度*
            * 常数阶`O(1)`：只要代码的执行时间不随 n 的增大而增长，这样代码的时间复杂度我们都记作 O(1)
            * 对数阶`O(logn)`、线性对数阶`O(nlogn)`
                - 实际上，不管是以 2 为底、以 3 为底，还是以 10 为底，我们可以把所有对数阶的时间复杂度都记为 O(logn)。
                - 由于对数之间是可以互相转换的，`log3(n)`(此处为以3为底n的对数，由于编辑器排版问题展示不好位置) 就等于 `log3(2) * log2(n)`，所以 `O(log3n) = O(C * log2n)`，其中 `C=log3(2)` 是一个常量
                - 在采用大 O 标记复杂度的时候，可以忽略系数，即 `O(Cf(n)) = O(f(n))`，所以，`O(log2(n))` 就等于 `O(log3(n))`，因此，在对数阶时间复杂度的表示方法里，我们忽略对数的“底”，统一表示为 `O(logn)`
                - 如果一段代码的时间复杂度是 O(logn)，我们循环执行 n 遍，时间复杂度就是 O(nlogn) 了，O(nlogn) 也是一种非常常见的算法时间复杂度。比如，归并排序、快速排序的时间复杂度都是 O(nlogn)
            * `O(m+n)`、`O(m*n)`
                - 跟前面都不一样的时间复杂度，代码的复杂度由两个数据的规模来决定
                - 我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个
    - *空间复杂度分析*
        + 类比一下时间复杂度，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系
        + 常见的空间复杂度就是 `O(1)`、`O(n)`、`O(n^2)`，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。而且，空间复杂度分析比时间复杂度分析要简单很多
    - 四个复杂度分析方面的知识点
        + *最好情况时间复杂度（best case time complexity）*
            * 在最理想的情况下，执行这段代码的时间复杂度。
        + *最坏情况时间复杂度（worst case time complexity）*
            * 在最糟糕的情况下，执行这段代码的时间复杂度
        + *平均情况时间复杂度（average case time complexity）*
            * 最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，我们需要引入另一个概念：平均情况时间复杂度，后面简称为平均时间复杂度
            * 平均时间复杂度 的全称应该叫`加权平均时间复杂度`或者`期望时间复杂度`(将各种情况发生的概率考虑进去)
        + *均摊时间复杂度（amortized time complexity）*
            * 大部分情况下，我们并不需要区分最好、最坏、平均三种复杂度。平均复杂度只在某些特殊情况下才会用到，而均摊时间复杂度应用的场景比它更加特殊、更加有限
    - [03 | 复杂度分析（上）：如何分析、统计算法的执行效率和资源消耗？](https://time.geekbang.org/column/article/40036)
    - [04 | 复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度](https://time.geekbang.org/column/article/40447)
* [05 | 数组：为什么很多编程语言中数组都从0开始编号？](https://time.geekbang.org/column/article/40961)
    - 数组（Array）是一种线性表数据结构。它用一组`连续`的内存空间，来存储一组具有`相同类型`的数据
    - 数组为了保证内存数据连续性，会导致插入、删除操作比较低效
        + 插入时间复杂度分析
            * 假设数组的长度为 n，现在，如果我们需要将一个数据插入到数组中的第 k 个位置。为了把第 k 个位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位
                - 如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 `O(1)`
                - 但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 `O(n)`
                - 因为我们在每个位置插入元素的`概率是一样的`，所以平均情况时间复杂度为 (1+2+…n)/n=`O(n)`
            * 若数组中的数据是*有序*的，则在某位置插入一个新元素时，就必须按照刚才的方法搬移 k 之后的数据
            * 但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集合，则可`改进`为：
                - 在这种情况下，如果要将某个数据插入到第 k 个位置，为了避免大规模的数据搬移，我们还有一个简单的办法就是，*直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位置*
                - 利用这种处理技巧，在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 `O(1)`。这个处理思想在快排中也会用到
        + 删除操作复杂度分析
            * 跟插入数据类似，如果我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了
                - 和插入类似，如果删除数组末尾的数据，则最好情况时间复杂度为 `O(1)`；如果删除开头的数据，则最坏情况时间复杂度为 `O(n)`；平均情况时间复杂度也为 `O(n)`(删除末尾只操作1个数，删除开头操作n个数，第二个n-1...，删除每个元素概率为1/n，`(1+...+n)/n=(n+1)n/2n=(n+1)/2`)
            * 实际上，在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，删除的效率是不是会提高很多呢？
                - 可以*先记录*下已经删除的数据。每次的删除操作*并不是真正地搬移数据，只是记录数据已经被删除*。当数组没有更多空间存储数据时，我们再*触发执行一次*真正的删除操作，这样就大大减少了删除操作导致的数据搬移。(JVM 标记清除垃圾回收算法与之类似)
    - 开篇的问题：为什么大多数编程语言中，数组要从 0 开始编号，而不是从 1 开始呢？
        + 从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”
            * 如果用 a 来表示数组的首地址，a[0]就是偏移为 0 的位置，也就是首地址，a[k]就表示偏移 k 个 `type_size` 的位置，a[k]的内存地址只需要用这个公式：
                - `a[k]_address = base_address + k * type_size`
            * 如果数组从 1 开始计数，那我们计算数组元素 a[k]的内存地址就会变为
                - `a[k]_address = base_address + (k-1)*type_size`
            * 从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于CPU来说，就是多了一次减法指令。为了减少一次减法操作，数组选择了从 0 开始编号，而不是从 1 开始。
    - 在平时的业务开发中，我们可以直接使用编程语言提供的容器类(如Java的ArrayList)，但是，如果是特别底层的开发，直接使用数组可能会更合适
* [06 | 链表（上）：如何实现LRU缓存淘汰算法?](https://time.geekbang.org/column/article/41013)
    - 链表 Linked list
        + 相关概念：结点、后继指针next、头结点、尾结点
    - 缓存淘汰算法，常见策略：
        + 先进先出策略FIFO(First In, First Out)
        + 最少使用策略LFU(Least Frequently Used)
        + 最近最少使用策略LRU(Least Recently Used)
    - 针对链表的插入和删除，时间复杂度为`O(1)`
        + 只需要考虑相邻结点的指针改变
    - 三种最常见的链表结构
        + 单链表
            * 想访问第k个元素，只能根据指针一个结点一个结点依次遍历，时间复杂度`O(n)`
        + 双向链表
            * 每个结点不止有后继指针next，还有一个前驱指针prev指向前面的结点
            * 从结构上来看，双向链表可以支持`O(1)`时间复杂度的情况下找到前驱结点
                - 这也使双向链表在某些情况下的插入、删除等操作都要比单链表简单、高效
                - 单链表的插入、删除已经是O(1)时间复杂度了，还能再怎么高效？
                - 删除结点：
                    + `情况1.` 删除结点中“值等于某个给定值”的结点：尽管单纯的删除操作时间复杂度是O(1)，但**遍历查找**的时间是主要的耗时点，对应的时间复杂度为`O(n)`，根据时间复杂度分析中的加法法则，删除值等于给定值的结点对应的链表操作的*总时间复杂度*为`O(n)`
                    + `情况2.` 删除给定指针指向的结点：已经找到了某个结点q，但是要删除q需要知道它的前驱结点，而单链表并不支持直接获取前驱结点，所以，为了找到前驱结点，我们还是要从头结点开始遍历链表(`O(n)`)，而对于双向链表就不需要遍历了(`O(1)`)
                - 插入结点
                    + 和删除结点类似，对于单链表需要遍历其前驱结点以便插入，而双向链表则不需要
            * 除了插入、删除操作有优势之外，对于一个*有序链表*，双向链表的按值查询的效率也要比单链表高一些。
                - 因为，我们可以记录上次查找的位置p，每次查询时，根据要查找的值与p的大小关系，决定是往前还是往后查找，所以平均只需要查找一半的数据
                - Java的`LinkedHashMap`就用了双向链表这个结构(此外还用到散列表)
            * `空间换时间`的设计思想
                - 当内存空间充足的时候，如果我们更加追求代码的执行速度，我们就可以选择空间复杂度相对较高、但时间复杂度相对很低的算法或者数据结构。
                - 相反，如果内存比较紧缺，比如代码跑在手机或者单片机上，这个时候，就要反过来用时间换空间的设计思路
                - 缓存就是利用了空间换时间的设计思想
        + 循环链表
            * 单链表的尾结点指针指向空地址，而循环链表的尾节点指向链表的头结点
            * 和单链表相比，循环链表的优点是从链尾到链头比较方便
            * 和双向链表整合在一起，就是双向循环链表
    - 链表和数组
        + 它们插入、删除、随机访问操作的时间复杂度正好相反
        + 数组使用连续的内存空间，可以借助 CPU 的缓存机制，预读数组中的数据，所以访问效率更高
        + 数组的缺点是大小固定，一经声明就要占用整块连续内存空间，过大可能没有足够内存来分配，过小可能不够用而只能申请更大空间进行拷贝；
        + 链表本身没有大小的限制，天然地支持动态扩容，链接中描述为这是它与数组最大的区别
    - 解答开篇问题：基于*链表*实现 LRU 缓存淘汰算法
        + 维护一个有序单链表，越靠近链表尾部的结点是越早之前访问的。
        + 当有一个新数据被访问时，就从链表头开始遍历链表
            * a. 若链表中有该数据(表示已缓存在链表中)，则将该结点删除，并插入到链表头部
            * b. 若链表中未缓存在链表中
                - 缓存未满时：将该结点插入到链表头部
                - 缓存已满：则删除链表尾结点，将新的结点插入到链表头部
        + 时间复杂度：遍历`O(n)`，结点操作`O(1)`，因此缓存访问的时间复杂度为`O(n)`
            * 优化：可引入*散列表（Hash table）*(后续)，记录每个数据的位置，将复杂度降到`O(1)`
        + 基于*数组*实现LRU缓存淘汰算法的思路：
            * 维护一个数组缓存访问的数据，越靠近数组开始位置的元素为越早访问
            * 有新数据访问时，从数组尾部开始查找
                - 若该数据已缓存在数组中，则将其删除(?涉及后面的数据搬移)，新增到数组尾
                - 若不在，直接在数组尾新增
                    + 缓存满时，删除数组首结点，元素全部前移
            * 删除结点搬移数据想想都觉得太暴力了，查找`O(n)`，搬移`O(n)`，复杂度是链表2倍，虽然最后还是为`O(n)`
            * 优化思路：
                - 由于每次删除结点搬移其后面的数据太耗时，用一个新的数组Arr保存要删除结点的位置(需要额外空间了，空间换时间)
                - 还是上面的逻辑走，发现要删除结点则先不做删除，而是记录位置到Arr，待缓存的空间不够时再做一次性删除和搬移处理
                - 复杂度：
                    + 由于需要额外空间，可指定固定的空间，当空间满时也触发批量删除，固定空间则空间复杂度也为`O(1)`
                    + 时间复杂度：还是需要查找和搬移，不过性能比上面要好些，虽然还是`O(n)`
    - 思考题：如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？
        + 解法
            * a. 将链表各结点的值复制到数组中，再从首尾依次取结点比较是否相同，有不同则非回文
                - 空间复杂度O(n)，时间复杂度 复制O(n)+判断O(n/2)=O(n)
            * b. 双指针法，定义两个指针以步长1和以步长2移动，直到找到中间结点，中间结点之后的部分反转，再各自比较两部分，最后再反转还原
                - 空间复杂度O(1), 时间复杂度 中间结点O(n/2)+反转O(n/2)+比较O(n/2)+反转O(n/2) = O(n)
        + 代码参考github：[linkedlist_palindromic](https://github.com/xiaodongQ/LeetCode/blob/master/datastruct_algo/linkedlist_palindromic.go)
        + LeetCode有该题：[234. 回文链表](https://leetcode-cn.com/problems/palindrome-linked-list/submissions/)
    - 关于链表的更多操作可见LeetCode
        + 参考：[链表标签](https://leetcode-cn.com/problemset/all/?topicSlugs=linked-list)
    - 几个写链表代码相关的技巧
        + [07 | 链表（下）：如何轻松写出正确的链表代码？](https://time.geekbang.org/column/article/41149)
        + 技巧一：理解指针或引用的含义
        + 技巧二：警惕指针丢失和内存泄漏
            * a和b结点间插入结点x，假设p指向a: 则先`x->next = p->next`，再`p->next = x`，若顺序反了则链表后面的部分访问不到
            * 删除结点，对该结点的内存手动释放(C/C++里，有垃圾回收的语言如Java/Go则不需要)
        + 技巧三：利用哨兵简化实现难度
            * 不用哨兵的场景，针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理
                - 向链表的结点p后插入一个结点：
                    + `new_node->next = p->next;`，`p->next = new_node;`，
                    + 但若是向空链表插入第一个结点，上面逻辑就不行了，需要：`if (head == null) { head = new_node; }`
                - 删除链表的结点p的后一个结点：
                    + `p->next = p->next->next;`
                    + 但若要删除结点的是链表所剩的最后一个结点，需要：`if (p->next == null) { head = null; }`
            * 引入哨兵结点(sentinel)
                - 不管链表是否为空，head指针都指向该哨兵结点。把这种有哨兵结点的链表叫*带头链表*(没有则称不带头链表)
                    + 哨兵节点不存任何数据
                - 插入
                    + 向空链表插入第一个结点则：`sentinel->next = new_node;`
                - 删除
                    + 删除只剩一个结点的链表结点，`sentinel->next = null;`
                - 因为哨兵结点一直存在，所以插入第一个结点和插入其他结点，删除最后一个结点和删除其他结点，都可以统一为相同的代码实现逻辑了
        + 技巧四：重点留意边界条件处理
            * 如果链表为空时，代码是否能正常工作？
            * 如果链表只包含一个结点时，代码是否能正常工作？
            * 如果链表只包含两个结点时，代码是否能正常工作？
            * 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？
        + 技巧五：举例画图，辅助思考
            * 当写完代码之后，也可以举几个例子，画在纸上，照着代码走一遍，很容易就能发现代码中的 Bug
        + 技巧六：多写多练，没有捷径
            * 单链表反转、链表中环的检测、两个有序的链表合并、删除链表倒数第 n 个结点、求链表的中间结点
            * 链表操作相关的LeetCode个人练习链接：[链表相关操作](https://github.com/xiaodongQ/LeetCode/blob/master/linked_list/operate_linkedlist_test.go)
* [08 | 栈：如何实现浏览器的前进和后退功能？](https://time.geekbang.org/column/article/41222)
    - 栈既可以用数组来实现(*顺序栈*)，也可以用链表来实现(*链式栈*)。
    - 入栈和出栈只涉及栈顶个别数据操纵，时间复杂度为`O(1)`
    - 支持动态扩容的顺序栈
        + 当栈满了之后，就申请一个更大的数组，将原来的数据搬移到新数组中
        + 对于入栈操作来说，最好情况时间复杂度是 O(1)，最坏情况时间复杂度是 O(n)。
        + 平均情况下的时间复杂度，用`摊还分析法`分析：
            * 不满时，入栈`O(1)`
            * 当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存(假设满了扩展为2倍)，并且做 K 个数据的搬移操作，然后再入栈
            * 但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push(此处表示不涉及内存搬移的入栈操作) 操作就可以完成，这K次入栈总共涉及K个数据搬移和K次simple-push操作
            * 将 K 个数据搬移均摊到 K 次入栈操作，那每个入栈操作只需要一个数据搬移和一个 simple-push 操作。以此类推，入栈操作的均摊时间复杂度就为 `O(1)`。
    - 应用
        + 函数调用栈的应用
            * 参考示例链接中的代码调用，`main()`中调用了`add(int x, int y)`函数，`main`中定义的变量依次push入栈，然后是`add`的形参赋值后push入栈，而后add中定义的变量入栈，add中return时出栈
        + 表达式求值应用(编译器利用栈来实现表达式求值)
            * 编译器就是通过*两个栈*来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。
            * 我们从左向右遍历表达式，当遇到数字：我们就*直接压入操作数栈*；当遇到运算符：就与*运算符栈*的栈顶元素进行比较
                - 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；
                - 如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶*取 2 个操作数*，然后进行计算，*再把计算完的结果压入操作数栈*，继续比较。
                    + 注意此时遇到的运算符还没有压入栈，直到该运算符比运算符栈的栈顶运算符优先级高时，才将该运算符压入栈
            * 链接中用图示演示了一个简单示例：`3+5*8-6`，过程用文字说明如下(还是图示更清晰)：
                - `3`->操作数栈，`+`->运算符栈，`5`->操作数栈，`*`->运算符栈，`8`->操作数栈
                - `-`比运算符栈栈顶的`*`优先级低，则从操作数栈取`8`和`5`(出栈)，并取运算符栈栈顶`*`，得到`40`，压入操作数栈，此时操作数栈从栈顶向下为`40`，`3`
                - `-`和运算符栈当前栈顶`+`优先级相同，从操作数栈中取两个操作数`40`，`3`，取`+`，得到`43`(3+40=43，栈下面的元素作为操作符左边)压入操作数栈，此时操作数栈仅`43`一个栈帧
                - 将`-`压入运算符栈，`6`->操作数栈，表达式结束清空栈，从运算符栈取`-`，取两个操作数，得到`37`(43-6=37)
        + 括号匹配应用
            * 假设表达式中只包含三种括号，圆括号 ()、方括号[]和花括号{}，并且它们可以任意嵌套
                - 说明：`{[] ()[{}]}` 和 `[{()}([])]` 等为合法格式；`{[}()]` 或 `[({)]` 为不合法格式
            * 用栈来保存未匹配的左括号，从左到右依次扫描字符串。
                - 当扫描到左括号时，则将其压入栈中；
                - 当扫描到右括号时，从栈顶取出一个左括号
                - 如果能够匹配，比如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。
                - 如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式
            * 当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。
    - 解答开篇问题：如何实现浏览器的前进和后退功能？
        + 使用两个栈，X 和 Y，我们把首次浏览的页面依次压入栈 X，当点击*后退*按钮时，再依次从栈 X 中出栈，并将出栈的数据依次放入栈 Y
        + 当我们点击*前进*按钮时，我们依次*从栈 Y 中取出数据，放入栈 X 中*。当栈 X 中没有数据时，那就说明没有页面可以继续后退浏览了。当栈 Y 中没有数据，那就说明没有页面可以点击前进按钮浏览了
    - LeetCode：[栈相关题目](https://leetcode-cn.com/problemset/all/?topicSlugs=stack)
* [09 | 队列：队列在线程池等有限资源池中的应用](https://time.geekbang.org/column/article/41330)
    - 两个操作：入队`enqueue()` 和 出队`dequeue()`
    - 和栈类似，用数组实现的队列叫作`顺序队列`，用链表实现的队列叫作`链式队列`
    - 队列需要两个指针：一个是 head 指针，指向队头；一个是 tail 指针，指向队尾(tail不存数据)
        + 栈只需要一个栈顶指针
        + 入队时tail指针后移，出队时head指针后移
            * 入队时判断队列是否已满，出队时判断队列是否为空
    - 数组实现
        + 随着不停地进行入队、出队操作，head 和 tail(注意tail不存数据) 都会持续往后移动，当 tail 移动到最右边，即使数组中还有空闲空间，也无法继续往队列中添加数据了，如何解决？
            * 入队(enqueue)时，若队尾没有空闲空间了(tail==n)，则将数据搬移到队列头(需要队头有空闲空间，通过队头指针判断是否为第一个位置)
            * 入队O(1) (摊还分析法，head位置分别为1、n/2、n-1，搬移一次可支持后续n-head次入队)，出队O(1)
    - 链表实现
        + `enqueue`: tail->next=new_node, tail = tail->next
        + `dequeue`: head = head->next
    - 循环队列(可参考链接中的图示，更直观一些)
        + 避免上面数组实现方式的数据搬移
        + 最关键的是，确定好队空和队满的判定条件
            * 用数组实现的非循环队列中，队满条件：`tail==n`，队空：`head==tail`
            * 数组实现的循环队列中，队满条件：`(tail+1)%n==head`，队空还是：`head==tail`
                - 当队列满时，tail 指向的位置实际上是没有存储数据的。所以，循环队列会浪费一个数组的存储空间
        + `enqueue`: 判是否满 `if ((tail+1)%n == head)`，入队后 `tail = (tail+1)%n`
        + `dequeue`: 判是否空 `if (head == tail)`，出队后 `head = (head+1)%n`
    - 应用
        + 阻塞队列(生产者 - 消费者模型)
            * 在队列基础上增加了阻塞操作
                - 队列为空的时候，从队头取数据会被阻塞
                - 如果队列已经满了，那么插入数据的操作就会被阻塞
        + 并发队列
            * 在多线程情况下，会有多个线程同时操作队列，这个时候就会存在线程安全问题
            * 线程安全的队列我们叫作并发队列
                - 最简单直接的方式是给`enqueue`和`dequeue`操作加锁，但锁粒度大并发度会降低，同一时刻只允许一个存或取操作
                - 基于数组的循环队列，利用 `CAS` 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因
            * 后续讲 Disruptor时，会再涉及并发队列的应用
    - 开篇问题：线程池没有空闲线程时，新的任务请求线程资源时，线程池该如何处理？各种处理策略又是如何实现的呢？
        + 一般有两种处理策略
            * 第一种是非阻塞的处理方式，直接拒绝任务请求；
            * 另一种是阻塞的处理方式，将请求排队，等到有空闲线程时，取出排队的请求继续处理
        + 阻塞两种实现方式
            * 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长
                - 所以，针对响应时间比较敏感的系统，基于链表实现的无限排队的线程池是不合适的
            * 基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理
                - 需要设置一个合理的队列大小。队列太大导致等待的请求太多，队列太小会导致无法充分利用系统资源、发挥最大性能
    - 对于大部分资源有限的场景，当没有空闲资源时，基本上都可以通过“队列”这种数据结构来实现请求排队
        + 如数据库连接池
    - 笔记
        + C++里面的`condition_variable`，参考：[笔记](https://github.com/xiaodongQ/devNoteBackup/blob/master/%E5%90%84%E8%AF%AD%E8%A8%80%E8%AE%B0%E5%BD%95/C%2B%2B.md)，(搜`std::condition_variable`章节)
        + linux下的 `pthread_cond_t`，参考：[笔记](https://github.com/xiaodongQ/devNoteBackup/blob/master/%E5%90%84%E8%AF%AD%E8%A8%80%E8%AE%B0%E5%BD%95/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B.md)，(搜`## 条件变量(condition variable)`章节)
* [11 | 排序（上）：为什么插入排序比冒泡排序更受欢迎？](https://time.geekbang.org/column/article/41802)
    - 最经典的、最常用的一些排序算法：
        + 冒泡排序、插入排序、选择排序、
            * `O(n^2)`
        + 归并排序、快速排序、
            * `O(nlogn)`
        + 计数排序、基数排序、桶排序
            * `O(n)`
    - 思考：
        + 插入排序和冒泡排序的时间复杂度相同，都是`O(n^2)`，在实际的软件开发里，为什么我们更倾向于使用插入排序算法而不是冒泡排序算法呢？
    - 排序算法的**执行效率**，一般从这几个方面来衡量：
        + 1. 最好情况、最坏情况、平均情况时间复杂度
            * 同时要说出对应情况下要排序的原始数据是什么样的
                - 对于要排序的数据，有的接近有序，有的完全无序
        + 2. 时间复杂度的系数、常数 、低阶
            * 时间复杂度反应的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶
            * 但是实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据
            * 所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来
        + 3. 比较次数和交换（或移动）次数
            * 在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去
    - 排序算法的**内存消耗**
        + 针对排序算法的空间复杂度，还引入了一个新的概念，`原地排序（Sorted in place）`
        + 原地排序算法，就是特指空间复杂度是 `O(1)` 的排序算法
    - 排序算法的**稳定性**
        + 如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变，那么这个排序就是稳定的
        + 为什么要考察排序算法的稳定性呢？
            * 演示数据结构和算法时，很多是用整数举例，但真正的软件开发中，要排序的对象往往是一组对象，而只是根据某个key排序
            * 如果要依据两个要素的顺序排列，如示例中的 先按时间先后再按金额大小，第一遍稳定的排序算法之后已经是有时间先后了，第二遍稳定排序后，先后顺序并不会改变；若第二遍排序不稳定，则按金额排序后相同金额的顺序可能发生变化
    - **冒泡排序**（Bubble Sort）
        + 每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。
        + 一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作
            * 时间：`O(n^2)`，空间：`O(1)` (原地排序)
        + e.g.
            * `data := [...]int{3, 5, 4, 1, 2, 6}` 数组从小到大排列，第一次循环(冒泡)，依次比较相邻元素，会将6存在最后
            * 循环n次，遍历一遍所有元素进行：`if (data[j]>data[j+1]) swap`
                - 注意：每次内层遍历时，并不需要`len(data)-1`次，而是`len(data)-1-i`次(假设外层遍历下标i)，前面冒泡的数据并不必要再做比较
        + 对于上面的过程可以优化：
            * 当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作

* 极客时间视频课程：[7天入门数据结构与算法](https://u.geekbang.org/lesson/7?article=159518)
    - [数据结构脑图](https://naotu.baidu.com/file/b832f043e2ead159d584cca4efb19703?token=7a6a56eb2630548c)
    - [算法脑图](https://naotu.baidu.com/file/0a53d3a5343bd86375f348b2831d3610?token=5ab1de1c90d5f3ec)

知识简洁记忆

听七牛云许式伟老师的架构课里说到，“架构能力的提升，本质上是对你的知识脉络的反复梳理与融会贯通的过程”。
目标是把这些内容先读厚，再读薄，融会贯通。

* 数据结构(分三大块，一维数据结构、二维数据结构、特殊数据结构)
    - 一维：
        + 基础型
            * 数组 array
            * 链表 linked list
        + 高级
            * 栈 stack
            * 队列 queue
            * 双端队列 deque
            * 集合 set
            * 映射 map (hash or map)
    - 二维(从一维泛化过来)：
        + 基础型
            * 树 tree
            * 图 graph
        + 高级(在树的基础上加了很多判断)
            * 二叉搜索树 binary search tree (red-black tree, AVL)
            * 堆 heap
            * 并查集 disjoint set
            * 字典树 Trie
    - 特殊(用于工程中特定的场景)
        + 位运算 Bitwise
        + 布隆过滤器 BloomFilter
        + LRU Cache 缓存
* 算法(8大点，前三点是算法最基础的地方，)
    - 分支 Branch: if-else/switch
    - 循环 Iteration: for/while loop
    - 递归 Recursion: Divide & Conquer/Backtrace
    - 搜索 Search: 深度优先搜索 Depth first search/广度优先搜索 Breadth first search/启发式搜索 A*
    - 动态规划 Dynamic Programming
    - 二分查找 Binary Search
    - 贪心 Greedy
    - 数学 Math, 几何 Geometry(英 /dʒiˈɒmətri/)
