# 分布式和微服务


* [分布式系统漫谈一 Google三驾马车: GFS,mapreduce,Bigtable](http://blog.sina.com.cn/s/blog_4ed630e801000bi3.html)
    - 谈到分布式系统，就不得不提Google的三驾马车:`Google fs`,`Mapreduce`,`Bigtable`
        + Hadoop也有按照这三篇论文的开源Java实现:Hadoop对应Mapreduce, Hadoop Distributed File System (HDFS)对应Google fs,Hbase对应Bigtable
    - Google fs
        + GFS是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，提供容错功能
        + 中文相关介绍：[Google File System](https://zhuanlan.zhihu.com/p/28155582)
        + [google论文：GFS](https://www.cnblogs.com/billowkiller/archive/2013/04/10/3012085.html)
    - Mapreduce
        + Mapreduce是针对分布式并行计算的一套编程模型
        + 中文介绍：[Google MapReduce 论文详解](https://zhuanlan.zhihu.com/p/34849261)
    - Bigtable
        + 就像文件系统需要数据库来存储结构化数据一样，GFS也需要Bigtable来存储结构化数据
            * BigTable 是建立在 GFS ，Scheduler ，Lock Service 和 MapReduce 之上的
        + [Bigtable 论文详述](https://zhuanlan.zhihu.com/p/35687103)
* 部分了解博客(仅作了解，内容主观有争议)
    - [大数据那些事（2）：三驾马车之永垂不朽的GFS](https://zhuanlan.zhihu.com/p/24382357)
    - [大数据的那些事(3):三驾马车之坑人的MapReduce](https://zhuanlan.zhihu.com/p/24382810)

## Consul

* Consul
    - 分布式服务发现和配置
    - 协议：Mozilla Public License 2.0，只要该许可证的代码在单独的文件中，新增的其他文件可以不用开源
    - [用 Consul 来做服务注册与服务发现](https://segmentfault.com/a/1190000018731395)
        + 为什么需要有服务注册与服务发现？
            * 环境背景：分布式系统中有服务Service-A(S-A)和Service-B(S-B)，S-A调用S-B
            * 演进过程：服务规模小只有一个S-B (演进)-> 每个服务不止部署一个实例(此时多个S-B如何调用?) -> 引入Nginx -> 某服务实例挂掉后需要Nginx不向其分配请求 -> 引入"`服务注册和服务发现工具`" (架构图参考链接) -> 引入Consul(或其他工具)
        + 业界常用的服务注册与服务发现工具有 `ZooKeeper`、`etcd`、`Consul` 和 `Eureka`，Consul和其区别参考链接中的链接
        + Consul 作为一种分布式服务工具，为了避免单点故障常常以集群的方式进行部署，在 Consul 集群的节点中分为 `Server` 和 `Client` 两种节点（所有的节点也被称为`Agent`）
            * Server 节点保存数据，Client 节点负责健康检查及转发数据请求到 Server；
            * Server 节点有一个 `Leader` 节点和多个 `Follower` 节点，Leader 节点会将数据同步到 Follower 节点，在 Leader 节点挂掉的时候会启动选举机制产生一个新的 Leader。
            * Client 节点很轻量且无状态，它以 RPC 的方式向 Server 节点做读写请求的转发，此外也可以直接向 Server 节点发送读写请求。 Consul 的架构图参考链接。
    - [Consul 使用手册](http://www.liangxiansen.cn/2017/04/06/consul/)

* 官网文档：[HashiCorp Consul Documentation](https://www.consul.io/docs/index.html)
* 入门使用文档，[GETTING STARTED](https://learn.hashicorp.com/consul?track=getting-started#getting-started)
    - 安装
        + Download Consul：[官网下载地址](https://www.consul.io/downloads.html)
        + zip包中只有一个`consul`二进制文件(104M, `consul_1.6.2_linux_amd64`)，执行路径添加到PATH环境变量即可
    - 启动`Consul agent`
        + 每个`Consul agent`都运行在`server`或者`client`模式。
        + 每个Consul数据中心都至少有一个`server`，负责维护Consul的状态
            * 包括其他Consul server和client、什么服务可被发现、哪些服务可以和其他服务交互
            * 注意：强烈反对单服务器产品部署Consul

## hystrix

* hystrix
    - 服务熔断和降级

## 缓存

* [淘宝开源 Key/Value 结构数据存储系统 Tair 技术剖析](https://www.infoq.cn/article/taobao-tair/)
    - Tair 是由淘宝网自主开发的 Key/Value 结构数据存储系统，在淘宝网有着大规模的应用
    - 默认支持基于内存和文件的两种存储方式，分别和我们通常所说的缓存和持久化存储对应
- [技术选型系列 - Tair&Redis对比](https://cloud.tencent.com/developer/article/1371099)

## 一致性哈希

* 进一步学习可参考：[24 | 一致性哈希：如何高效地均衡负载？](https://time.geekbang.org/column/article/256780)
    - 以及自己对该文章的学习笔记(搜索`24 | 一致性哈希：如何高效地均衡负载？`)：[devNoteBackup/各语言记录/Linux性能优化实践.md](https://github.com/xiaodongQ/devNoteBackup/blob/master/%E5%90%84%E8%AF%AD%E8%A8%80%E8%AE%B0%E5%BD%95/Linux%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5.md)
* 一个Go实现版本：[naza/pkg/consistenthash/](https://github.com/q191201771/naza/tree/master/pkg/consistenthash)
    - 哈希函数默认(可更改)用的是：`crc32.ChecksumIEEE` (hash/crc32包)
        + 循环冗余校验（`Cyclic Redundancy Check`， `CRC`）是一种根据网络数据包或计算机文件等数据产生简短固定位数校验码的一种信道编码技术，主要用来检测或校验数据传输或者保存后可能出现的错误。它是利用除法及余数的原理来作错误侦测的
            * [CRC （循环冗余校验）](https://baike.baidu.com/item/CRC/1453359)
* 循环冗余校验（CRC）算法（`Cyclic Redundancy Check`， `CRC`）
    - [循环冗余校验（CRC）算法入门引导](https://blog.csdn.net/liyuanbhu/article/details/7882789)
    - 从`奇偶校验`说起
        + 所谓奇偶校验就是在发送的每一个字节后都加上一位，使得每个字节中1的个数为奇数个或偶数个
            * 比如我们要发送的字节是`0x1a`，二进制表示为`0001 1010`
            * 采用奇校验(让该字节二进制中1的个数凑成奇数个)，则在数据后补上个0(因为原来已经为3个1)，数据变为`0001 1010 0`，数据中1的个数为奇数个（3个）
            * 采用偶校验，则在数据后补上个1(为了凑成偶数个1，3+1)，数据变为`0001 1010 1`，数据中1的个数为偶数个（4个）
        + 接收方通过计算数据中1个数是否满足奇偶性来确定数据是否有错
        + 奇偶校验的缺点很明显
            * 首先，它对错误的检测概率大约只有50%。也就是只有一半的错误它能够检测出来。
            * 另外，每传输一个字节都要附加一位校验位，对传输效率的影响很大
            * 因此，在高速数据通讯中很少采用奇偶校验
        + 奇偶校验优点也很明显
            * 它很简单，因此可以用硬件来实现，这样可以减少软件的负担。
            * 因此，奇偶校验也被广泛的应用着
        + 之所以从奇偶校验说起，是因为这种校验方式最简单，而且后面将会知道奇偶校验其实就是CRC 校验的一种(`CRC-1`)
    - `累加和校验`
        + 另一种常见的校验方式是累加和校验
        + 所谓累加和校验实现方式有很多种，最常用的一种是在一次通讯数据包的最后*加入一个字节的校验数据*。这个字节内容为前面数据包中全部数据的*忽略进位的按字节累加和*
            * e.g. 要传输的信息为： `6、23、4`，加上校验和后的数据包：`6、23、4、33`
        + 累加和校验由于实现起来非常简单，也被广泛的采用。
            * 但是这种校验方式的检错能力也比较一般，对于单字节的校验和大概有`1/256`的概率将原本是错误的通讯数据误判为正确数据
        + 之所以这里介绍这种校验，是因为CRC校验在传输数据的形式上与累加和校验是相同的，都可以表示为：通讯数据 校验字节（也可能是多个字节）
    - 初识 `CRC 算法`
        + CRC算法的基本思想是将传输的数据当做一个位数很长的数。
            * 将这个数除以另一个数。得到的`余数`作为校验数据附加到原数据后面
        + 还是上面的例子数据
            * `6、23、4` 可以看做一个2进制数： 00000110 00010111 00000100
            * 假如被除数选9，二进制表示为：1001
            * 除法过程参考链接，可看到最后的余数为1。如果我们将这个余数作为校验和的话，传输的数据则是：`6、23、4、1`
            * CRC 算法和这个过程有点类似，不过采用的不是上面例子中的通常的这种除法
        + 在CRC算法中，将二进制数据流作为多项式的系数，然后进行的是`多项式的乘除法`
            * 乘法：
                - 比如有两个二进制数，分别为：1101 和1011
                - 1101 与如下的多项式相联系：`1x^3+1x^2+0x^1+1x^0=x^3+x^2+x^0`
                - 1011与如下的多项式相联系：`1x^3+0x^2+1x^1+1x^0=x^3+x^1+x^0`
                - 两个多项式的乘法：`(x^3+x^2+x^0)(x^3+x^1+x^0)=x^6+x^5+x^4+x^3+x^3+x^3+x^2+x^1+x^0`
                - 得到结果后，合并同类项时采用模2运算，所谓模2运算就是结果除以2后取余数，即`x^3+x^3+x^3`变为`x^3`(即异或)
                - 上面最终得到的多项式为：`x^6+x^5+x^4+x^3+x^2+x^1+x^0`，对应的二进制数:`111111`
            * 除法
                - 除法运算与上面给出的乘法概念类似，还是遇到加减的地方都用`异或`运算来代替
                - 例如要传输的数据为：1101011011，除数为：10011
                    + 在计算前先将原始数据后面填上4个0：11010110110000
                    + 位宽为4（W=4），按照CRC算法的要求，计算前要在原始数据后填上W个0，也就是4个0
                - 参考链接图示，每次相除时不需要考虑借位的问题(异或得到本次结果)，所以除法变简单了
                - 最后得到的余数就是`CRC 校验字`
            * 为了进行CRC运算，也就是这种特殊的除法运算，必须要指定个被除数，在CRC算法中，这个被除数有一个专有名称叫做“`生成多项式`”
                - 生成多项式的选取是个很有难度的问题，如果选的不好，那么检出错误的概率就会低很多
                - 这个问题已经被专家们研究了很长一段时间了，对于我们这些使用者来说，只要把现成的成果拿来用就行了
                - 最常用的几种生成多项式如下：
                    + `CRC8=X^8+X^5+X^4+X^0`，还有CRC16、CRC12等
                    + `CRC32=X^32+X^26+X^23+X^22+X^16+X^12+X^11+X^10+X^8+X^7+X^5+X^4+X^2+X^1+X^0`
                - 文献中提到的生成多项式经常会说到多项式的`位宽`（Width，简记为W），这个位宽不是多项式对应的二进制数的位数，而是位数减1
                    + 比如CRC8中用到的位宽为8的生成多项式，其实对应的二进制数有九位 100110001

## Raft

* Raft一致性算法论文的中文翻译：[寻找一种易于理解的一致性算法（扩展版）](https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md)
    - `Raft` 是一种为了管理复制日志的一致性算法。它提供了和 `Paxos` 算法相同的功能和性能，但是它的算法结构和 Paxos 不同，使得 Raft 算法更加容易理解并且更容易构建实际的系统
        + Raft 将一致性算法分解成了几个关键模块，`领导人选举`、`日志复制`和`安全性`
        + 同时它通过实施一个更强的一致性来减少需要考虑的状态的数量
            * 减少状态机的状态（相对于 Paxos，Raft 减少了非确定性和服务器互相处于非一致性的方式）
    - `复制状态机`
        + 一致性算法是从复制状态机的背景下提出的
        + 在这种方法中，一组服务器上的状态机产生相同状态的副本，并且在一些机器宕掉的情况下也可以继续运行。复制状态机在分布式系统中被用于解决很多容错的问题
            * 例如，大规模的系统中通常都有一个集群领导者，像 GFS、HDFS 和 RAMCloud，典型应用就是一个独立的的复制状态机去管理领导选举和存储配置信息并且在领导人宕机的情况下也要存活下来。比如 Chubby 和 ZooKeeper
        + 复制状态机通常都是基于复制日志实现的
            * 每一个服务器存储一个`包含一系列指令的日志`，并且按照日志的顺序进行执行
            * 每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列
    - 通过领导人的方式，Raft 将一致性问题分解成了三个相对独立的子问题
        + `领导选举`：当现存的领导人宕机的时候，一个新的领导人需要被选举出来
        + `日志复制`：领导人必须从客户端接收日志然后复制到集群中的其他节点，并且强制要求其他节点的日志保持和自己相同
        + `安全性`：在 Raft 中安全性的关键是状态机安全：如果有任何的服务器节点已经应用了一个确定的日志条目到它的状态机中，那么其他服务器节点不能在同一个日志索引位置应用一个不同的指令
    - 一个关于 Raft 一致性算法的浓缩总结（不包括成员变换和日志压缩）
        + 选举安全特性
            * 对于一个给定的任期号，最多只会有一个领导人被选举出来
        + 领导人只附加原则
            * 领导人绝对不会删除或者覆盖自己的日志，只会增加
        + 日志匹配原则
            * 如果两个日志在相同的索引位置的日志条目的任期号相同，那么我们就认为这个日志从头到这个索引位置之间全部完全相同
        + 领导人完全特性
            * 如果某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有领导人中
        + 状态机安全特性
            * 如果一个领导人已经将给定的索引值位置的日志条目应用到状态机中，那么其他任何的服务器在这个索引位置不会应用一个不同的日志
    - Raft 基础
        + 一个 Raft 集群包含若干个服务器节点；通常是 5 个，这允许整个系统容忍 2 个节点的失效
        + 在任何时刻，每一个服务器节点都处于这三个状态之一：领导人、跟随者或者候选人
            * 在通常情况下，系统中只有一个领导人并且其他的节点全部都是跟随者
            * 跟随者都是被动的：他们不会发送任何请求，只是简单的响应来自领导者或者候选人的请求。
                - 领导人处理所有的客户端请求（如果一个客户端和跟随者联系，那么跟随者会把请求重定向给领导人）
            * 第三种状态，候选人，是用来选举新领导人时使用
        + Raft 把时间分割成任意长度的`任期`
            * 任期用连续的整数标记。每一段任期从一次选举开始，一个或者多个候选人尝试成为领导者
            * 如果一个候选人赢得选举，然后他就在接下来的任期内充当领导人的职责
            * 在某些情况下，一次选举过程会造成选票的瓜分。在这种情况下，这一任期会以没有领导人结束；一个新的任期（和一次新的选举）会很快重新开始
                - Raft 保证了在一个给定的任期内，最多只有一个领导者
            * 任期在 Raft 算法中充当`逻辑时钟`的作用，这会允许服务器节点查明一些过期的信息比如陈旧的领导者
                - 每一个节点存储一个当前任期号，这一编号在整个时期内单调的增长。
                - 当服务器之间通信的时候会交换当前任期号；
                - 如果一个服务器的当前任期号比其他人小，那么他会更新自己的编号到较大的编号值。
                - 如果一个候选人或者领导者发现自己的任期号过期了，那么他会立即恢复成跟随者状态。
                - 如果一个节点接收到一个包含过期的任期号的请求，那么他会直接拒绝这个请求
        + Raft 算法中服务器节点之间通信使用`远程过程调用（RPCs）`，并且基本的一致性算法只需要两种类型的 RPCs
            * `请求投票`（RequestVote） RPCs 由候选人在选举期间发起
            * `附加条目`（AppendEntries）RPCs 由领导人发起，用来复制日志和提供一种心跳机制
    - 领导人选举
        + Raft 使用一种心跳机制来触发领导人选举。当服务器程序启动时，他们都是跟随者身份
            * *领导者*周期性的向所有跟随者`发送心跳包`（即不包含日志项内容的附加日志项 RPCs）来维持自己的权威
            * 如果一个跟随者在一段时间里没有接收到任何消息，也就是`选举超时`，那么他就会认为系统中没有可用的领导者,并且发起选举以选出新的领导者
        + 要开始一次选举过程，跟随者先要增加自己的当前任期号并且转换到候选人状态
            * 然后他会并行的向集群中的其他服务器节点发送请求投票的 RPCs 来给自己投票
            * 候选人会继续保持着当前状态直到以下三件事情之一发生：
                - (a) 他自己赢得了这次的选举
                    + 当一个候选人从整个集群的大多数服务器节点获得了针对同一个任期号的选票，那么他就赢得了这次选举并成为领导人
                    + 每一个服务器最多会对一个任期号投出一张选票，按照先来先服务的原则
                - (b) 其他的服务器成为领导者
                    + 在等待投票的时候，候选人可能会从其他的服务器接收到声明它是领导人的附加日志项 RPC。如果这个领导人的任期号（包含在此次的RPC中）不小于候选人当前的任期号，那么候选人会承认领导人合法并回到跟随者状态
                    + 如果此次 RPC 中的任期号比自己小，那么候选人就会拒绝这次的 RPC 并且继续保持候选人状态
                - (c) 一段时间之后没有任何一个获胜的人
                    + 如果有多个跟随者同时成为候选人，那么选票可能会被瓜分以至于没有候选人可以赢得大多数人的支持
                    + 当这种情况发生的时候，每一个候选人都会超时，然后通过增加当前任期号来开始一轮新的选举。然而，没有其他机制的话，选票可能会被无限的重复瓜分
                    + Raft 算法使用`随机选举超时时间`的方法来确保很少会发生选票瓜分的情况，就算发生也能很快的解决
                        * 为了阻止选票起初就被瓜分，选举超时时间是从一个固定的区间（例如 150-300 毫秒）随机选择
                        * 每一个候选人在开始一次选举的时候会重置一个随机的选举超时时间，然后在超时时间内等待投票的结果
    - 日志复制
        + 一旦一个领导人被选举出来，他就开始为客户端提供服务
        + 客户端的每一个请求都包含一条被复制状态机执行的指令。领导人把这条指令作为一条新的日志条目附加到`日志`中去，然后并行的发起附加条目 RPCs 给其他的服务器，让他们复制这条日志条目
            * 日志由有序序号标记的条目组成。每个条目都包含创建时的任期号（图中框中的数字），和一个状态机需要执行的指令
        + 领导人来决定什么时候把日志条目应用到状态机中是安全的；这种日志条目被称为`已提交`
            * Raft 算法保证所有已提交的日志条目都是持久化的并且最终会被所有可用的状态机执行
            * 在领导人将创建的日志条目复制到*大多数的服务器*上的时候，日志条目就会被提交
            * 一旦跟随者知道一条日志条目已经被提交，那么他也会将这个日志条目应用到本地的状态机中（按照日志的顺序）
        + 在 Raft 算法中，领导人处理不一致是通过强制跟随者直接复制自己的日志来解决
            * 这意味着在跟随者中的冲突的日志条目会被领导人的日志覆盖
            * 领导人针对每一个跟随者维护了一个nextIndex，这表示下一个需要发送给跟随者的日志条目的索引地址。当一个领导人刚获得权力的时候，他初始化所有的 nextIndex 值为自己的最后一条日志的 index 加 1
                - 如果一个跟随者的日志和领导人不一致，那么在下一次的附加日志 RPC 时的一致性检查就会失败
                - 在被跟随者拒绝之后，领导人就会减小 nextIndex 值并进行重试
                - 最终 nextIndex 会在某个位置使得领导人和跟随者的日志达成一致
                - 当这种情况发生，附加日志RPC就会成功，这时就会把跟随者冲突的日志条目全部删除并且加上领导人的日志。一旦附加日志 RPC 成功，那么跟随者的日志就会和领导人保持一致，并且在接下来的任期里一直继续保持
        + 通过这种机制，领导人在获得权力的时候就不需要任何特殊的操作来恢复一致性
            * 他只需要进行正常的操作，然后日志就能自动的在回复附加日志 RPC 的一致性检查失败的时候自动趋于一致。
            * 领导人从来不会覆盖或者删除自己的日志
    - 安全性
        + 前面描述了Raft算法是如何选举和复制日志的，到目前为止描述的机制并不能充分的保证每一个状态机会按照相同的顺序执行相同的指令
            * 例如，一个跟随者可能会进入不可用状态同时领导人已经提交了若干的日志条目，然后这个跟随者可能会被选举为领导人并且覆盖这些日志条目；因此，不同的状态机可能会执行不同的指令序列
        + 本节通过在领导选举的时候增加一些限制来完善 Raft 算法
            * 这一限制保证了任何的领导人对于给定的任期号，都拥有了之前任期的所有被提交的日志条目
        + `选举限制`
            * 在任何基于领导人的一致性算法中，领导人都必须存储所有已经提交的日志条目
            * Raft可以保证所有之前的任期号中已经提交的日志条目在选举的时候都会出现在新的领导人中，不需要传送这些日志条目给领导人
                - 这意味着日志条目的传送是单向的，只从领导人传给跟随者，并且领导人从不会覆盖自身本地日志中已经存在的条目
                - Raft 使用投票的方式来阻止一个候选人赢得选举除非这个候选人包含了所有已经提交的日志条目
                - 候选人为了赢得选举必须联系集群中的大部分节点，这意味着每一个已经提交的日志条目在这些服务器节点中肯定存在于至少一个节点上。如果候选人的日志至少和大多数的服务器节点一样新，那么他一定持有了所有已经提交的日志条目
            * 请求投票RPC实现了这样的限制：RPC中包含了候选人的日志信息，然后投票人会拒绝掉那些日志`没有自己新`的投票请求
                - Raft 通过比较两份日志中最后一条日志条目的索引值和任期号定义谁的日志比较新。
                - 如果两份日志最后的条目的任期号不同，那么任期号大的日志更加新。如果两份日志最后的条目任期号相同，那么日志比较长的那个就更加新
    - 跟随者和候选人崩溃
        + 跟随者和候选人崩溃后的处理方式比领导人要简单的多，并且他们的处理方式是相同的
        + 如果跟随者或者候选人崩溃了，那么后续发送给他们的 RPCs 都会失败。Raft 中处理这种失败就是简单的通过`无限的重试`；
            * 如果崩溃的机器重启了，那么这些 RPC 就会完整的成功。
            * 如果一个服务器在完成了一个 RPC，但是还没有响应的时候崩溃了，那么在他重新启动之后就会再次收到同样的请求
            * Raft 的 RPCs 都是`幂等`的，所以这样重试不会造成任何问题
                - 例如一个跟随者如果收到附加日志请求但是他已经包含了这一日志，那么他就会直接忽略这个新的请求
    - 时间和可用性
        + Raft 的要求之一就是安全性不能依赖时间：整个系统不能因为某些事件运行的比预期快一点或者慢一点就产生了错误的结果
        + 但是，可用性（系统可以及时的响应客户端）不可避免的要依赖于时间。
            * 例如，如果消息交换比服务器故障间隔时间长，候选人将没有足够长的时间来赢得选举；没有一个稳定的领导人，Raft 将无法工作
        + 领导人选举是 Raft 中对时间要求最为关键的方面。
            * Raft 可以选举并维持一个稳定的领导人,只要系统满足下面的时间要求：
                - `广播时间（broadcastTime） << 选举超时时间（electionTimeout） << 平均故障间隔时间（MTBF）`
                    + `广播时间`指的是从一个服务器并行的发送 RPCs 给集群中的其他服务器并接收响应的平均时间
                    + `选举超时时间`是选举的超时时间限制
                    + `平均故障间隔时间`就是对于一台服务器而言，两次故障之间的平均时间
                - `广播时间`必须比`选举超时时间`小一个量级，这样领导人才能够发送稳定的心跳消息来阻止跟随者开始进入选举状态
                    + 通过*随机化选举超时时间*的方法，这个不等式也使得选票瓜分的情况变得不可能
                - `选举超时时间`应该要比`平均故障间隔时间`小上几个数量级，这样整个系统才能稳定的运行
            * 当领导人崩溃后，整个系统会大约相当于`选举超时的时间`里不可用；我们希望这种情况在整个系统的运行中很少出现
                - 广播时间和平均故障间隔时间是由系统决定的，但是选举超时时间是我们自己选择的
                - Raft 的 RPCs 需要接收方将信息持久化的保存到稳定存储中去，所以广播时间大约是 *0.5 毫秒到 20 毫秒*，取决于存储的技术
                    + CPU 访问缓存的耗时在 10 纳秒左右，访问内存的时延则翻了 10 倍
                    + 如果访问 SSD固态磁盘，时间还要再翻个 1000 倍，达到 100 微秒；
                    + 如果访问机械硬盘，对随机小 IO 的访问要再翻个 100 倍，时延接近 10 毫秒
                    + 如果跨越网络，访问时延更要受制于主机之间的物理距离。比如杭州到伦敦相距 9200 公里，ping 时延接近 200 毫秒。当然，网络传输的可靠性低很多，一旦报文丢失，TCP 还需要至少 1 秒钟才能完成报文重传
                    + 参考[25 | 过期缓存：如何防止缓存被流量打穿？](https://time.geekbang.org/column/article/258035)
                - 因此，选举超时时间可能需要在*10毫秒到500毫秒*之间。大多数的服务器的平均故障间隔时间都在几个月甚至更长，很容易满足时间的需求
    - 集群成员变化
        + 在实践中，偶尔是会改变集群的配置的，例如替换那些宕机的机器或者改变复制级别。尽管可以通过暂停整个集群，更新所有配置，然后重启整个集群的方式来实现，但是在更改的时候集群会不可用
            * 为了让配置修改机制能够安全，那么在转换的过程中不能够存在任何时间点使得两个领导人*同时*被选举成功在同一个任期里
            * 不幸的是，任何服务器直接从旧的配置直接转换到新的配置的方案都是不安全的
        + 直接从一种配置转到新的配置是十分不安全的，因为各个机器可能在任何的时候进行转换
            * 例如，集群配额从 3 台机器变成了 5 台。不幸的是，存在这样的一个时间点，两个不同的领导人在同一个任期里都可以被选举成功。一个是通过旧的配置，一个通过新的配置
        + 为了保证安全性，配置更改必须使用两阶段方法
            * 在Raft中，集群先切换到一个过渡的配置，我们称之为`共同一致`；一旦共同一致已经被提交了，那么系统就切换到新的配置上
            * `共同一致`是老配置和新配置的结合：
                - 日志条目被复制给集群中新、老配置的所有服务器
                - 新、旧配置的服务器都可以成为领导人
                - 达成一致（针对选举和提交）需要分别在两种配置上获得大多数的支持
            * 共同一致允许独立的服务器在不影响安全性的前提下，在不同的时间进行配置转换过程
                - 此外，共同一致可以让集群在配置转换的过程中依然响应客户端的请求
            * 集群配置在复制日志中以特殊的日志条目来存储和通信
                - 当一个领导人接收到一个改变配置从 C-old 到 C-new 的请求，C-old 和 C-new 没有任何机会同时做出单方面的决定(具体过程见链接)
    - 日志压缩
        + Raft 的日志在正常操作中不断的增长，但是在实际的系统中，日志不能无限制的增长
        + 快照是最简单的压缩方法。
            * 在快照系统中，整个系统的状态都以快照的形式写入到稳定的持久化存储中，然后到那个时间点之前的日志全部丢弃
        + 增量压缩的方法，例如日志清理或者日志结构合并树，都是可行的
            * 这些方法每次只对一小部分数据进行操作，这样就分散了压缩的负载压力
        + 一个运行非常缓慢的跟随者或者新加入集群的服务器，这时让这个跟随者更新到最新的状态的方式就是通过网络把快照发送给他们
            * 在这种情况下领导人使用一种叫做`安装快照`的新的 RPC 来发送快照给太落后的跟随者
            * 通常快照会包含没有在接收者日志中存在的信息。在这种情况下，跟随者丢弃其整个日志；它全部被快照取代，并且可能包含与快照冲突的未提交条目
            * 如果接收到的快照是自己日志的前面部分（由于网络重传或者错误），那么被快照包含的条目将会被全部删除，但是快照后面的条目仍然有效，必须保留
                - 这种快照的方式背离了 Raft 的强领导人原则，因为跟随者可以在不知道领导人情况下创建快照
                - 但是我们认为这种背离是值得的。领导人的存在，是为了解决在达成一致性的时候的冲突，但是在创建快照的时候，一致性已经达成，这时不存在冲突了，所以没有领导人也是可以的
                - 数据依然是从领导人传给跟随者，只是跟随者可以重新组织他们的数据了
    - 客户端交互
        + Raft 中的客户端发送所有请求给领导人。当客户端启动的时候，他会随机挑选一个服务器进行通信
            * 如果客户端第一次挑选的服务器不是领导人，那么那个服务器会拒绝客户端的请求并且提供他最近接收到的领导人的信息（附加条目请求包含了领导人的网络地址）
            * 如果领导人已经崩溃了，那么客户端的请求就会超时；客户端之后会再次重试随机挑选服务器的过程
        + Raft 的目标是要实现`线性化语义`
            * 每一次操作立即执行，在他调用和收到回复之间，只执行一次
            * 但是如上所述，Raft 是可以执行同一条命令多次的：例如，如果领导人在提交了这条日志之后，但是在响应客户端之前崩溃了，那么客户端会和新的领导人重试这条指令，导致这条命令就被再次执行了
                - 解决方案就是客户端对于每一条指令都赋予一个`唯一的序列号`
                - 然后，状态机跟踪每条指令最新的序列号和相应的响应。如果接收到一条指令，它的序列号已经被执行了，那么就立即返回结果，而不重新执行指令
        + 只读的操作可以直接处理而不需要记录日志
            * 但是，在不增加任何限制的情况下，这么做可能会冒着返回脏数据的风险，因为领导人响应客户端请求时可能已经被新的领导人作废了，但是他还不知道
            * 线性化的读操作必须不能返回脏数据，Raft 需要使用两个额外的措施在不使用日志的情况下保证这一点
                - 首先，领导人必须有关于被提交日志的最新信息
                - 第二，领导人在处理只读的请求之前必须检查自己是否已经被废黜了
                    + Raft 中通过让领导人在响应只读请求之前，先和集群中的大多数节点交换一次心跳信息来处理这个问题
                    + (可选)领导人可以依赖心跳机制来实现一种租约的机制，但是这种方法依赖时间来保证安全性





# 分布式技术原理与算法解析

* [07 | 分布式锁：关键重地，非请勿入](https://time.geekbang.org/column/article/145505)
    - `分布式锁`是指分布式环境下，系统部署在多个机器中，实现多进程分布式互斥的一种锁
    - 为了保证多个进程能看到锁，锁被存在*公共存储*（比如 Redis、Memcache、数据库等三方存储中），以实现多个进程并发访问同一个临界资源，同一时刻只有一个进程可访问共享资源，确保数据的一致性



