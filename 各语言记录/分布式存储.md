# 分布式存储

* [18 | 分布式存储：你知道对象存储是如何保存图片文件的吗？](https://time.geekbang.org/column/article/220609)
    - 保存像图片、音视频这类大文件，最佳的选择就是对象存储
        + 对象存储不仅有很好的大文件读写性能，还可以通过水平扩展实现近乎无限的容量，并且可以兼顾服务高可用、数据高可靠这些特性
    - 对象存储之所以能做到这么“全能”，最主要的原因是，对象存储是`原生的分布式存储系统`
        + 这里讲的“`原生分布式存储系统`”，是相对于 MySQL、Redis 这类单机存储系统来说的
        + 虽然这些非原生的存储系统，也具备一定的集群能力，但用它们构建大规模分布式集群的时候，其实是非常不容易的
    - 随着云计算的普及，很多新生代的存储系统，都是原生的分布式系统，它们一开始设计的目标之一就是分布式存储集群，比如说`Elasticsearch`、`Ceph`和国内很多大厂推出的新一代数据库，大多可以做到：
        + 近乎无限的存储容量
        + 超高的读写性能
        + 数据高可靠：节点磁盘损毁不会丢数据
        + 实现服务高可用：节点宕机不会影响集群对外提供服务
    - 对象存储它的查询服务和数据结构都非常简单，是最简单的原生分布式存储系统
        + 本节通过对象存储来认识一下分布式存储系统的一些共性
    - 对象存储数据是如何保存大文件的？
        + 对象存储对外提供的服务，其实就是一个近乎无限容量的大文件KV存储，所以对象存储和分布式文件系统之间，没有那么明确的界限
        + 对象存储的内部，有很多的存储节点，用于保存这些大文件，这个就是**数据节点的集群**
        + 另外，为了管理这些数据节点和节点中的文件，还需要一个存储系统保存集群的节点信息、文件信息和它们的映射关系。
            * 这些为了管理集群而存储的数据，叫做`元数据 (Metadata)`
            * 元数据对于一个存储集群来说是非常重要的，所以保存元数据的存储系统必须也是一个集群(**元数据集群**)
            * 元数据集群存储的数据量比较少，数据的变动不是很频繁，加之客户端或者网关都会缓存一部分元数据，所以元数据集群对并发要求也不高
            * 一般使用类似`ZooKeeper`或者`etcd`这类分布式存储就可以满足要求
        + 另外，存储集群为了对外提供访问服务，还需要一个**网关集群**，对外接收外部请求，对内访问元数据和数据节点
            * 网关集群中的每个节点不需要保存任何数据，都是`无状态`的节点
            * 有些对象存储没有网关，取而代之的是客户端，它们的功能和作用都是一样的
        + 那么，对象存储是如何来处理对象读写请求的呢？
            * 处理读和写请求的流程是一样的
            * 网关收到对象读写请求后，首先拿着请求中的Key，去元数据集群查找这个Key在哪个数据节点上，然后再去访问对应的数据节点读写数据，最后把结果返回给客户端
            * 以上是一个比较粗略的大致流程，相关名词改一改完全可以套用到绝大多数分布式文件系统和数据库上去，比如说 HDFS
    - 对象是如何拆分和保存的？
        + 一般来说，对象存储中保存的文件都是图片、视频这类大文件。
            * 在对象存储中，每一个大文件都会被拆成多个大小相等的`块（Block）`，拆分的方法很简单，就是把文件从头到尾按照固定的块大小，切成一块儿一块儿，最后一块的长度有可能不足一个块的大小，也按一块来处理。
            * 块的大小一般配置为`几十 KB` 到`几个 MB` 左右
        + 把大对象文件拆分成块的目的有两个：
            * 第一是为了提升读写性能，这些块儿可以分散到不同的数据节点上，这样就可以并行读写
            * 第二是把文件分成大小相等块，便于维护管理
        + 一般都会再把块聚合一下，放到块的容器里面
            * 这里的“容器”就是存放一组块的逻辑单元。容器这个名词，没有统一的叫法，比如在`ceph`中称为 `Data Placement`
            * 这个容器的概念，就比较类似于 MySQL 和 Redis 的“`分片`”的概念，都是复制、迁移数据的基本单位
            * 每个容器都会有 N 个副本，这些副本的数据都是一样的。其中有一个主副本，其他是从副本，主副本负责数据读写，从副本去到主副本上去复制数据，保证主从数据一致
        + 对象存储一般都不记录类似MySQL的Binlog这样的日志。主从复制的时候，复制的不是日志，而是整块儿的数据，这么做有两个原因：
            * 第一个原因是基于性能的考虑
                - 操作日志里面，实际上就包含着数据。在更新数据的时候，先记录操作日志，再更新存储引擎中的数据，相当于在磁盘上串行写了 2 次数据
                - 对于像数据库这种，每次更新的数据都很少的存储系统，这个开销是可以接受的
                - 对于对象存储来说，它每次写入的块儿很大，两次磁盘 IO 的开销就有些不太值得了
            * 第二个原因是它的存储结构简单，即使没有日志，只要按照顺序，整块儿的复制数据，仍然可以保证主从副本的数据一致性
        + 怎么把块儿映射到容器中
            * 可以参考：[《15 | MySQL 存储海量数据的最后一招：分库分表》](https://time.geekbang.org/column/article/217568)这节课中讲到的几种分片算法
            * 不同的系统选择实现的方式也不一样，有用哈希分片的，也有用查表法把对应关系保存在元数据中的
            * 找到容器之后，再去元数据中查找容器的N个副本都分布在哪些数据节点上。然后，网关直接访问对应的数据节点读写数据就可以了
    - 小结
        + 对象存储是最简单的分布式存储系统，主要由`数据节点集群`、`元数据集群`和`网关集群（或者客户端）`三部分构成
            * 数据节点集群负责保存对象数据，
            * 元数据集群负责保存集群的元数据，
            * 网关集群和客户端对外提供简单的访问 API，对内访问元数据和数据节点读写数据
        + 对象存储虽然简单，但是它具备一个分布式存储系统的全部特征。所有分布式存储系统共通的一些特性，对象存储也都具备，比如说数据如何分片，如何通过多副本保证数据可靠性，如何在多个副本间复制数据，确保数据一致性等等
* [15 | MySQL存储海量数据的最后一招：分库分表](https://time.geekbang.org/column/article/217568)
    - 解决海量数据的问题，必须要用到分布式的存储集群，因为 MySQL 本质上是一个单机数据库，所以很多场景下不是太适合存 TB 级别以上的数据
        + 但是，绝大部分的电商大厂，它的在线交易这部分的业务，比如说，订单、支付相关的系统，还是舍弃不了 MySQL，原因是，只有 MySQL 这类关系型数据库，才能提供`金融级的事务保证`
        + 那些新的分布式数据库提供的所谓的分布式事务，目前还达不到这些交易类系统对数据一致性的要求
    - 既然 MySQL 支持不了这么大的数据量，这么高的并发，还必须要用它，就需要进行`分片`
        + `分片`，也就是拆分数据。1TB 的数据，一个库撑不住，我把它拆成 100 个库，每个库就只有 10GB 的数据了，这不就可以了么？这种拆分就是所谓的 `MySQL 分库分表`

# ceph

* github：[ceph/ceph](https://github.com/ceph/ceph)
    - 源码编译
        + github下载源码(看git clone下来有`650M`)
        + 编译脚本
            * 默认是debug版本，可以如下添加编译选项，编译非debug版本(README中说会快5倍)
            * `ARGS="-DCMAKE_BUILD_TYPE=RelWithDebInfo" ./do_cmake.sh` 执行完后大小比较大，查看大小有1.7G
            * 会创建`build`目录，若编译或者编译检查失败，下一次执行需要手动删除`build`目录(再次执行上面脚本时会提示)
        + 问题
            * 脚本最后报错：Can't find sphinx-build
                - 解决：`yum install python-sphinx`
            * 安装上面的python包后，再次执行脚本，报错：GCC 7+ required due to C++17 requirements
                - 需要`gcc7`以上版本(支持C++17)，本机版本为`gcc 版本 4.8.5 20150623 (Red Hat 4.8.5-36) (GCC)`
                - gcc对C++各版本的支持，之前笔记也有记录(`#### C++11 编译器支持：`)：[devNoteBackup/各语言记录/C++.md](https://github.com/xiaodongQ/devNoteBackup/blob/master/%E5%90%84%E8%AF%AD%E8%A8%80%E8%AE%B0%E5%BD%95/C%2B%2B.md)
                - 可以临时使用gcc高版本
                    + `yum -y install centos-release-scl`
                    + `yum -y install devtoolset-7-gcc devtoolset-7-gcc-c++ devtoolset-7-binutils`
                    + `scl enable devtoolset-7 bash`
                    + 重启后会恢复原系统gcc版本，每次临时使用可以`scl enable devtoolset-7 bash`，会进入一个新的bash
                    + 如果要长期使用gcc 7.3的话
                        * `echo "source /opt/rh/devtoolset-7/enable" >>/etc/profile`
                    + 其他gcc版本可参考：[为CentOS 6、7升级gcc至4.8、4.9、5.2、6.3、7.3等高版本](https://www.vpser.net/manage/centos-6-upgrade-gcc.html)
            * gcc版本升级后，再次执行报错：`Could NOT find verbs`
                - `yum install rdma-core-devel -y`
            * 报错：`Could NOT find udev`
                - `yum install systemd-devel -y`
            * 上面的问题，这里面都碰到了，直接继续下面的安装：[CentOS 7.6 源码编译安装Ceph](https://blog.csdn.net/helloanthea/article/details/103728684)
                - `yum install libblkid-devel -y`
                - `yum install keyutils-libs-devel -y`
                - `yum install openldap-devel -y`
                - `yum install leveldb-devel -y`
                - `yum install snappy-devel -y`
                - `yum install lz4-devel -y`
                - `yum install curl-devel -y`
                - 还有这些：`yum install libaio-devel openssl-devel expat-devel liboath-devel lttng-ust-devel libbabeltrace-devel python36-Cython fuse-devel libnl3-devel librabbitmq-devel libcap-ng-devel gperf librabbitmq-devel librdkafka-devel -y`
            * 继续报错：`Could NOT find zbc`


* 中文文档：[Ceph Documentation ](http://docs.ceph.org.cn/start/intro/)
    - 跟英文文档有些出入，貌似一些内容已经out了，建议通过中文文档大致了解后，后续深入再读英文文档(主要是一些概念)
* 英文文档：[Ceph Documentation](https://ceph.readthedocs.io/en/latest/start/intro/)
* Ceph简介
    - 所有 Ceph 存储集群的部署都始于部署一个个 `Ceph 节点`、`网络`和 `Ceph 存储集群`
        + `Ceph 存储集群`至少需要一个 `Ceph Monitor`、`Ceph Manager` 和 `Ceph OSD`(`Object Storage Daemon`守护进程)
            + 运行 Ceph 文件系统客户端时，还必须要有`元数据服务器`（`Ceph Metadata Server`）
        + `Monitors`
            * Ceph Monitor(`ceph-mon`)维护着*集群状态*的各种图表(map)，包括监视器图、管理器图、OSD图、MDS图、和 CRUSH 图，这些图(map)是Ceph守护程序相互协调所必需的关键集群状态
            * Monitor还负责守护进程和客户端的认证
            * 通常至少需要`三个`Monitor才能实现冗余和高可用
        + `Managers`
            * Ceph Manager守护进程(`ceph-mgr`)负责跟踪运行时指标和当前Ceph集群的状态，包括存储利用率、当前性能指标和系统负载
            * Manager守护进程还管理着一个基于Python的模块，用来管理和展示Ceph集群信息，包括一个Ceph仪表盘和REST API
            * 通常至少需要`两个`Manager才能实现高可用
        + `Ceph OSDs`
            * Ceph OSD(object storage daemon, `ceph-osd`)用来*存储数据*，处理数据的复制、恢复、再平衡，并通过检查其他OSD守护进程的心跳来向 `Ceph Monitors`和`Managers` 提供一些监控信息
            * 通常至少需要`三个`Ceph OSDs才能实现冗余和高可用
        + `MDSs`
            * Ceph Metadata Server（MDS, `ceph-mds`）为 Ceph 文件系统存储*元数据*(metadata)（也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS ）
            * Ceph Metadata Server 允许 POSIX 文件系统的用户们，可以在不对Ceph存储集群造成负担的前提下，执行诸如`ls`、`find` 等基本命令
    - Ceph 把数据作为`对象`存储在逻辑存储池中。通过`CRUSH`算法，Ceph 可以计算出哪个归置组（`placement group`，PG）应该包含指定的对象(Object)，然后进一步计算出哪个Ceph OSD守护进程应该存储这个placement group。
        + CRUSH 算法使得 Ceph 存储集群能够动态地伸缩、再均衡和修复
* 硬件推荐
    - [硬件推荐](http://docs.ceph.org.cn/start/hardware-recommendations/)
    - 里面涉及CPU、RAM内存、硬盘、固态硬盘(SSD)、网络等的要求和注意事项