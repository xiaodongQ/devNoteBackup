# 大规模数据处理实战

* 极客时间课程学习记录：[大规模数据处理实战](https://time.geekbang.org/column/article/90067)

> 学会用一个技术只是第一步，最重要的是要追问自己：  
	- 这个技术解决了哪些痛点？  
	- 别的技术为什么不能解决？  
	- 这个技术用怎样的方法解决问题？  
	- 采用这个技术真的是最好的方法吗？  
	- 如果不用这个技术，你会怎样独立解决这类问题？  

* 为什么 MapReduce 会被硅谷一线公司淘汰
	- 超大规模数据处理的技术发展阶段
		+ "石器时代"：MapReduce 诞生之前的时期
		+ "青铜时代"：2003 年，MapReduce 的诞生标志了超大规模数据处理的第一次革命
			* 《MapReduce: Simplified Data Processing on Large Clusters》论文，从纷繁复杂的业务逻辑中，为我们抽象出了 Map 和 Reduce 这样足够通用的编程模型，后面的 Hadoop 仅仅是对于 GFS、BigTable、MapReduce 的依葫芦画瓢
		+ "蒸汽机时代"：到了 2014 年左右，Google 内部已经几乎没人写新的 MapReduce 了
			* 2016 年开始，Google 在新员工的培训中把 MapReduce 替换成了内部称为 FlumeJava（不要和 Apache Flume 混淆，是两个技术）的数据处理技术
	- 为什么 MapReduce 会被取代？
		+ 高昂的维护成本
			* 使用 MapReduce，你需要严格地遵循分步的 Map 和 Reduce 步骤。当你构造更为复杂的处理架构时，往往需要协调多个 Map 和多个 Reduce 任务。然而，每一步的 MapReduce 都有可能出错。
			* 为了这些异常处理，很多人开始设计自己的协调系统（orchestration）。例如，做一个状态机（state machine）协调多个 MapReduce，这大大增加了整个系统的复杂度。
		+ 除了高昂的维护成本，MapReduce 的时间性能也是个棘手的问题。
			* 在实际的工作中，不是每个人都对 MapReduce 细微的配置细节了如指掌。这种情况下开发的系统是很难发挥好 MapReduce 的性能的。
			* MapReduce 的性能优化配置究竟复杂在哪，Google500 多页的 MapReduce 性能优化手册足够说明它的复杂度了。。。
	- MapReduce系统示例：`预测美团的股价`，其中一个重要特征是活跃在街头的美团外卖电动车数量，而你负责`处理所有美团外卖电动车的图片`
		+ 在真实的商用环境下，为了解决这个问题，你可能至少需要 10 个 MapReduce 任务
		+ 首先，我们需要搜集每日的外卖电动车图片
			* 数据的搜集往往不全部是公司独自完成，许多公司会选择部分外包或者众包
			* 在`数据搜集（Data collection）`部分，你至少需要 4 个 MapReduce 任务：
				- `数据导入（data ingestion）`：下载散落的图片（比如众包公司上传到网盘的照片）下载到存储系统
				- `数据统一化（data normalization）`：用来把不同外包公司提供过来的各式各样的照片进行格式统一
				- `数据压缩（compression）`：在质量可接受的范围内保持最小的存储资源消耗
				- `数据备份（backup）`：大规模的数据处理系统我们都需要一定的数据冗余来降低风险
		+ 然后，需要一部分`数据质量控制（quality control）`流程
			* 仅仅是做完数据搜集这一步，离真正的业务应用还差得远
			* 比如：
				- `数据时间有效性验证 （date validation）`：检测上传的图片是否是你想要的日期的
				- `照片对焦检测（focus detection）`：需要筛选掉那些因对焦不准而无法使用的照片
		* 最后才到你负责的重头戏：找到这些图片里的外卖电动车
			* 这一步因为人工的介入是最难控制时间的。你需要做 4 步：
				- `数据标注问题上传（question uploading）`：上传你的标注工具，让你的标注者开始工作
				- `标注结果下载（answer downloading）`：抓取标注完的数据
				- `标注异议整合（adjudication）`：标注异议经常发生，比如一个标注者认为是美团外卖电动车，另一个标注者认为是京东快递电动车
				- `标注结果结构化（structuralization）`：要让标注结果可用，你需要把可能非结构化的标注结果转化成你的存储系统接受的结构
	- 真实的商业 MapReduce 场景极端复杂，像上面这样 10 个子任务的 MapReduce 系统在硅谷一线公司司空见惯
		* 在应用过程中，每一个 MapReduce 任务都有可能出错，都需要重试和异常处理的机制。所以，协调这些子 MapReduce 的任务往往需要和业务逻辑紧密耦合的`状态机`
* [02 | MapReduce后谁主沉浮：怎样设计下一代数据处理技术？](https://time.geekbang.org/column/article/90533)
	- 需要一种技术抽象让多步骤数据处理变得易于维护
		+ 可以用有向无环图（DAG）来抽象表达。
			* 有向无环图(Directed acyclic graph)可参考(相对文章中的有向图，只是加了无环限定)：[30 | 图的表示：如何存储微博、微信等社交网络中的好友关系？](https://time.geekbang.org/column/article/70537)
			* 因为有向图能为多个步骤的数据处理依赖关系，建立很好的模型
		+ 西红柿炒鸡蛋这样一个菜，就是一个有向无环图概念的典型案例
			* 示例见链接
			* 如果用 MapReduce 来实现的话，在这个图里面，每一个箭头都会是一个独立的 Map 或 Reduce。为了协调那么多 Map 和 Reduce，难以避免会去做很多检查，比如：番茄是不是洗好了，鸡蛋是不是打好了，最后这个系统就不堪重负了
		+ 对于上面的MapReduce实现问题，用`有向图`建模，图中的每一个节点都可以被抽象地表达成一种通用的`数据集`，每一条边都被表达成一种通用的`数据变换`
			* 如此，你就可以用`数据集`和`数据变换`描述极为宏大复杂的数据处理流程，而不会迷失在依赖关系中无法自拔
	- 我们不想要复杂的配置，需要能自动进行性能优化
		+ 要能把两条数据处理过程中重复的部分进行合并
		+ 数据流水线的预处理部分也应该把一些无关的数据操作优化掉
		+ 另一种自动的优化是计算资源的自动弹性分配
			* 优化系统要有可以处理这种问题的弹性的劳动力分配机制。它要能自动分配，比如 100 台机器处理 1000 个番茄，如果是 10000 个番茄那就分配 1000 台机器，但是只给热油 1 台机器可能就够了
			* 在数据处理开始前，我们需要有一个自动优化的步骤和能力，而不是按部就班地就把每一个步骤就直接扔给机器去执行了。
	- 要能把数据处理的描述语言，与背后的运行引擎解耦合开来
		+ 有向图可以作为`数据处理描述语言`和`运算引擎`的前后端分离协议
		+ 除了有向图表达需要`数据处理描述语言`和`运算引擎`协商一致，其他的实现都是灵活可拓展的
	- 要统一`批处理`和`流处理`的编程模型
		+ `批处理`处理的是有界离散的数据，比如处理一个文本文件；`流处理`处理的是无界连续的数据，比如每时每刻的支付宝交易数据。
		+ `MapReduce` 的一个局限是它为了批处理而设计的，应对流处理的时候不再那么得心应手
			* 即使后面的 `Apache Storm`、`Apache Flink` 也都有类似的问题，比如 `Flink` 里的批处理数据结构用 `DataSet`，但是流处理用 `DataStream`。
			* 但是真正的业务系统，批处理和流处理是常常混合共生，或者频繁变换的
			* 因此数据处理框架里，就得有更高层级的数据抽象
		+ 不论是批处理还是流处理的，都用统一的数据结构表示。编程的 API 也需要统一。这样不论业务需求什么样，开发者只需要学习一套 API。即使业务需求改变，开发者也不需要频繁修改代码
	- 要在架构层面提供异常处理和数据监控的能力
		+ 在一个复杂的数据处理系统中，难的不是开发系统，而是异常处理。
			* 要设计一套基本的数据监控能力，对于数据处理的每一步提供自动的监控平台，比如一个监控网站
			* 需要把每一步的相关信息进行存储，比如是谁去买的蛋，哪些人打蛋。这样出错后可以帮助用户快速找到可能出错的环节
	- 后续会深入和上面这些设计理念最接近的大数据处理框架：`Apache Spark` 和 `Apache Beam`。
* [03 | 大规模数据处理初体验：怎样实现大型电商热销榜？](https://time.geekbang.org/column/article/91125)
	- 数据规模变大时该怎么设计系统，需要有规模增长的技术思维（`mindset of scaling`）
		+ 因为产品从 1 万用户到 1 亿用户，技术团队从 10 个人到 1000 个人，你的技术规模和数据规模都会完全不一样
		+ 同样的问题举一反三，可以应用在淘宝热卖，App 排行榜，抖音热门，甚至是胡润百富榜，因为实际上他们背后都应用了相似的大规模数据处理技术
	- 简化示例：
		+ 假设你的电商网站销售 10 亿件商品，已经跟踪了网站的销售记录：商品 id 和购买时间 {product_id, timestamp}，整个交易记录是 1000 亿行数据，TB 级
		+ 作为技术负责人，你会怎样设计一个系统，根据销售记录统计去年销量前 10 的商品呢？
		+ `小规模`的经典算法
			* 第一步，统计每个商品的销量。可以用哈希表（hashtable）数据结构来解决，是一个 `O(n)` 的算法，这里 n 是 1000 亿
			* 第二步，找出销量前十，可以用经典的 `Top K` 算法，也是 `O(n)` 的算法
			* 可以参考堆的使用：[29 | 堆的应用：如何快速获取到Top 10最热门的搜索关键词？](https://time.geekbang.org/column/article/70187)
	- 在小规模系统中，我们确实完全可以用经典的算法简洁漂亮地解决。
		+ 但在一切系统中，随着尺度的变大，很多方法就不再适用
		+ 具体在我们这个问题中，同样的 `Top K` 算法当数据规模变大会遇到哪些问题呢？
			* 第一，内存占用
				- 对于 TB 级的交易记录数据，很难找到单台计算机容纳那么大的哈希表了
			* 第二，磁盘 I/O 等延时问题
				- 当数据规模变大，我们难以避免地需要把一些中间结果存进磁盘，以应对单步任务出错等问题。一次磁盘读取大概需要 10ms 的时间
				- 如果按照上一点提到的文件替代方法，因为我们是一个 `O(n * log k)` 的算法(topk)，就需要`10ms * 10^9 = 10 ^ 7 s = 115 天`的时间
		+ 当单台机器已经无法适应我们数据或者问题的规模，我们需要横向扩展
	- `大规模`分布式解决方案
		+ 之前的思路依然没错。但是，我们需要把每一步从简单的函数算法，升级为计算集群的分布式算法。
		+ 我们需要的第一个计算集群，就是`统计商品销量的集群`
			* 例如，1000台机器，每台机器一次可以处理1万条销售记录。对于每台机器而言，它的单次处理又回归到了我们熟悉的传统算法，数据规模大大缩小
		+ 我们需要的第二个计算集群，则是`找出销量前十的集群`
			* 在上一个统计销量集群得到的数据输出，将会是我们这个处理流程的输入
			* 每台机器，先找出自己机器上销量前 K 大的商品，所以有1000个前K大记录(相同key的数据先哈希到同一台或者一组机器上)
		+ 最后一步，把在“销量前 K 集群”中的结果`汇总`出来
			* 这时候完全可以用单一机器解决了。因为实际上你汇总的就是这 1000 台机器的结果，规模足够小
		+ 当你辛辛苦苦设计了应对 1 亿用户的数据处理系统时，可能你就要面临另一个维度的`规模化（scaling）`
			* 那就是应用场景数量从 1 个变成 1000 个。每一次都为不同的应用场景单独设计分布式集群
			* 这时，你需要一个数据处理的框架
	- 大规模数据处理框架的功能要求
		+ 先忘掉上一节大规模数据处理框架的要求
		+ 两个最基本的需求：
			* 高度抽象的数据处理流程描述语言
				- 作为框架使用者，希望框架是非常简单的，能够用几行代码把业务逻辑描述清楚，而不用再配置分布式系统的每台机器
				- 如：`sales_count = sale_records.Count()`，对于这样简单的描述，在我们框架设计层面，就要能自动构建成上文描述的“销量统计计算集群”
			* 根据描述的数据处理流程，自动化的任务分配优化
				- `top_k_sales = sales_count.TopK(k)`，这行代码需要自动构建成上文描述的“找出销量前 K 集群”

